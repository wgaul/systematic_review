---
title: "Biological Records Systematic Review - Preliminary Methods and Results"
date: 18 Jan 2020
csl: ecology.csl
output:
  pdf_document: default
  word_document:
    toc: true
    toc_depth: 4
  fig_caption: yes
  html_document:
    number_sections: true
    df_print: kable
---

<div style="line-height: 2em;">

# Preliminary structure of analysis and results
This document states the questions I plan to ask and shows methods, figures, tables and results text that I expect to use to answer each question in the biological records systematic review. Specific results and interpretations will change when I code the rest of the articles.

# Questions for Dina

Do I need to use the author, institution of the author, or source of the data in some way to account for possible pseudo-replication?  For example, the UK Butterfly Monitoring Scheme data is used in multiple studies.  Is it appropriate to count each of those uses as an independent data point in analyses of e.g temporal extent of studies?  I think it might be ok, because many studies (about a third) seem to integrate multiple datasets, so that many of the things I test might not depend only on a single dataset.  Thoughts?

Do I need some sort of multiple comparison correction for doing multiple tests with the same dataset?


TODO:

- auto-generate table of contents

- add cross-validation to all model evaluation?

- Methods
    - *Study eligibiligy* [DONE]
    - Article coding
        - Data Type
        - Study Questions
        - *Analysis approach* [DONE]
        - *Agreement* [DONE]
    - *Temporal Extent* [DONE]
    - Analysis of study questions
        - *Most common paradigms* [DONE]
        - Development of methodology across different types of study questions
        - Type of data used across study question paradigms
    - Analysis approach & data type
    - Authors and Data Providing Institutions
- Results - Statistical
    - Search results
        - n returned, n not returned by multiple searche methods (represents incompleteness of search), eligibility, n vars coded, n studies coded, n double coded
    - Reader agreement
    - *Temporal extent* [DONE]
    - Authors and Data Providing Institutions
    - Study Question
        - *Paradigm* [DONE]
        - Data type by question paradigm (add stats)
    - Analysis approach & data type
- Results - Descriptive
    - Spatial extent
    - *Specific focus* [DONE]
    - Author associated with data provider
    - Spatial bias correction
    - Taxonomic group
    - Role of biological records
    - Prediction performance measure
    - Tabulate analysis methods
- Discussion
    - Article coding
        - Agreement
    - Temporal extent
    - Study question
    - Analysis approach & data type
    - Bio recs as predictors, facilitative roles (horizon scanning)
 


```{r, echo=F}
knitr::opts_chunk$set(echo=F, message=F, warning=T, cache=F)
```

```{r, load_data, cache = F, results='hide', warning = F}
library(wgutil)
library(Hmisc)
library(boot)
library(irr)
library(ResourceSelection)
library(captioner)
library(pander)
library(knitr)
library(ggridges)
library(bestglm)
library(MASS)
library(car)
library(GGally)
library(tidyverse)

diag <- F # produce exploratory & diagnostics plots & results
diag_present <- T # produce diagnostics for report to Dina
plot_text_size <- 15

if(diag == F) { # turn off warnings when not running diagnostics
  knitr::opts_chunk$set(echo=F, message=F, warning=F, cache=F)
}

set.seed(18012020) # 18 Jan 2020 

source("./clean_data_syst_rev.R")
```

```{r numberingPrep}
# make functions for adding numbered captions to figures 
figs <- captioner(prefix = "Fig.") 
tabs <- captioner(prefix = "Table")
```


# Methods

**See Hao et al as a template for how to write syst. rev. methods.**

We did a systematic review of original research published since 2014 that used biological records from Ireland and/or the United Kingdom (UK).  We generated a list of relevant articles from Web of Science, Scopus, ProQuest, GoogleScholar via Publish or Perish software (@Harzing2007), and the Global Biodiversity Information Facility (GBIF) website using the search terms shown in Appendix 2.  One researcher evaluated each article for inclusion eligibility and coded information on 46 variables describing characteristics of each article.  A second reader coded all variables for a subset of 20% of the articles.  Agreement between the two readers was evaluated using Krippendorff's alpha (Krippendorff 2004).  Variables that did not meet minimum standards for reader agreement were not used in subsequent analyses.  Analyses were conducted in R version 3.5 (@RCoreTeam2015).

## Study eligibility
Studies were deemed eligible if they presented original research (no reviews or idea papers) published in the English language, used opportunistic biological records data collected with non-standardized or semi-standardized designs, included (but were not necessarily limited to) data from Ireland or the UK, and the full text of the study was available through the University College Dublin library online platform, Google Scholar, Google search results, or ResearchGate.  Grey literature was included and therefore peer review was not required.  Studies using semi-standardized data collection schemes (e.g. UK Butterfly Monitoring Scheme) were included as long as data collection included some opportunistic elements (e.g. locations chosen opportunistically by volunteers).  Publications of data (e.g. atlases or data papers) were not considered eligible unless they included analysis of the data.  Only studies using a sample size of greater than 20 were included; this sample size was chosen arbitrarily, mainly to exclude studies in which re-examination of museum specimens resulted in a taxonomic identification revision for one or a few specimens, thereby changing the known range of a species to include or exclude Ireland and the UK.  Studies using museum data were considered eligible when the museum data used was similar in format to biological records data (e.g. “what, where, when” data); studies that used museum specimens only for taxonomic, genetic or morphology studies were excluded.  Studies using only fossil records were not included.  Studies using data from phenology networks were included; for the purposes of this review such data are considered biological records data with associated additional data (e.g. the flowering status of plants).  Studies for which all data was collected by the study authors were not considered biological records data for the purposes of this review and were excluded.  The minimum required information in the data was a taxonomic name, a location, and date (“what, where, when”); additional information was permissible.  

## Article coding

For a list of the variables along with a description of each, see Appendix 1 *Instructions for Coders*.  Here we provide a brief summary of the broad types of information we coded.

### Data Type
We coded thirteen binary variables describing aspects of the type and structure of the biological records data.  The data type categories are not mutually exclusive.  

For most analyses, we grouped the data types "physical specimen", "photo", "audio", and "video" into a "voucher specimen" group, but we coded them individually in order to identify emerging trends in how vouchers are collected.

For statistical analyses, we included data type variables in models based on *a-priori* expectations about the variables' influence on data analysis methods and based on expectations about different levels of complexity and effort involved in collecting, recording, and storing the data.  The data type variables we kept in models were: what, where, when only; sampling effort known; abundance; non-detection; organized monitoring scheme; visit-specific covariates; multiple datasets integrated for analysis; life stage information; and voucher specimen. 

### Study Questions
We described the main focus(es) of each study in terms of whether the study was asking questions about individual species, ecological communities, or about methodology. We also identified whether studies analyzed each of the following biological or ecological responses: species richness; species turnover (beta-diversity); species distributions; abundance; phenology.

### Analysis approach
We classified the broad analysis approach used by each study into one of three categories: inference; prediction; or description.  For each study, we evaluated the analysis approach only for analyses that used biological records data.

We considered studies estimating parameters and reporting some measure of uncertainty (e.g. *p*-values, confidence intervals, posterior probability distributions) to be using inference.  We considered studies that built models and made predictions to be using an analysis strategy of prediction, even if no uncertainty or inference was reported with the predictions.  Finally, we considered the analysis approach to be descriptive only if results were descriptive with no prediction or inference.  Descriptive results could be narrative, graphical, or quantitative as long as no inference or uncertainty was reported (e.g. point estimates of descriptive statistics without any confidence interval or *p*-values).  Studies could use both inference and prediction as analysis strategies and so those categories were not mutually exclusive, but studies were only categorized as using a descriptive analysis approach if all results were descriptive only and the study used no inference or prediction.  

### Coding reliability
We used Krippendorff's *alpha* [@Krippendorff2004] to evaluate the agreement between two readers for each of `r ncol(er)-6` variables that were coded by two people.  We calculated Krippendorff's *alpha* using the `kripp.alpha` function in the `irr` package [@irr2012].  Krippendorff's alpha measures agreement between multiple coders while accounting for agrement by chance.  Rule-of-thumb guidelines suggest that Krippendorff's alpha values below 0.67 indicate poor agreement such that the data cannot be trusted for analysis; values between 0.67 and 0.8 suggest that the data are moderately reliable and may be suitable for only tentative interpretation, and values above 0.8 indicate that the data are reliable.  In order to estimate reliability using Krippendorff's alpha, there must be sufficient variation in the variable being coded.  The amount of data needed to estimate alpha for a categorical variable depends on the probability of the least common class for that variable. Krippendorff [@Krippendorff2004] provides guidelines about the number of coded cases needed to estimate the value of Krippendorff's alpha given the probability of the least common class for a variable.  We examined Krippendorff's alpha values only for variables for which the least common class had a probability of 0.14 or higher, which should allow us to identify with a confidence level of 0.1 those variables for which Krippendorff's alpha is above 0.667 [@Krippendorff2004].  We removed from subsequent statistical analyses all variables for which we did not have enough codings to estimate Krippendorff's alpha, and all variables with Krippendorff's alpha values below 0.67 from analyses. 

```{r inter_coder_agreement_subset_dfs, warning=TRUE}
## ellie's results ----------------------------------------------------
er <- er[order(er$title), ] # order rows
er <- er[, order(colnames(er))] # order columns
# put title column first
er <- er[, c(which(colnames(er) == "title"), 
                       which(colnames(er) != "title"))]
## end reorder ellie's results ------------------------------------------------

## reorder my results  --------------------------------------------------------
# subset my coded results to titles and columns coded by ellie
wg_dbl_cd <- wg[which(wg$title %in% er$title), which(colnames(wg) %in% colnames(er))]
# order rows
wg_dbl_cd <- wg_dbl_cd[order(wg_dbl_cd$title), ] 
# order cols
wg_dbl_cd <- wg_dbl_cd[, order(colnames(wg_dbl_cd))] 
wg_dbl_cd <- wg_dbl_cd[, c(which(colnames(wg_dbl_cd) == "title"), 
             which(colnames(wg_dbl_cd) != "title"))] # put title column first
## end reorder my results  ----------------------------------------------------

## make sure all titles are present
if(any(wg_dbl_cd$title %nin% er$title) | 
   any(er$title %nin% wg_dbl_cd$title)) {
  warning("er and wg_dbl_cd don't have the same titles. One or both of those data frames will be subsetted.")
  wg_dbl_cd <- wg_dbl_cd[which(wg_dbl_cd$title %in% er$title), ]
  er <- er[which(er$title %in% wg_dbl_cd$title), ]
}

## make sure df rows and cols are in the same order
if(!identical(colnames(er), colnames(wg_dbl_cd))) {
  stop("Columns in er and wg_dbl_cd must be identical and in the same order. They must be in the same order for creating the variable data frames that will be used to calculate Krippendorf's alpha. ")
}
```

```{r make_dfs_of_double_coded_variables_all_studies}
# each row is codings from one person
# each column is a study
dbl_coded_list <- list()
if(colnames(er)[1] != "title" | colnames(wg_dbl_cd)[1] != "title") {
  stop("The first column of er and wg must be the study title")}

for(i in 2:ncol(er)) {
  dbl_coded_list[[i-1]] <- data.frame(matrix(
    data = c(er[, i], wg_dbl_cd[, i]), 
    nrow = 2, ncol = nrow(er), byrow = T))
}
names(dbl_coded_list) <- colnames(er)[2:ncol(er)]
```

```{r krippendorfs_alpha_calc, warning=FALSE, message=FALSE}
do_kripp.alpha <- function(x, method) {
  # function to call kripp.alpha while converting dataframe x to a matrix
  # ARGS: x - data frame in which rows are observers and columns are studies
  #       method - character string of method to pass to kripp.alpha
  kripp.alpha(as.matrix(x), method = method)
}

warning("WG says: the kripp.alpha function makes a 'dv' object that is 'data values' but when values are character or logical (as in my case) this producees NAs during coercion.  However, that 'dv' object is only used if the method is set to interval', so in my case the NAs that the warning is about are never used because I am using the nominal method.  So the warnings are nothing to worry about as long as method is 'nominal'.")
kripp_all_dbl_coded <- lapply(dbl_coded_list, FUN = do_kripp.alpha, 
                              method = "nominal")
```


## Temporal extent of studies

```{r correlation_cap}
correlation_cap <- figs(name = "correlation_cap", 
                        "Correlation of the binary data type predictor variables.")
```

We used linear regression with natural-log transformed time (in years) as the response variable.  We used five binary predictor variables indicating whether the study used at least some data that had the following characteristics or information: known sampling effort; explicit non-detection data; data came from an organized monitoring scheme; visit-specific covariates; multiple different biological records datasets integrated for analyses.  Although some of the predictor variables may be correlated (`r figs("correlation_cap", display = "cite")`), we kept all variables in the regression model because we were interested in the effect of each additional type of information *after accounting for other information types*.  For example, we were interested in the additional effect of having non-detection data after adjusting for whether the data came from an organized monitoring scheme.  We assessed collinearity of predictor variables using variance inflation factors in order to determine if collinearity is likely to cause additional uncertainty in the the coefficient estimates for the effect of each data type (Fox 2016). 

Because the variables are not mutually exclusive, each category is treated as an individual binary variable, rather than treating each variable as a factor level of a categorical "data type" variable.  

We assessed the significance data types for predicting temporal extent by using a likelihood ratio test to compare a model containing main effects (but no interactions) for all data types to an intercept only model.  We then assessed the significance of each individual data type by comparing the model with all main effects to a reduced model in which the variable being tested had been removed.

$H_a$: ~~The mean temporal extent of studies using only "what, where, when" data is longer than the mean temporal extent of studies using richer data types.~~  The use of more complex data types will reduce the mean temporal extent of studies.  

$H_0$: ~~The mean temporal extent of studies using what, where, when-only data is the same as the mean temporal extent of studies using richer data types.~~  Studies using more complex data types do not have shorter mean temporal extents than studies that do not use those data types.  
 
```{r prepare_temporal_extent_df}
temp_extent <- select(
  wg, c(title, temp_extent, 
        # data.type...what.where.when.only, # alpha too low
        data.structure...sampling.effort.known, 
        # data.type...abundance, # alpha too low
        data.structure...non.detection,
        data.structure...organized.data.collection.scheme, 
        data.type...visit.specific.covariates, 
        data.structure...multiple.datasets.integrated.for.analysis
        # data.type...life.stage, # can't estimate alpha
        # data.type...voucher.of.some.kind.necessary.for.analysis # can't estimate alpha
        )) %>%
  gather(key = "data_type", value = "dt_value", 
         data.structure...sampling.effort.known:data.structure...multiple.datasets.integrated.for.analysis, 
         factor_key = T) %>%
  filter(dt_value == T) %>%
  group_by(data_type)

# make names prettier
temp_extent$data_type <- gsub("data.type...", "", temp_extent$data_type)
temp_extent$data_type <- gsub("data.structure...", "", temp_extent$data_type)

temp_extent$data_type <- factor(as.character(temp_extent$data_type), 
                            levels = c("sampling.effort.known",
                                       "non.detection", 
                                       "organized.data.collection.scheme", 
                                       "visit.specific.covariates", 
                                       "multiple.datasets.integrated.for.analysis"), 
                            labels = c("sampling effort known",
                                       "non detection", 
                                       "organized scheme", 
                                       "visit specific covariates", 
                                       "multiple datasets"))
```


```{r fit_temp_extent_lm}
## fit linear regression with log-transformed years as response variable
## and each data type as a separate variable 
# fit full model
wg$log_years <- log(wg$temp_extent)
# variables added based on a-priori expectation of most to least important
time_lm_full <- lm(log_years ~ 1 + 
                     data.structure...sampling.effort.known +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis,
                   data = wg[!is.na(wg$log_years), ], 
                   na.action = na.exclude, y = TRUE) 

time_resids <- rstandard(time_lm_full) # calculate residuals

time_summary <- summary(time_lm_full)
temp_extent_confint <- confint(time_lm_full)

## to interpret outcome on original scale:
# for a one-unit increase in x, the predicted value of y 
# changes by (e^beta1 - 1)*100 percent
```

```{r time_data_type_full_model_sig_test}
# Test significance of full model
m0 <- lm(log_years ~ 1, data = wg[!is.na(wg$log_years), ], y = T) # fit null model 
m_time_full_anova <- anova(m0, time_lm_full)
```

```{r time_data_type_test_main_effects, include = F}
## test effect of each data type predictor by comparing the full model to a
# model with that predictor removed
# 
# TODO: Does this need multiple comparison correction?

m_no_effort <- lm(log_years ~ 1 + 
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis,
                  data = wg[!is.na(wg$log_years), ])
m_no_nonDet <- lm(log_years ~ 1 + 
                     data.structure...sampling.effort.known + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis,
                  data = wg[!is.na(wg$log_years), ])
m_no_scheme <- lm(log_years ~ 1 + 
                     data.structure...sampling.effort.known + 
                     data.structure...non.detection + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis,
                  data = wg[!is.na(wg$log_years), ])
m_no_visitCovs <- lm(log_years ~ 1 + 
                     data.structure...sampling.effort.known + 
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.structure...multiple.datasets.integrated.for.analysis,
                     data = wg[!is.na(wg$log_years), ])
m_no_multData <- lm(log_years ~ 1 + 
                     data.structure...sampling.effort.known + 
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates,
                    data = wg[!is.na(wg$log_years), ])

indiv_mods_time <- list("m_no_effort" = m_no_effort, "m_no_nonDet" = m_no_nonDet, 
                        "m_no_scheme" = m_no_scheme, 
                        "m_no_visitCovs" = m_no_visitCovs, 
                        "m_no_multData" = m_no_multData)
```

```{r temp_extent_predictions}
temp_extent_preds <- data.frame(
  "data.structure...sampling.effort.known" = c(T,F,F,F,F), 
  "data.structure...non.detection" = c(F, T, F, F, F), 
  "data.structure...organized.data.collection.scheme" = c(F, F, T, F, F), 
  "data.type...visit.specific.covariates" = c(F,F,F,T,F), 
  "data.structure...multiple.datasets.integrated.for.analysis" = c(F,F,F,F,T))
temp_extent_preds$pred_log_years <- predict(time_lm_full, 
                                            newdata = temp_extent_preds)
temp_extent_preds$pred_log_y_lowerCI <- predict(time_lm_full, 
                                            newdata = temp_extent_preds, 
                                            interval = "confidence", 
                                            level = 0.95)[, "lwr"]
temp_extent_preds$pred_log_y_upperCI <- predict(time_lm_full, 
                                            newdata = temp_extent_preds, 
                                            interval = "confidence", 
                                            level = 0.95)[, "upr"]
temp_extent_preds$pred_years <- exp(temp_extent_preds$pred_log_years)
temp_extent_preds$pred_years_upperCI <- exp(temp_extent_preds$pred_log_y_upperCI)
temp_extent_preds$pred_years_lowerCI <- exp(temp_extent_preds$pred_log_y_lowerCI)
```


## What is the most popular study question paradigm for studies using biological records?

We categorized study questions into two broad question paradigms: individual species questions and community questions [Appendix 2].  We estimated the proportion of studies that focused on each of the two broad study question paradigms, and used bias-corrected accelerated bootstrap confidence intervals [@Efron1993] to estimate 95% confidence intervals around the point estimates of the proportions.  

```{r paradigm_bootstrap}
proportion_fun <- function(x, indices) {
  # ARGS: x - a vector of TRUE/FALSE values, for which the TRUEs will be counted
  #       indices - a vector of indices as required by boot
  x_b <- x[indices]
  sum(as.numeric(x_b), na.rm = TRUE)/length(x_b)
}

count_fun <- function(x, indices) {
  # ARGS: x - a vector of TRUE/FALSE values, for which the TRUEs will be counted
  #       indices - a vector of indices as required by boot
  x_b <- x[indices]
  sum(as.numeric(x_b), na.rm = TRUE)
}

get_boot_CI <- function(x, fun_to_use) {
  b_o <- boot(x, fun_to_use, R = 10000, stype = "i")
  ci_o <- tryCatch(boot.ci(b_o, conf = 0.95, type = "bca"), 
                   error = function(x) NA)
  to_return <- list(obs = x, 
                    boot_obj = b_o, 
                    ci_obj = ci_o)
  to_return
}

proportion_individual <- get_boot_CI(wg$individual.species.question,
                                     fun_to_use = proportion_fun)
```

## Ecological focus

We assessed which ecological or biological phenomena were used as the response variable in analyses using biological records.  We coded each study based on whether it used the following response variables: species distribution; species abundance; phenology, species richness (*alpha*-diversity or the number of species present); species turnover (*beta*-diversity or any measure that incorporated species identity and/or abundance in addition to the number of species).  A study could use more than one of these response variables if it contained more than one analysis using biological records. After coding, we created an "other" response variable category which was coded as "TRUE" for studies that did not use any of the other biological or ecological response variables that we searched for during coding.  We used bias-corrected accelerated bootstrap confidence intervals with 10,000 bootstrap replicates [@Efron1993] to estimate 95% confidence intervals around the number of studies investigating each response variable. We determined statistical significance of differences in the number of studies investigating each type of response by assessing whether the 95% bootstrap confidence intervals overlapped.  

$H_0$: There is no difference in the number of studies investigating each response.  

$H_a$: Some responses are more commonly investigated than others. 

```{r make_other_response_variable_columns}
# make columns indicating which studies analyze something that does not fit
# within the response variable categories I am using.  I do this by finding
# studies that do not have any of my response variables marked TRUE, which means
# they must have been doing something else.
resp_var_df <- select(wg, title, 
                      response.variable...species.richness,
                      response.variable...distribution, 
                      response.variable...abundance,
                      response.variable...phenology) 
# make column indicating individual species studies that don't use one of the 
# 4 main response variables
resp_var_df$other_resp <- 
  resp_var_df$response.variable...distribution == F & 
  resp_var_df$response.variable...abundance == F & 
  resp_var_df$response.variable...phenology == F  & 
  resp_var_df$response.variable...species.richness == F
```

```{r boot_response_variable}
resp_var_list <- lapply(
  list(sp_rich = resp_var_df$response.variable...species.richness, 
       dist = resp_var_df$response.variable...distribution,
       abund = resp_var_df$response.variable...abundance, 
       phen = resp_var_df$response.variable...phenology, 
       other = resp_var_df$other_resp),
  FUN = get_boot_CI, fun_to_use = proportion_fun)

resp_var_list_counts <- lapply(
  list(sp_rich = resp_var_df$response.variable...species.richness, 
       dist = resp_var_df$response.variable...distribution,
       abund = resp_var_df$response.variable...abundance, 
       phen = resp_var_df$response.variable...phenology, 
       other = resp_var_df$other_resp),
  FUN = get_boot_CI, fun_to_use = count_fun)
```

## Development of methodology across different study questions

We asked whether methodological development is more active for some types of ecological questions than for others.  To test for differences in the proportion of methodology studies across the different ecological study focuses, we used logistic regression with six binary predictor variables indicating the specific biological or ecological focus(es) of the study and a binary response variable indicating whether the study addressed a methodological question (e.g. development of a new method or analysis of the performance of existing methods).  To evaluate whether assumptions were met for using logistic regression, we evaluated the number of samples per predictor variable, checked that all levels of predictor variables had more than five cases, checked for multicollinearity using the variance inflation factors [@Fox1992], performed a Hosmer-Lemeshow goodness of fit test, and identified influential points using Pearson and deviance residuals and leverage.  

$H_0$: There is no effect of the ecological focus of a study on the probability of the study addressing methodological questions.  

$H_a$: The ecological focus of a study affects the probability of the study addressing methodological questions.  

```{r methodology_by_study_focus_mod}
# make an "other" response variable / study focus column on wg data frame
wg$other_focus <- wg$response.variable...abundance == F & 
  wg$response.variable...distribution == F & 
  wg$response.variable...phenology == F & 
  wg$response.variable...species.richness == F & 
  wg$trends.over.time == F

# fit glm
method_mod <- glm(factor(as.character(methodology.development.or.analysis), 
                         levels = c("FALSE", "TRUE"), 
                         labels = c("FALSE", "TRUE")) ~ 1 +
                    factor(response.variable...species.richness) + 
                    factor(response.variable...distribution) + 
                    factor(response.variable...abundance) + 
                    factor(response.variable...phenology) + 
                    factor(trends.over.time) + 
                    factor(other_focus), 
                  data = wg, family = binomial())
method_mod_null <- glm(factor(as.character(methodology.development.or.analysis), 
                         levels = c("FALSE", "TRUE"), 
                         labels = c("FALSE", "TRUE")) ~ 1, 
                  data = wg, family = binomial())
```

```{r print_method_focus_model, include = F}
method_chisq <- anova(method_mod_null, method_mod, test = "Chisq")
```

## What data types are used for each study question? 

We asked whether different types of biological records data are used to study different types of questions.  We fit Poisson regression models of the counts (number of studies) for each combination of data type and ecological study focus, fitting both a saturated model and an independence model with only main effects.  We used a Chi-squared likelihood ratio test to compare the saturated and independence models. 

$H_a$: Analyses of different ecological study questions use different types of biological records data.

$H_0$: Analyses of different ecological study questions do not use different types of biological records data.

The response is the cell counts in the contingency table in section 2.5 (Table 5 I think).  I think an assumption of the Poisson regression is that the cell counts are independent, meaning that the count in one cell doesn’t go up or down with the count in another cell. So I need to asses if there is any dependence between the levels of each categorical variable. For example, does the count in the “sampling effort x community” cell go up in correlation with the count in the “abundance x community” cell? Could I use some kind of permutation approach to test for independence of cell counts? **Dina, how should I check to make sure cell counts are independent?** I could perhaps do this using a permutation procedure to look for non-random patterns in the data types.  For each pair of data type variables, I could a table in which each study is a row and there is a column for each of the two data type variables being assessed, and a 1 or a 0 for whether the study had the data type.  I could then take the row sums and look at the distribution of row sums of 0 (studies using neither of the two data types being tested), 1 (studies using only 1 of the two data types), and 2 (studies using both of the two data types).  I could then randomly permute one of the two columns, breaking any associations between the two data types, and take row sums again, getting a distribution of row sums under the null hypothesis.   

**Dina, how should I check other model assumptions for the Poisson regression?**


```{r data_type_paradigm_df_prepare}
# make data frame with counts of data type by study paradigm
dt_pd_df <- select(wg, data.structure...organized.data.collection.scheme, 
                   data.structure...sampling.effort.known,
                   data.structure...non.detection,
                   data.structure...multiple.datasets.integrated.for.analysis,
                   data.type...visit.specific.covariates, 
                   response.variable...distribution,
                   response.variable...phenology, 
                   response.variable...abundance, 
                   response.variable...species.richness, 
                   trends.over.time, 
                   other_focus) %>%
  gather(key = "data_type", value = "dt_val", 
         data.structure...organized.data.collection.scheme:
           data.type...visit.specific.covariates) %>%
  filter(dt_val == TRUE) %>%
  gather(key = "study_focus", value = "focus_val", 
         response.variable...distribution:other_focus) %>%
  filter(focus_val == TRUE) %>%
  count(study_focus, data_type) %>%
  complete(study_focus, data_type, fill = list(n = 0))

# clean character strings
dt_pd_df$study_focus <- gsub("response.variable...", "", dt_pd_df$study_focus)
dt_pd_df$data_type <- gsub("data.structure...", "", dt_pd_df$data_type)
dt_pd_df$data_type <- gsub("data.type...", "", dt_pd_df$data_type)

# spread for easy visualisation of 2-way table
dt_pd_df_wide <- spread(dt_pd_df, key = study_focus, value = n)
```

```{r data_type_paradigm_model}
dt_pd_mod_full <- glm(n ~ 1 + study_focus + data_type + study_focus:data_type, 
                      data = dt_pd_df, family = poisson(link = log))
dt_pd_mod_ind <- glm(n ~ 1 + study_focus + data_type, 
                     data = dt_pd_df, family = poisson(link = log))

# Do likelihood ratio test for full model vs. independence model
dt_pd_interaction_test <- anova(dt_pd_mod_ind, dt_pd_mod_full,  test = "Chisq")



# look at predicted values compared to observed values
dt_pd_df$predicted <- dt_pd_mod_ind$fitted.values

# get residuals from independence model
dt_pd_df$resids <- residuals(dt_pd_mod_ind)
# show big residuals.  This is very interesting b/c it looks like the worst
# places where the independence model doesn't fit have to do with when 
# multiple data types are integrated for analysis.  So that might be the data
# type for which the study questions are different.  Specifically, people study
# abundance way less than expected with that data type, and they study 
# distribution more than expected with that data type.

# dt_pd_df[abs(dt_pd_df$resids) > 2, ] 

# Looking at resids > 1 shows a similar story.  Studies of abundance use 
# structured data types (non detection, sampling effort, etc) more than expected,
# while studies of distributions use structured data types less than expected.

# dt_pd_df[abs(dt_pd_df$resids) > 1, ] 
```

## Analysis approach & data type

To investigate whether the data type affects the analysis approach, we used random permutations to test the null hypothesis of no difference in the types of data used with different analysis approaches.  We identified the analysis approach of each study as inference, prediction, or descriptive analyses only.  The data type categories we tested based on *a-priori* expectations of importance were: sampling effort; non-detection; organized monitoring scheme; visit-specific covariates; and multiple datasets integrated for analysis.  In the ranom permutation test, we broke the association between data type and analysis approach by randomly shuffling the analysis approach results.  We preserved associations between analysis approaches by preserving the rows (and therefore associations) within the analysis approach variables, and we preserved associations between data types by preserving the rows (and therefore associations) within the data type variables (Fig. S??).  We performed 10,000 permutations.  To test for an overall interaction between data type and analysis approach, we calculated a chi-squared test statistic for each permuted dataset and for the observed dataset, and calculated the significance level as the proportion of chi-squared statistic values from the randomized datasets that were higher than the chi-squared statistic for the observed dataset.  To determine which combinations of data type and analysis approach were found more or less often than expected by chance, we counted the number of studies using each combination of analysis approach and data type for the permuted datasets and for the observed data.  We calculated residuals for each cell of the contingency table by subtracting the median count for each cell in the permuted data contingency tables from the observed cell count for the same cells. We calculated permutation significance levels for each cell by finding the proportion of the cell counts from the permuted datasets that were more extreme than the observed count for each cell.  Small permutation significance levels for a cell indicate that the observed number of studies in that cell was unlikely if there was no relationship between that data type and that analysis approach.


```{r data_type_analysis_approach_df}
dt_an_df <- select(wg, data.structure...organized.data.collection.scheme, 
                   data.structure...sampling.effort.known,
                   data.structure...non.detection,
                   data.structure...multiple.datasets.integrated.for.analysis,
                   data.type...visit.specific.covariates, 
                   results.type...inference, 
                   results.type...prediction, 
                   results.type...descriptive.only) %>%
  gather(key = "data_type", value = "dt_val", 
         data.structure...organized.data.collection.scheme:
           data.type...visit.specific.covariates) %>%
  filter(dt_val == TRUE) %>%
  gather(key = "analysis_approach", value = "an_val", results.type...inference,
         results.type...prediction, results.type...descriptive.only) %>%
  filter(an_val == TRUE) %>%
  count(analysis_approach, data_type) %>%
  complete(analysis_approach, data_type, fill = list(n = 0))

# clean character strings
dt_an_df$data_type <- gsub("data.structure...", "", dt_an_df$data_type)
dt_an_df$data_type <- gsub("data.type...", "", dt_an_df$data_type)
dt_an_df$analysis_approach <- gsub("results.type...", "", dt_an_df$analysis_approach)

# spread for easy visualisation of 2-way table
dt_an_df_wide <- spread(dt_an_df, key = analysis_approach, value = n)
dt_an_df_wide$data_type <- gsub("\\.", " ", dt_an_df_wide$data_type)
```

```{r data_type_analysis_approach_model}
# fit saturated model
dt_an_mod_full <- glm(n ~ 1 + analysis_approach + data_type + 
                        analysis_approach:data_type, data = dt_an_df, 
                      family = poisson(link = log))
# fit model assuming independence between analysis approach and data type
dt_an_mod_ind <- glm(n ~ 1 + analysis_approach + data_type, 
                     data = dt_an_df, family = poisson(link = log))
dt_an_mod_independence_test <- anova(dt_an_mod_ind, dt_an_mod_full, test = "Chisq")

dt_an_df$predicted <- dt_an_mod_ind$fitted.values
dt_an_df$resids <- residuals(dt_an_mod_ind)
```

```{r data_analysis_approach_randomization}
# randomly re-shuffle analysis strategies (but keep them associated with each
# other in the same way), and keep data types associated with each other.
# This will give H_0 counts for each cell.
n_perm <- 10000 # number of permutations to perform
rand_array <- array(NA, c(5, 3, n_perm))

for(i in 1:n_perm-1) {
  ind <- sample(1:nrow(wg), replace = F)
  # make data frame of data types
  df_dat <- wg[, grepl(
    ".*multiple.dat.*|.*non.det.*|.*organized.*|.*sampling.eff.*|.*visit.speci.*", colnames(wg))]
  # make data frame of analysis methods
  df_an <- wg[, grepl(".*descriptive.on.*|.*inferen.*|.prediction.*", 
                      colnames(wg))]
  # permute analysis methods
  df_an <- df_an[ind, ]
  df <- cbind(df_dat, df_an)
  
  df <- gather(df, key = "data_type", value = "dt_val", 
               data.structure...organized.data.collection.scheme:
                 data.type...visit.specific.covariates) %>%
    filter(dt_val == TRUE) %>%
    gather(key = "analysis_approach", value = "an_val", results.type...inference,
           results.type...prediction, results.type...descriptive.only) %>%
    filter(an_val == TRUE) %>%
    count(analysis_approach, data_type) %>%
    complete(analysis_approach, data_type, fill = list(n = 0))
  
  # clean character strings
  df$data_type <- gsub("data.structure...", "", df$data_type)
  df$data_type <- gsub("data.type...", "", df$data_type)
  df$analysis_approach <- gsub("results.type...", "", df$analysis_approach)
  
  # spread for easy visualisation of 2-way table
  df <- spread(df, key = analysis_approach, value = n)
  df$data_type <- gsub("\\.", " ", df$data_type) 
  rand_array[, , i] <- as.matrix(df[, 2:4])
}

# add observed values as one permutation
rand_array[, , n_perm] <- as.matrix(dt_an_df_wide[, 2:ncol(dt_an_df_wide)])


# get permutation chi-squared p-value to test for overall interaction
# See Manley (2007) p. 313
chis_perm <- c()
for(k in 1:n_perm) {
  chis_perm[k] <- chisq.test(rand_array[, , k])$statistic
}
chis_perm_fun <- ecdf(chis_perm)
# calculate permutation p-value for observed chi-squared test statistic
chis_perm_pvalue <- 1 - chis_perm_fun(chisq.test(dt_an_df_wide[, 2:4])$statistic)
# double check using chisq.test.  This gives a different p-value by an order
# of magnitude (though still small).  I think this must be because the built-in
# MC method does not preserve correlations within each variable in the way my
# randomization process does. 
# chisq.test(dt_an_df_wide[, 2:4], simulate.p.value = TRUE, B = n_perm)


# make empty template df to hold cell p-values later
perm_p_vals <- dt_an_df_wide
perm_p_vals[, 2:4] <- NA # clear counts
# make empty template df to hold residuals later
perm_resids <- dt_an_df_wide
perm_resids[, 2:4] <- NA # clear counts 
# make template df to hold expected counts later
perm_exp_counts <- dt_an_df_wide
perm_exp_counts[, 2:4] <- NA # clear counts

for(i in 1:nrow(perm_p_vals)) {
  for(j in 1:(ncol(perm_p_vals) - 1)) {
    emp_dist <- ecdf(rand_array[i, j, ])
    # calculate empirical p-value
    perm_p_vals[i, j + 1] <- emp_dist(dt_an_df_wide[i, j + 1])
    # make this a 2-tailed p-value
    if(perm_p_vals[i, j + 1] > 0.5) {
      perm_p_vals[i, j + 1] <- 1 - perm_p_vals[i, j + 1]
    }
    # Double the proportion to make it roughly two-tailed (i.e. the proportion
    # of values more extreme than the observed value)
    perm_p_vals[i, j + 1] <- 2 * perm_p_vals[i, j + 1] 
    # calculate residual
    perm_resids[i, j + 1] <- dt_an_df_wide[i, j + 1] - median(rand_array[i, j, ])
    # get expected counts
    perm_exp_counts[i, j + 1] <- median(rand_array[i, j, ])
  }
}
```



## Authors and Data Providing Institutions

To answer the question of who chooses to use biological records data, we used bootstrap confidence intervals to describe the proportion of studies that have an author associated with the institution that provided the data. 

# Results

```{r kripp_sum_cap}
kripp_sum_cap <- figs(name = "kripp_sum_cap", 
                      paste0("Reader agreement measured using Krippendorff's alpha, for variables for which the sample size is large enough to estimate Krippendorff's alpha.  Dashed and dotted horizontal lines show Krippendorff's alpha values of 0.8 and 0.67, which are recommended preferred and minimum values at which variables can be relied upon for analysis."))
```

## Search results 

The search returned `r nrow(elig)` potentially relevant studies, of which we judged `r length(which(wg$qualifies == TRUE))` (`r round((length(which(wg$qualifies == TRUE))/nrow(elig)) * 100, digits = 1)`%) to be eligible for inclusion in this review.  The number of potentially relevant studies returned by more than one search method was `r length(n_duplicate_titles)` (`r round((length(n_duplicate_titles)/nrow(elig))*100, digits = 1)`%).  Of the studies that we deemed eligible for inclusion in this review, `r length(which(wg$title %in% names(n_duplicate_titles)))` (`r round((length(which(wg$title %in% names(n_duplicate_titles)))/nrow(wg))*100, digits = 1)`%) were returned by more than one search method.  

One reader coded all `r nrow(wg)` studies and a second reader independently coded `r nrow(er)` of the studies. 

## Coding reliability

Most variables for which Krippendorff's *alpha* was estimated show that the coding is reliable enough to be used in analysis, at least for drawing tentative conclusions (Tab. 1, Fig. S1).  Many of the variables have Krippendorff's *alpha* values above the minimum required level of 0.67 but below the prefered level of 0.8, which indicates that they should be interpreted with caution.  The number of articles that have been evaluated by two readers was too small for reliable estimation of Krippendorff's alpha for some variables for which the smallest class is particularly uncommon (Tab. S1). 


```{r kripp_samp_size_cap}
kripp_samp_size_cap <- tabs(name = "kripp_samp_size_cap", 
                            paste0("Coding reliability for systematic review variables coded by two people for ", nrow(er), " studies.  Krippendorff's alpha values above 0.8 suggest that coding reliability is high enough to interpret variables with confidence, while values below 0.8 but above 0.667 suggest that coding reliability is only moderate and variables should be interpreted with caution.  Krippendorff's alpha values below 0.667 suggest that coding is unreliable and the variable should not be interpreted.  Coding reliability was estimated for all variables for which the least common class had enough observations to allow us to estimate with a confidence level of 0.1 whether Krippendorff's alpha was higher than 0.667."))
```

`r if(diag_present) tabs("kripp_samp_size_cap", display = "full")`

```{r calculate_proportion_T}
## This calculates the probability of the smallest class for each variable, for 
## use in calculating minimum sample size needed for Kripp's alpha for that 
## variable.  
## This treats all variables as factor variables.
prob_of_values <- lapply(
  wg[, which(colnames(wg) %in% names(kripp_all_dbl_coded))], 
  FUN = function(x) {
    x <- factor(x)
    mn <- 1
    for(j in 1:length(levels(x))) {
      prob <- length(which(x == levels(x)[j])) / length(which(!is.na(x)))
      if(prob < mn) mn <- prob
    }
    mn
  })

prob_of_values <- data.frame(variable = names(prob_of_values), 
                             prob_small_class = unlist(prob_of_values), 
                             row.names = NULL)
prob_of_values$kripp_alpha <- NA
for(i in 1:nrow(prob_of_values)) {
  # add krippendorff's alpha for codings
  prob_of_values$kripp_alpha[i] <- kripp_all_dbl_coded[[which(
    names(kripp_all_dbl_coded) == prob_of_values$variable[i])]]$value
}
prob_of_values <- prob_of_values[order(prob_of_values$kripp_alpha, 
                                       decreasing = T), ]

# On p 240 of Krippendorff book, it says that if acceptable alpha is 0.667 and 
# desired significance level is 0.1, then sample size needed is as follows:
#prob_of_values$n_25_adequate <- prob_of_values$prob_small_class > 0.25
#prob_of_values$n_30_adequate <- prob_of_values$prob_small_class > 0.2
prob_of_values$n_40_adequate <- prob_of_values$prob_small_class >= 0.14
# prob_of_values$n_50_adequate <- prob_of_values$prob_small_class >= 0.105 

## using 50 double-codings would only get us an additional 2 variables, and I
## don't think those are variables important enough to be worth the effort.

# Put vars with adequate sample size at top
prob_of_values <- prob_of_values[order(prob_of_values$n_40_adequate, 
                                       prob_of_values$kripp_alpha, 
                                       decreasing = TRUE),]

# list only those for which sample size is adequate
samp_adequate <- prob_of_values[which(prob_of_values$n_40_adequate == T), ]
rownames(samp_adequate) <- NULL
samp_adequate$variable <- gsub("\\.\\.\\.", " - ", samp_adequate$variable)
samp_adequate$variable <- gsub("\\.", " ", samp_adequate$variable)
kable(samp_adequate[order(samp_adequate$kripp_alpha, decreasing = T), -c(2, 4)], 
      col.names = c("Variable", "Krippendorff's alpha"),
      digits = 2)
rm(samp_adequate)
```

## Temporal extent of studies

```{r temp_extent_cap}
temp_extent_cap <- figs(name = "temp_extent_cap", 
                        "The temporal extent covered by studies using different types biological records data.  A linear regression showed that the log of the temporal extent of studies (in years) was significantly different for studies using different types of data.  This plot shows the observed temporal extent of studies (not fitted values from the linear regression model). Note the log-transformed y-axis.")
```

```{r print_time_individual_predictor_significance}
if(diag) {
  for(i in 1:length(indiv_mods_time)) { 
    print(names(indiv_mods_time)[i])
    print(anova(indiv_mods_time[[i]], time_lm_full))
  }
}
```

The overall linear regression model using data type to predict the natural log of the temporal extent of studies (in years) was  significant ($F_{`r format(round(m_time_full_anova[2, "Df"], digits = 2), scientific = F)`, `r format(round(m_time_full_anova[2, "Res.Df"], digits = 2), scientific = F)`}$ = `r format(round(m_time_full_anova[, "F"][2], digits = 2), scientific = F)`, *p* `r if(m_time_full_anova[6][[1]][2] >= 0.0001) {format(round(m_time_full_anova[6][[1]][2], digits = 4), scientific = T)} else "< 0.0001"`, Adjusted $R^2$ = `r format(round(time_summary$adj.r.squared, digits = 2), scientific = F)`, `r figs("temp_extent_cap", display = "cite")`).  Studies that used multiple different biological records datasets covered significantly longer temporal extents than studies that did not use multiple different datasets after accounting for other data types ($F_{1, 195}$ = 39.4, *p* < 0.0001, Fig. 1).  The expected temporal extent covered by a study that used multiple biological records datasets was `r round(exp((time_lm_full$coefficients[1] + time_lm_full$coefficients[6])), digits = 1)` years (95% CI: [`r round(temp_extent_preds$pred_years_lowerCI[5], digits = 1)`, `r round(temp_extent_preds$pred_years_upperCI[5], digits = 1)`]) when the data did not have any of the other data types we recorded.  Studies that used visit-specific covariates covered significantly shorter temporal extents than studies that did not, after accounting for other data types ($F_{1,195}$ = 25.8, *p* < 0.0001).  The expected temporal extent covered by a study that used visit-specific covariates was `r round(exp((time_lm_full$coefficients[1] + time_lm_full$coefficients[5])), digits = 1)` years (95% CI: [`r round(temp_extent_preds$pred_years_lowerCI[4], digits = 1)`, `r round(temp_extent_preds$pred_years_upperCI[4], digits = 1)`]) when the data did not have any of the other data types we recorded. We did not detect a significant effect of sampling effort data ($F_{1,195}$ = 0.2, *p* = 0.65), non-detection data ($F_{1,195}$ = 2.4, *p* = 0.13), or thedata coming from an organized monitoring scheme ($F_{1,195}$ = 3.6, *p* = 0.06) on the temporal extent of studies.  However, pairs plots and variance inflation factors show that the "sampling effort" and "non-detection" data types are strongly correlated with each other, and both are correlated less strongly with the "organized monitoring scheme" data type (Fig. S?, Tab. S1), suggesting that we may have been unable to dis-entangle the effects of each of these correlated variables with our current sample size (Fox 2016, p. 341-2).

```{r time_extent_results_plot, fig.cap=temp_extent_cap}
temp_extent_plot <- ggplot(data = temp_extent, 
                           aes(y = temp_extent, 
                               x = factor(data_type, 
                                          levels = c("sampling effort known", 
                                                     "non detection", 
                                                     "organized scheme", 
                                                     "visit specific covariates", 
                                                     "multiple datasets"), 
                                          labels = c("sampling effort\nknown", 
                                                     "non-detection", 
                                                     "organized\nscheme", 
                                                     "visit-specific\ncovariates", 
                                                     "multiple\ndatasets")))) + 
  geom_boxplot(varwidth = T) +
  scale_y_log10() + 
  xlab("Data Type") + 
  ylab("Temporal Extent of Study (years)") + 
  # annotate("text", x = 3, y = 2600, 
  #          label = "paste(\"Adjusted \", 
  #                italic(R) ^ 2, \" = 0.36\")", 
  #          parse = TRUE) + 
  # annotate("text", x = 3, y = 1200, 
  #          label = "paste(\"Overall model significance \", 
  #                italic(p), \" < 0.0001\")", 
  #          parse = TRUE) +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 35, vjust = 0.9, hjust = 1)) 
print(temp_extent_plot)
```

```{r time_extent_results_plot_for_presentation_slide, eval = F, include = F}
## This code is to be run in the console for making plot to be exported as jpg for slides
## TODO: need to hand-annotate R2 and p-value in plot
t_size = 22 # plot text size
ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + #, color = sig_different_from_WhWhWh
  geom_boxplot(varwidth = T) +
  scale_y_log10() + 
  xlab("Data Type") + 
  ylab("Temporal Extent of Study (years)") + 
  annotate("text", x = 3, y = 2000,
           label = "paste(\"Adjusted \",
                 italic(R) ^ 2, \" = 0.39\")",
           parse = TRUE,
           size = t_size - 15) +
  annotate("text", x = 3, y = 5000, 
           label = "paste(\"Overall model significance \", 
                 italic(p), \" < 0.0001\")", 
           parse = TRUE, 
           size = t_size - 14) +
  theme_bw() + 
  scale_color_discrete(name = "Significantly Different from\nWhat, Where, When-only\nstudies (alpha = 0.1 level)") + 
  theme(axis.text.x = element_text(angle = 330, vjust = 1, hjust = 0), 
        text = element_text(size = t_size)) 
```


## What is the most popular study question paradigm?

The proportion of studies asking questions about individual species was `r round(proportion_individual$boot_obj$t0, digits = 2)` (95% boostrap CI: [`r round(proportion_individual$ci_obj$bca[4], digits = 2)`, `r round(proportion_individual$ci_obj$bca[5], digits = 2)`]).  Coder agreement was low when determining whether studies were asking questions about communities (Krippendorff's alpha = 0.65, Tab. ??), which prevented us from interpreting that variable, and therefore prevented a comparison between the proportion of studies that were focused on individual species and the proportion of studies that focused on communities.  

## Ecological focus

```{r resp_var_tab_cap}
resp_var_tab_cap <- tabs(name = "resp_var_tab_cap", 
                         "The number of studies (with lower and upper 95% bootstrap confidence intervals) focusing on each type of ecological or biological response.")
```

```{r resp_var_plot_cap}
resp_var_plot_cap <- figs(name = "resp_var_plot_cap", 
                     "The proportion of studies analyzing different ecological responses.  Points show proportion of studies, vertical lines show 95% bootstrap confidence intervals.  Proportions add up to more than one because some studies performed multiple analyses and used multiple different response variables.")
```

Over half the studies in this review analyzed species distribution as a response variable (proportion of studies = `r round(resp_var_list$dist$boot_obj$t0, digits = 2)` studies, 95% bootstrap CI [`r round(resp_var_list$dist$ci_obj$bca[4], digits = 2)`, `r round(resp_var_list$dist$ci_obj$bca[5], digits = 2)`], Fig. ??, Tab. S?).  The proportion of studies analyzing species distributions was significantly higher than the proportion analyzing any other response variable based on non-overlapping 95% bootstrap confidence intervals (Fig. ??, Tab. S?).  The second most common response variable for studies in this review was abundance (proportion of studies = `r round(resp_var_list$abund$boot_obj$t0, digits = 2)` studies, 95% bootstrap CI [`r round(resp_var_list$abund$ci_obj$bca[4], digits = 2)`, `r round(resp_var_list$abund$ci_obj$bca[5], digits = 2)`]), but the proportion of studies analyzing abundance was not significantly different from the proportion analyzing species richness (proportion of studies = `r round(resp_var_list$sp_rich$boot_obj$t0, digits = 2)` studies, 95% bootstrap CI [`r round(resp_var_list$sp_rich$ci_obj$bca[4], digits = 2)`, `r round(resp_var_list$sp_rich$ci_obj$bca[5], digits = 2)`]), phenology (proportion of studies = `r round(resp_var_list$phen$boot_obj$t0, digits = 2)` studies, 95% bootstrap CI [`r round(resp_var_list$phen$ci_obj$bca[4], digits = 2)`, `r round(resp_var_list$phen$ci_obj$bca[5], digits = 2)`]), or other response variables (proportion of studies = `r round(resp_var_list$other$boot_obj$t0, digits = 2)` studies, 95% bootstrap CI [`r round(resp_var_list$other$ci_obj$bca[4], digits = 2)`, `r round(resp_var_list$other$ci_obj$bca[5], digits = 2)`]) based on overlapping 95% bootstrap confidence intervals.  Too few studies were classified as having "diversity" as a response variable to estimate coder agreement using Krippendorff's alpha.  Therefore, we did not evaluate the diversity response variable category on its own, but studies that had been coded as using diversity and no other response variables are included in the "other" response variable category.  

```{r make_response_variable_df}
resp_count_df <- data.frame(matrix(nrow = length(resp_var_list), 
                                        ncol = 5))
colnames(resp_count_df) <- c("response_variable", "n_studies", "prop_studies",
                             "l_95ci", "u_95ci")

# fill response variable counts data frame using the list of bootstrap results
for(i in 1:length(resp_var_list)) {
  resp_count_df[i, "response_variable"] <- names(resp_var_list)[i]
  resp_count_df[i, "n_studies"] <- resp_var_list_counts[[i]]$ci_obj$t0
  resp_count_df[i, "prop_studies"] <- resp_var_list[[i]]$ci_obj$t0
  resp_count_df[i, "l_95ci"] <- resp_var_list[[i]]$ci_obj$bca[4]
  resp_count_df[i, "u_95ci"] <- resp_var_list[[i]]$ci_obj$bca[5]
}

# clean response variable strings
resp_count_df$response_variable <- gsub(".*sp_rich.*", "species richness", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*other.*", "other", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*dist.*", "species distribution", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*abund.*", "abundance", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*phen.*", "phenology", 
                                        resp_count_df$response_variable)
```


```{r resp_var_by_paradigm_plot, fig.cap=resp_var_plot_cap}
# plot proportion of studies using each response variable, with 95% CIs
print(ggplot(data = resp_count_df, 
             aes(x = factor(response_variable, 
                            levels = c("species distribution", 
                                       "phenology", 
                                       "abundance",
                                       "species richness", 
                                       "other")), 
                 y = prop_studies)) + 
        geom_point() + 
        geom_linerange(aes(x = factor(response_variable, 
                            levels = c("species distribution", 
                                       "phenology", 
                                       "abundance",
                                       "species richness", 
                                       "diversity (turnover)", 
                                       "other")), 
                           ymin = l_95ci, 
                           ymax = u_95ci)) + 
        xlab("") + 
        ylab("Proportion of Studies") + 
        ylim(c(0, 1)) + 
        theme_bw() + 
        theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1), 
              strip.background = element_blank(), strip.placement = "outside"))
      # theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)))
```

## Development of methodology across different study questions

```{r method_study_focus_cap}
method_study_focus_cap <- figs(name = "method_study_focus_cap", 
                               "The coefficient estimates from a logistic regression testing whether studies developing or testing methodology make up a larger proportion of total studies studies for some types of questions than for others.  Positive coefficient estimates mean the probability of a study developing or testing methodology is higher for a study addressing that ecological study question; negative coefficient estimates mean the probability of a study developing or testing methodology is lower for a study addressing that ecological question.  Confidence intervals (vertical lines) that overlap 0 indicate that the coefficient estimate is not significantly different from zero.")
```

The overall model predicting the probability of a study addressing a methodological question based on the ecological focus of the study was not significant (Intercept-only model residual deviance = `r method_chisq$'Resid. Dev'[1]`, full model residual deviance = `r method_chisq$'Resid. Dev'[2]`, Chi-squared test *p* = `r round(anova(method_mod, method_mod_null, test = "Chisq")[2, 5], 2)`).  

```{r plot_method_study_focus_coefficients, fig.cap=method_study_focus_cap}
meth_df <- data.frame(matrix(nrow = 6, ncol = 4))
colnames(meth_df) <- c("variable", "coef_estimate", "l_bound", "h_bound")
meth_df$variable <- c("species richness", "diversity (turnover)", 
                      "species disribution", "abundance", "phenology", 
                      "temporal trends")
cis <- confint(method_mod)
for(i in 1:nrow(meth_df)) {
  # get coefficient point estimate
  meth_df$coef_estimate[i] <- summary(method_mod)$coefficients[1 + i]
  meth_df$l_bound[i] <- cis[1 + i, "2.5 %"]
  meth_df$h_bound[i] <- cis[1 + i, "97.5 %"]
}

# make graph
print(ggplot(data = meth_df, 
             aes(x = factor(variable, 
                            levels = c("species richness", 
                                       "diversity (turnover)", 
                                       "species disribution", 
                                       "abundance", 
                                       "phenology", 
                                       "temporal trends")), 
                 y = coef_estimate)) + 
        geom_point() + 
        geom_linerange(aes(x = factor(variable, 
                            levels = c("species richness", 
                                       "diversity (turnover)", 
                                       "species disribution", 
                                       "abundance", 
                                       "phenology", 
                                       "temporal trends")), 
                           ymin = l_bound, 
                           ymax = h_bound)) + 
        xlab("Study question") + 
        ylab("Coefficient estimate") + 
        theme_bw() + 
        ylim(-2, 2) +
        theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)))
```


## What data types are used for each type of ecological study question?

A Chi-squared likelihood ratio test comparing the saturated Poisson regression model to a reduced model with only main effects for data type and ecological study focus (but no interaction term) rejected the null hypothesis that the reduced model fit the data as well as the full model (*p* = `r round(dt_pd_interaction_test$"Pr(>Chi)"[2], 2)`).  Thus, there is evidence that the use of data types differs between the different study questions.  Two cells had residuals with an absolute value bigger than two (Tab ??) - the cell for the number of studies investigating abundance using multiple biological datasets (residual = -3.7), and the cell for the number of studies investigating species distributions using multiple biological records datasets (residual = 3).  Furthermore, looking at all cells for which the absolute value of residuals was larger than one (Tab ??) showed a pattern in which residuals were positive for combinations of structured data types and study questions about abundance, and negative for combinations of structured data types and species distribution study questions.  This suggests that abundance studies use structured data types more often than would be expected if data type and study question were not related, and distribution studies use structured data types less often than would be expected if study question and data type were not related. 

```{r data_type_study_question_graph}
ggplot(data = dt_pd_df, 
       aes(x = factor(study_focus, 
                      levels = c("abundance", "distribution", "phenology", 
                                 "species.richness", "trends.over.time", 
                                 "other_focus"), 
                      labels = c("abundance", "distribution", "phenology", 
                                 "species\nrichness", "trends\nover\ntime", 
                                 "other\nfocus")), 
           y = factor(data_type,
                      levels = c("multiple.datasets.integrated.for.analysis", 
                                 "sampling.effort.known", 
                                 "non.detection",
                                 "visit.specific.covariates",
                                 "organized.data.collection.scheme"), 
                      labels = c("multiple\ndatasets", 
                                 "sampling effort", 
                                 "non detection", 
                                 "visit-specific\ncovariates",
                                 "organized\nscheme")))) + 
  geom_tile(aes(fill = resids)) + 
  xlab("Study Question") + 
  ylab("Data Type") + 
  geom_text(aes(label = round(resids, digits = 1))) + # write the values
  scale_fill_gradient2(low = "blue", mid = "white", high = "orange") + 
  theme(panel.grid.major.x=element_blank(), #no gridlines
        panel.grid.minor.x=element_blank(), 
        panel.grid.major.y=element_blank(), 
        panel.grid.minor.y=element_blank(),
        panel.background=element_rect(fill="white"), # background=white
        axis.text.x = element_text(angle=30, hjust = 1, vjust=1),
        axis.text.y = element_text(angle=45)) + 
  labs(fill = "Poisson\nresiduals") + 
  ggtitle("Residuals from Poisson\nindependence model\nData Type by Study Question")
```


## Analysis approach & data type

```{r data_type_analysis_tab_cap}
data_type_analysis_tab_cap <- tabs(name = "data_type_analysis_tab_cap", 
                                   "Table of counts of studies using each data type and analysis approach.")
```

```{r data_type_analysis_cap}
data_type_analysis_cap <- figs(name = "data_type_analysis_cap", 
                            "The number of studies using different types of biological records data as part of different analysis approaches.  Analysis approaches are inference (e.g. *p*-values or confidence intervals), prediction (e.g. prediction of species presence or absence into unsampled areas) and description (maps, graphs, narrative descriptions, or descriptive statistics with no inference or prediction).")
```

There was strong evidence that data type and analysis approach are not independent (permutation-based significance level of the chi-squared test statistic for the observed data = `r chis_perm_pvalue` with `r n_perm` permutations).  Residuals and permutation significance levels for each combination of data type and analysis method show that residuals are generally large, negative, and highly significantly different from zero for combinations of more structured or "rich" data types and descriptive-only analyses (Fig. ??), suggesting that those rich data types are analyzed descriptively less often than would be expected if there was no relationship between data type and analysis approach.  Residuals are generally large, positive, and highly significantly different from zero for combinations of "rich" data types and inferential analyses, suggesting that those rich data types are analyzed with inferential methods more frequently than would be expected if there were no relationship between data type and analysis approach.  Notably, the absolute value of residuals for combinations of data with visit-specific covariates and both descriptive-only and inferential studies are small and not significantly different from zero, despite visit-specific covariates being a relatively "rich" additional type of data to have associated with biological records.  The absolute value of residuals and significance levels for combinations of all data types with predictive analysis methods generally indicated that the use of data types in predictive analyses did not differ strongly from what would be expected if there were no relationship between data type and analysis approach.  The residuals and significance levels for the cells associated with multiple data sets being used in an analysis suggest that multiple datasets are less often for inference and more often for descriptive-only analyses than would be expected if there was no relationship between data type and analysis approach, though both the absolute magnitued of the residuals and the significance levels suggested that the observed use of multiple datasets for those two analysis approaches differed less strongly from the null expectation than did the use of data with non-detection and/or sampling effort information and data from organized monitoring schemes.  

`r if(diag_present) tabs("data_type_analysis_tab_cap", display = "full")`

```{r print_data_type_analysis_approach_contingency_table}
if(diag_present) {
    colnames(dt_an_df_wide) <- gsub(
    "results.type\\.\\.\\.", "", 
    colnames(dt_an_df_wide))
  colnames(dt_an_df_wide) <- gsub("\\.", " ", colnames(dt_an_df_wide))
  dt_an_df_wide$data_type <- gsub(
    "data\\.structure\\.\\.\\.|data\\.type\\.\\.\\.", "",  
    dt_an_df_wide$data_type)
  dt_an_df_wide$data_type <- gsub("\\.", " ", dt_an_df_wide$data_type)

  kable(dt_an_df_wide)
}
```

```{r data_type_analysis_approach_graph, fig.cap=data_type_analysis_cap}
### plot data type by analysis approach ----------------------
# make plot, order factor levels by most important a-priori
# dt_an_df was created in the Methods section
ggplot(dt_an_df, 
       aes(x = factor(data_type, 
                      levels = c("sampling.effort.known", 
                                 "non.detection", 
                                 "organized.data.collection.scheme", 
                                 "multiple.datasets.integrated.for.analysis",
                                 "visit.specific.covariates"), 
                      labels = c("sampling effort", 
                                 "non detection", 
                                 "organized scheme", 
                                 "multiple datasets", 
                                 "visit-specific covariates")), 
           y = n, 
           fill = factor(data_type, 
                         levels = c("sampling.effort.known", 
                                    "non.detection", 
                                    "organized.data.collection.scheme", 
                                    "multiple.datasets.integrated.for.analysis", 
                                    "visit.specific.covariates"), 
                         labels = c("sampling effort", 
                                    "non detection", 
                                    "organized\nscheme", 
                                    "multiple\ndatasets", 
                                    "visit-specific\ncovariates")))) + 
  geom_bar(position = position_dodge(preserve = "single"), stat = "identity") + 
    facet_wrap( ~ factor(analysis_approach, 
                         levels = c("inference", 
                                    "prediction",
                                    "descriptive.only"),
                         labels = c("inference", "prediction",
                                    "description only")), 
                strip.position = "bottom") +
  ggtitle("Data used in different analysis approaches") + 
  scale_fill_discrete(name = "Data Type") + 
  xlab("Analysis Approach") + 
  ylab("Number of studies") + 
  scale_y_continuous(breaks = seq(0, max(dt_an_df$n) + 5, 5)) + 
  theme_bw() + 
  theme(axis.text.x = element_blank(), 
        strip.background = element_blank(), strip.placement = "outside")
```

```{r print_data_type_analysis_method_heatmap_table_permutation}
perm_p_vals_long <- gather(perm_p_vals, key = "analysis_approach", 
                           value = "significance", 2:4)
# ggplot(data = perm_p_vals_long,        
#        aes(x = factor(analysis_approach, 
#                       levels = c("descriptive.only", "inference", "prediction"), 
#                       labels = c("descriptive\nonly", 
#                                  "inference", "prediction")), 
#            y = factor(data_type,
#                       levels = c("multiple datasets integrated for analysis", 
#                                  "sampling effort known", 
#                                  "non detection",
#                                  "visit specific covariates",
#                                  "organized data collection scheme"), 
#                       labels = c("multiple\ndatasets", 
#                                  "sampling effort", 
#                                  "non detection", 
#                                  "visit-specific\ncovariates",
#                                  "organized\nscheme")))) + 
#   geom_tile(aes(fill = significance)) + 
#   xlab("Analysis Approach") + 
#   ylab("Data Type") + 
#   geom_text(aes(label = round(significance, 2))) + # write the values
#   scale_fill_gradient(low = "orange", high = "white") + 
#   theme(panel.grid.major.x=element_blank(), #no gridlines
#         panel.grid.minor.x=element_blank(), 
#         panel.grid.major.y=element_blank(), 
#         panel.grid.minor.y=element_blank(),
#         panel.background=element_rect(fill="white"), # background=white
#         axis.text.x = element_text(angle=30, hjust = 1, vjust=1),
#         axis.text.y = element_text(angle=45)) + 
#   labs(fill = "significance\nlevel") + 
#   ggtitle("Permutation-based significance levels")

perm_resids_long <- gather(perm_resids, key = "analysis_approach", 
                           value = "residual", 2:4)
# add significance levels to residuals data frame
perm_resids_long <- left_join(perm_resids_long, perm_p_vals_long, 
                              by = c("data_type", "analysis_approach"))

ggplot(data = perm_resids_long,        
       aes(x = factor(analysis_approach, 
                      levels = c("descriptive.only", "inference", "prediction"), 
                      labels = c("descriptive\nonly", 
                                 "inference", "prediction")), 
           y = factor(data_type,
                      levels = c("multiple datasets integrated for analysis", 
                                 "sampling effort known", 
                                 "non detection",
                                 "visit specific covariates",
                                 "organized data collection scheme"), 
                      labels = c("multiple\ndatasets", 
                                 "sampling effort", 
                                 "non detection", 
                                 "visit-specific\ncovariates",
                                 "organized\nscheme")))) + 
  geom_tile(aes(fill = residual)) + 
  xlab("Analysis Approach") + 
  ylab("Data Type") + 
  geom_text(aes(label = paste0(round(residual, digits = 1), " (", 
                               round(significance, digits = 2), ")"))) + # write the values
  scale_fill_gradient2(low = "#3366ff", mid = "white", high = "#ff751a") + 
  theme(panel.grid.major.x=element_blank(), #no gridlines
        panel.grid.minor.x=element_blank(), 
        panel.grid.major.y=element_blank(), 
        panel.grid.minor.y=element_blank(),
        panel.background=element_rect(fill="white"), # background=white
        axis.text.x = element_text(angle=30, hjust = 1, vjust=1),
        axis.text.y = element_text(angle=45)) +  
  # ggtitle("Residuals (significance level)\nfrom permutation test") + 
  labs(fill = "observed -\nexpected\ncounts")
```



## Authors and Data Providing Institutions

Coder agreement was low when determining whether a study had an author associated with the data providing institution (Krippendorff's alpha = 0.58).  Therefore we did not analyze the association between authors and data providers statistically.  In the Discussion we addressed some possible reasons that this determination was difficult for the coders to agree on.  


# Discussion

## Reader Agreement

The purpose of having a second researcher code a subset of articles was to address the question of whether my categories are clear enough that an independent researcher trying to do the same study would come up with the same conclusions. Poor agreement between the two readers for any given variable could be due to: 1) poor variable definitions or poor instructions about how to evaluate the variable; 2) true ambiguity in how a study should be coded; 3) insufficient knowledge or skill of the reader(s); 4) accidental error, perhaps due in part to skimming articles rather than reading them in their entirety.  

Consider adding a step after the agreement calculation where I summarize studies for which there was a lot of disagreement overall - ie disagreement on many categories.  These may be particularly perplexing, novel, or creative studies.  

One option to avoid ambiguity in coding is only coding and analyzing variables for which coding can be done based entirely or largely on character string matching.  This would provide greater reliability, but at the cost of less interesting, informative, and insightful analysis (Krippendorff 2004, Section 11.1).

The goal of having two readers is to estimate how replicatable the analysis is.  If another researcher could code the articles in the same way I do, then it provides confidence that the analysis is revealing some sort of true characteristics of the literature.  

## Who uses biological records?
While the number of studies that used data provided by the instituion of one of the lead authors was higher than expected by chance, the majority of studies did not have a lead author associated with the data providing institution.  If the lead authors are primarily the people who concieve of and design a study, then these results suggest that, to answer their research questions, researchers frequently use data that they are not directly involved in curating, perhaps because they do not already have the data necessary to answer their question.  This suggests that data providing institutions perform an important data sharing role, and the majority of research that uses biological records data could not have been done solely using the data resources of the lead authors.  

Most studies in this review did have at least one author associated with the data providing institution.  Co-authorship may be a prefered way for data providers to recieve credit for their role in the research pipeline, especially when the data form a "non-trivial" contribution to the study (@Roche2014, @Whitlock2011).  This could reflect [**do I have some reference about the messy nature of bio recs?**].

While standards for citing datasets are developing [**need citations**], there still seems to be some uncertainty about when and how to cite data resources [**I have a citation for this, right?**] which may motivate data providers to seek recognition through co-authorship.  


### What data types are used for each study question paradigm?
I think the story is that data use is roughly similar between question paradigms.  Integrating multiple datasets for analyses is common in both question paradigms, and so it is worth making an effort to ensure that published datasets are in standardized, compatible formats.  

### Study question paradigms
I may have done a poor job explaining the difference between studies about communities and studies that aggregate results about individual species.  My intention was to code whether the response variable was a property of an individual species or a property of a community (e.g. species richness).  However, the coding may have been complicated by studies that, for example, perform a regression to determine whether the range change of species is due to functional traits.  In such a study, it is perhaps not clear whether the response is a characteristic of individual species (e.g. range shift) or a response of the community (e.g. late flowering plants had greater range shifts).  

## Assorted bits to add
[[@Follett2015] found that methodology was focus of 17% of studies in a systematic review of cit. sci. use in publications. ]

[[@Follett2015] found that "the public preferred to publish their outcomes in" non peer reviewed places.  This is part of why I include grey literature. ] 

[[@Pearce-Higgins2018] Some data collection / providing schemes have a business model of getting funding to do research.  This might be particularly important for designed monitoring schemes, because the study authors can influence data collection methods.  ]

[[@Pearce-Higgins2018] warn of risks of free and open data being used without data providers knowledge and input.  Risks include inappropriate analysis or interpretation.  "Although not commonly regarded as a major issue in other disciplines where PDA is the norm, the complexities of ecological data can make archiving for independent use particularly challenging (Kenall et al. 2014, Mills et al. 2015)" [@Pearce-Higgins2018]. ]

[In a review of ecological and environmental citizen science projects, Pocock et al. [-@Pocock2017] distinguished between 'mass participation' projects that tended to have lower data quality, less detailed data, but cover larger geographic areas, and 'systematic monitoring' projects that tended to collect more complex and possible higher-quality data (e.g. counts instead of presence/absence observations) but covered smaller geographic areas.]

["There was no evidence that citizen science studies investigate a different organismal scale (community vs. species) compared to the urban ecology literature" in a systematic review @Wei2016. ]

### old questions to drop or keep?

$H2_a$: The data types "non-detection", "survey effort known", "abundance", and "organized monitoring scheme" each increase the probability of studies using a model-based analysis (either inferential or predictive) rather than a descriptive analysis.

**Another sub-question - Does data with more structure tend to be analyzed in-house more frequently than data w/less structure?  See @Pearce-Higgins2018**
**Not sure I should include this - I don't know if I can actually interpret it well.**


# Appendix 1 
## Model assumption checking

### Coding Reliability

```{r plot_kripp_for_adequate_sample_size, fig.cap=kripp_sum_cap}
# plot krippendorf's alpha values for only variables that have adequate sample size
ggplot(data = prob_of_values[which(prob_of_values$n_40_adequate == T), ], 
       aes(y = kripp_alpha)) +
  geom_boxplot() + 
  geom_point(aes(x = 0)) + 
  geom_hline(aes(yintercept = 0.667), lty = "dotted") + 
  geom_hline(aes(yintercept = 0.8), lty = "dashed") + 
  ggtitle("Krippendorf's alpha for the variables for\nwhich we have adequate sample size") +
  ylab("Krippendorf's Alpha value") + 
  xlab("") +
  theme_bw() + 
  theme(axis.text.x = element_blank())
```

```{r _print_kripp_sample_adequate_table}
kable(data.frame(prob_of_values), 
      col.names = c("variable", "prob of small class", 
                    "Krippendorff's alpha", "sample size 40 adequate"), 
      digits = 2)
```

### Temporal extent of studies

```{r print_time_anova, include = F}
kable(m_time_full_anova)
```

The following plots are for assessing assumptions for a linear regression of the form `log(number of years) ~ data type`.  Natural-log transformation of the number of years that studies cover improved normality (`r figs(name = "temp_norm_box_cap", display = "cite")`, `r figs(name = "temp_norm_ridge_cap", display = "cite")`).  

```{r time_assess_correlation_predictors, fig.height=8, fig.width = 8, fig.cap=correlation_cap}
### assess correlation between data type predictors ----------------------
dt_df <- wg[, which(
  colnames(wg) %in% 
    c("data.structure...organized.data.collection.scheme", 
      "data.structure...sampling.effort.known", 
      "data.structure...non.detection", 
      "data.structure...multiple.datasets.integrated.for.analysis", 
      "data.type...visit.specific.covariates"))]
for(i in 1:ncol(dt_df)) {
  dt_df[, i] <- as.factor(as.character(data.frame(dt_df)[, i]))
}

if(diag_present) {
  print(ggpairs(dt_df, columns = 1:ncol(dt_df), 
              upper = list(discrete = "blank"),
              lower = list(discrete = "ratio"), 
              diag = list(discrete = "barDiag"), 
              columnLabels = c("organized scheme", 
                               "sampling effort",
                               "non detection",
                               "multiple datasets", 
                               "visit specific covariates"), 
              labeller = label_wrap_gen(width = 10)) + 
        ggtitle("Correlation of data type predictor variables") + 
        theme(axis.text.x = element_text(angle = 50, hjust = 1)))
}
```

```{r temp_norm_box_cap} 
temp_norm_box_cap <- figs(name = "temp_norm_box_cap", 
                          "Boxplot of temporal extent of studies in log-transformed years for different data types.  Log transformation makes normality much better than it is with raw data.")
```

```{r temp_norm_ridge_cap} 
temp_norm_ridge_cap <- figs(name = "temp_norm_ridge_cap", 
                          "Histograms and densities of temporal extent of studies in log-transformed years for different data types.  Log transformation makes normality much better than it is with raw data.")
```

```{r temp_range_distribution_plot_print, fig.cap=temp_norm_box_cap} 
if(diag_present) {
  ## normality -------------------------------------
  # distribution of all data
  # hist(wg$temp_extent)
  # hist(wg$temp_extent[which(wg$temp_extent <= quantile(wg$temp_extent, 
  #                                                        0.90, na.rm = T))])
  
  # data very non-normal but log transformation kinda fixes this
  # (reciprocal and sqrt transformations did not help)
  print(ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
          geom_boxplot(varwidth = T) +
          scale_y_log10() + 
          theme_bw() + 
          ggtitle("log transformation of year\nmakes normality better") + 
          ylab("Temporal extent (years)") + 
          theme(axis.text.x = element_text(angle = 40, vjust = 0.9, 
                                           hjust = 1)))
}
```

```{r temp_range_distribution_ridge_plot_print, fig.cap=temp_norm_ridge_cap}
if(diag_present) {
  print(ggplot(data = temp_extent, aes(x = temp_extent, y = data_type)) + 
          scale_x_log10() + 
          geom_density_ridges(stat = "binline", alpha = 0.4) +
          geom_density_ridges(alpha = 0.4) + 
          xlab("Temporal extent (years)") + 
          theme_bw())
}
```


```{r temp_range_distribution_years_by_data_type}
# distribution of log(years) by data type (raw data, not residuals) 
# Looking at model residuals is more useful.  However, this does show that the
# log(years) distribution is more normal for some data types than others.
if(diag) {
  for(i in unique(temp_extent$data_type)) { 
    #hist(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
    qqnorm(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)], 
           main = i)
    qqline(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
  }
}
```

Diagnostics for main effects model log_years ~ all 5 data types (no interactions).
```{r temp_extent_lm_diags_print, fig.height=4, fig.width=4}
if(diag_present) {
  print("Diagnostics for main effects model log_years ~ all 5 data types (no interactions)")
  plot(time_lm_full, ask = FALSE)
  # hist(time_resids, 
  #      main = "standardized residuals of full model for temporal extent", 
  #      xlab = "standardized residuals")
  boxplot(time_resids, ylab = "standardized residuals", 
          main = "Residuals for lm model predicting log(temporal extent) ~ data type")
}
```

Table S1.  Variance inflation factors for predictor variables in a linear regression model of the log of number of years covered by a study as a function of the characteristics of the biological records data used in the study.  

```{r temp_extent_vif}
if(diag_present) {
  temp_extent_vif <- vif(time_lm_full) # assess multicollinearity
  temp_vif_df <- data.frame(temp_extent_vif)
  temp_vif_df$variable <- rownames(temp_vif_df)
  rownames(temp_vif_df) <- NULL
  kable(temp_vif_df, col.names = c("VIF", "variable"), digits = 2)
}
```

### Ecological Focus

`r tabs("resp_var_tab_cap", display = "full")`

```{r print_response_variable_table}
kable(resp_count_df, col.names = c("response variable", 
                                   "number of studies", 
                                   "proportion of studies", 
                                   "lower bound 95% bootstrap CI on proportion", 
                                   "upper bound 95% bootstrap CI on proportion"), 
      digits = 2)
```

### Develoment of methodology across different study questions

```{r method_assumptions}
if(diag_present) {
  method_mod_vif <- vif(method_mod) # assess multicollinearity
  # H-L test H_0 is that model fits data well
  method_mod_hlTest <- hoslem.test(method_mod$y, method_mod$fitted.values, 
                                   g = 10) 
  # make confusion matrix for full model with threshold 0.5
  method_conf_matrix <- table(method_mod$y, method_mod$fitted.values > 0.5)
  method_misclass_rate <- (method_conf_matrix["1", "FALSE"] + 
                             tryCatch(method_conf_matrix["0", "TRUE"], 
                                      error = function(x) 0))/length(method_mod$y)
  # confusion matrix for null model
  method_null_conf_matrix <- table(method_mod_null$y, 
                                   method_mod_null$fitted.values > 0.5)
  method_null_misclass_rate <- (method_null_conf_matrix["1", "FALSE"] + 
                             tryCatch(
                               method_null_conf_matrix["0", "TRUE"], 
                               error = function(x) 0))/length(
                                 method_mod_null$y)
  method_diags <- LogisticDx::dx(method_mod, byCov = F)
  
  # determine which articles had large residuals or leverage
  big_resids <- c(wg$title[abs(method_diags$Pr) > 2], 
                  wg$title[abs(method_diags$dr) > 2], 
                  wg$title[abs(method_diags$h) > 2])
  big_resids <- unique(big_resids)
  big_resids_df <- data.frame(
    title = wg$title[which(wg$title %in% big_resids)],
    pearson_resids = method_diags$Pr[which(wg$title %in% big_resids)], 
    deviance_resids = method_diags$dr[which(wg$title %in% big_resids)], 
    leverage = method_diags$h[which(wg$title %in% big_resids)], 
    methodology.development.or.analysis = 
      wg$methodology.development.or.analysis[which(wg$title %in% big_resids)])
}
```

```{r cell_count_cap}
cell_count_cap <- tabs(name = "cell_count_cap", 
                       "Cell counts for predictor variables in methodology logistic regression.  Cell counts should be > 5.")
```

```{r method_vif_cap} 
method_vif_cap <- tabs(name = "method_vif_cap", 
                       "Variance inflation factors for variables used in the logistic regression estimating the proportion of studies with a methodological focus across different ecological study questions.")
```

```{r pearson_resids_cap}
pearson_resids_cap <- figs(name = "pearson_resids_cap", 
                           "Histogram of Pearson residuals for the logistic regression model of methodology studies across study questions.  Residuals with an absolute value larger than 2 are potentially problematic.")
```

```{r deviance_resids_cap}
deviance_resids_cap <- figs(name = "deviance_resids_cap", 
                           "Histogram of deviance residuals for the logistic regression model of methodology studies across study questions.  Residuals with an absolute value larger than 2 are potentially problematic.")
```

```{r leverage_cap}
leverage_cap <- figs(name = "leverage_cap", 
                           "Histogram of leverage for the logistic regression model of methodology studies across study questions.  Value larger than 2 are potentially problematic.")
```

```{r big_resids_cap}
big_resids_cap <- tabs(name = "big_resids_cap", 
                       "Studies with large Pearson or deviance residuals or leverage > 2.")
```

```{r confusion_matrix_cap}
confusion_matrix_cap <- tabs(name = "confusion_matrix_cap", 
                             "Confusion matrix for logistic regression model. (Right now the highest predicted probability from the model is 0.46, so the model does not predict any TRUE values).")
```

There were `r table(wg$methodology.development.or.analysis)[[2]]` methodological studies and `r table(wg$methodology.development.or.analysis)[[1]]` non-methodological studies, suggesting that we can fit `r min(table(wg$methodology.development.or.analysis))/10` predictors if we need at least 10 events or non-events per predictor.

Variance inflation factors for all variables were small (`r tabs("method_vif_cap", display = "cite")`) indicating that correlation among the predictor variables is not likely to cause large uncertainty in the coefficient estimates of the logistic regression model [@Fox1992].  A Hosmer-Lemeshow goodness of fit test failed to reject the null hypothesis that the model fits the data well (*p* = `r round(method_mod_hlTest$p.value, 2)`). There were `r length(big_resids)` observations with big Pearson or deviance residuals or with leverage larger than two (`r tabs("big_resids_cap", display = "cite")`).

Most of the model diagnostics suggested that the logistic regression assumtions were met.  However, because there were `r length(big_resids)` large residuals, we decided to use bootstrapping to estimate 95% confidence intervals around the proportion of studies that were methodological within each of the six ecological or biological study focuses.  The 95% bootstrap CIs for the proportion of studies with each ecological focus that were methodological all overlapped (Fig. S??), confirming the conclusion from the logistic regression that we were unable reject a null hypothesis of no difference in the proportion of studies within each focus area that are methodological.

Print number of events:
```{r print_n_events, echo=TRUE}
if(diag_present) {
  # check number of events/ non-events to make sure we have enough data
  table(wg$methodology.development.or.analysis)
}
```

`r tabs("cell_count_cap", display = "full")`

```{r cell_counts_table_print}
if(diag_present){
  # check for sparse cells in predictors (need > 5 observations in each cell)
  kable(data.frame(select(wg, response.variable...phenology, trends.over.time, 
                          response.variable...abundance, 
                          response.variable...species.richness, 
                          response.variable...distribution, other_focus) %>%
                     apply(MARGIN = 2, FUN = table)) %>%
          rownames_to_column() %>%
          gather(-1, key = "variable", value = "count") %>%
          spread(rowname, count))
}
```

`r if(diag_present) tabs("method_vif_cap", display = "full")`

```{r print_method_vif}
if(diag_present) {
  vif_df <- data.frame(method_mod_vif)
  vif_df$variable <- rownames(vif_df)
  rownames(vif_df) <- NULL
  kable(vif_df, col.names = c("VIF", "variable"), digits = 2)
}
```

```{r print_pearson_resids_plot, fig.cap=pearson_resids_cap}
if(diag_present) {
  hist(method_diags$Pr, main = "Pearson residuals for methodology\nby study focus logistic regression")
}
```

```{r deviance_resids_methodology_plot, fig.cap=deviance_resids_cap}
if(diag_present) {
  hist(method_diags$dr, main = "Deviance residuals for methodology\nby study focus logistic regression")
}
```

```{r leverage_methodology_plot, fig.cap=leverage_cap}
if(diag_present) {
  hist(method_diags$h, main = "Leverage for methodology\nby study focus logistic regression")
}
```

`r if(diag_present) tabs("big_resids_cap", display = "full")`
```{r print_big_resids}
if(diag_present) kable(big_resids_df, digits = 2)
```

`r if(diag_present) tabs("confusion_matrix_cap", display = "full")`
```{r print_confusion_matrix_methodology}
if(diag_present) {
  kable(method_conf_matrix)
}
```


```{r method_prop_boot}
# Get bootstrap CIs for proportion of studies with each focus that are 
# methodological
# This is a sensitivity analysis since logistic regression had some big resids.

method_prop_fun <- function(x, indices) {
  # ARGS: x - a 2-column dataframe with TRUE/FALSE values in each column, in 
  #           which the 1st column indicates whether the study is within the
  #           target study focus (e.g. phenology) and the
  #           2nd column indicates whether the study is methodological
  #       indices - a vector of indices as required by boot
  x_b <- x[indices, ]
  
  x_b <- x_b[x_b[, 1] == T, ] # subset to only studies with this focus
  # calculate proportion that are methodological
  sum(as.numeric(as.logical(as.character(x_b[, 2]))), na.rm = TRUE)/nrow(x_b) 
}

method_list_boot <- lapply(
  list(sp_rich = wg[, c("response.variable...species.richness", 
                        "methodology.development.or.analysis")], 
       dist = wg[, c("response.variable...distribution", 
                     "methodology.development.or.analysis")], 
       abund = wg[, c("response.variable...abundance", 
                     "methodology.development.or.analysis")], 
       phen = wg[, c("response.variable...phenology", 
                     "methodology.development.or.analysis")], 
       temp_trends = wg[, c("trends.over.time", 
                     "methodology.development.or.analysis")], 
       other_focus = wg[, c("other_focus", 
                     "methodology.development.or.analysis")]),
  FUN = get_boot_CI, fun_to_use = method_prop_fun)
```

```{r make_method_boot_df}
method_boot_df <- data.frame(matrix(nrow = length(method_list_boot), 
                                        ncol = 4))
colnames(method_boot_df) <- c("response_variable", "prop_studies",
                             "l_95ci", "u_95ci")

# fill response variable counts data frame using the list of bootstrap results
for(i in 1:length(method_list_boot)) {
  method_boot_df[i, "response_variable"] <- names(method_list_boot)[i]
  method_boot_df[i, "prop_studies"] <- method_list_boot[[i]]$ci_obj$t0
  method_boot_df[i, "l_95ci"] <- method_list_boot[[i]]$ci_obj$bca[4]
  method_boot_df[i, "u_95ci"] <- method_list_boot[[i]]$ci_obj$bca[5]
}

# clean response variable strings
method_boot_df$response_variable <- gsub(".*sp_rich.*", "species richness", 
                                        method_boot_df$response_variable)
method_boot_df$response_variable <- gsub(".*other.*", "other", 
                                        method_boot_df$response_variable)
method_boot_df$response_variable <- gsub(".*dist.*", "species distribution", 
                                        method_boot_df$response_variable)
method_boot_df$response_variable <- gsub(".*abund.*", "abundance", 
                                        method_boot_df$response_variable)
method_boot_df$response_variable <- gsub(".*phen.*", "phenology", 
                                        method_boot_df$response_variable)
method_boot_df$response_variable <- gsub(".*temp.*", "temporal trends", 
                                        method_boot_df$response_variable)
```

```{r method_boot_plot}
# plot proportion of studies using each response variable, with 95% CIs
print(ggplot(data = method_boot_df, 
             aes(x = factor(response_variable, 
                            levels = c("species distribution", 
                                       "phenology", 
                                       "abundance",
                                       "species richness", 
                                       "temporal trends", 
                                       "other")), 
                 y = prop_studies)) + 
        geom_point() + 
        geom_linerange(aes(x = factor(response_variable, 
                            levels = c("species distribution", 
                                       "phenology", 
                                       "abundance",
                                       "species richness", 
                                       "temporal trends", 
                                       "other")), 
                           ymin = l_95ci, 
                           ymax = u_95ci)) + 
        xlab("") + 
        ylab("Proportion of Studies") + 
        ylim(c(0, 1)) + 
        theme_bw() + 
        theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1), 
              strip.background = element_blank(), strip.placement = "outside"))
      # theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)))
```

### What data types are used for each type of ecological study question?

```{r data_type_parad_table_cap}
data_type_parad_table_cap <- tabs(name = "data_type_parad_table_cap", 
                                  "Table of counts for data type by study question paradigm.")
```

`r tabs("data_type_parad_table_cap", display = "full")`

```{r print_data_type_paradigm_contingency_table}
if(diag_present) {
  dt_pd_df_wide$data_type <- gsub("\\.", " ", dt_pd_df_wide$data_type)
  colnames(dt_pd_df_wide) <- gsub("_", " ", colnames(dt_pd_df_wide))
  kable(data.frame(dt_pd_df_wide))
}
```

Big residuals from the Poisson regression of data type by study questions:

```{r print_big_resids_table}
kable(dt_pd_df[abs(dt_pd_df$resids) > 1, 
               c("study_focus", "data_type", "n", "predicted", "resids")], 
      col.names = c("Study Focus", "Data Type", "Observed number of studies", 
                    "Predicted number of studies", "Residual (on the scale of the linear predictors)"),digits = 1)
```

```{r print_data_type_paradigm_interaction_test, include = F}
# testing null hypothesis that the reduced model is as good as the full model
# cannot reject that null. Therefore there is no evidence that the interaction
# term is significant.  So we can say that the use of data types does not
# depend on the study question paradigm.  
dt_pd_interaction_test
```

```{r data_type_paradigm_assumption_check}
# following this: https://newonlinecourses.science.psu.edu/stat504/node/169/

# plot predicted v. observed for independence model
plot(dt_pd_df$predicted ~ dt_pd_df$n) # actually looks decent
abline(0, 1)

# for independence model:
summary(dt_pd_mod_ind)
37.545/20 # a bit bigger than one, but not huge.
1-pchisq(dt_pd_mod_ind$deviance, dt_pd_mod_ind$df.residual) 
# p = 0.01.  So reject null hypothesis that model is correctly specified.  
# However, http://thestatsgeek.com/2014/04/26/deviance-goodness-of-fit-test-for-poisson-regression/
# warns that this test can often wrongly indicate that the model is incorrectly
# specified.  But that seems to be more of a problem when expected means are low.
# When that post used means between 20 and 55, the pchisq test worked ok.  My
# observed cell count values are generally 10 or 15-ish up to a hundred.  So I
# am right on the border of having "large" means.  

# for saturated model:
summary(dt_pd_mod_full)
# (the residual deviance/df value would have division by 0.  Not meaningful. Of course fit is perfect in a saturated model.)
```


```{r data_type_parad_cap}
data_type_parad_cap <- figs(name = "data_type_parad_cap", 
                            "The number of studies using each type of biological records data within each of the broad study question paradigms.")
```

```{r data_type_by_paradigm_barplot, fig.cap=data_type_parad_cap, fig.height=7, fig.width=7, include = F}
### plot data type by study question paradigm ----------------------
# make plot, order factor levels by most important a-priori
# dt_pd_df was created in the Methods section
ggplot(dt_pd_df, 
       aes(x = factor(
         data_type, 
         levels = c("sampling.effort.known", 
                    "non.detection", 
                    "organized.data.collection.scheme", 
                    "multiple.datasets.integrated.for.analysis",
                    "visit.specific.covariates"), 
         labels = c("sampling effort", 
                    "non detection", 
                    "organized scheme", 
                    "multiple datasets", 
                    "visit-specific covariates")), 
         y = n, 
         fill = factor(
           data_type,  
           levels = c("sampling.effort.known", 
                      "non.detection", 
                      "organized.data.collection.scheme",
                      "multiple.datasets.integrated.for.analysis",
                      "visit.specific.covariates"), 
           labels = c("sampling effort", 
                      "non detection", 
                      "organized scheme", 
                      "multiple datasets", 
                      "visit-specific covariates")))) + 
  geom_bar(position = position_dodge(preserve = "single"), stat = "identity") + 
  facet_wrap( ~ factor(study_focus, levels = c("abundance", 
                                               "distribution",
                                               "phenology", 
                                               "species.richness", 
                                               "trends.over.time", 
                                               "other_focus"),
                       labels = c("abundance",
                                  "distribution",
                                  "phenology", 
                                  "species richness", 
                                  "trends over time", 
                                  "other focus")), 
              strip.position = "bottom") +
  ggtitle("Data type used for different ecological study questions") + 
  scale_fill_discrete(name = "Data Type") + 
  xlab("Study question") + 
  ylab("Number of studies") + 
  scale_y_continuous(breaks = seq(0, max(dt_pd_df$n), 5)) + 
  theme_bw() + 
  theme(axis.text.x = element_blank(), 
        text = element_text(size = plot_text_size),
        strip.background = element_blank(), strip.placement = "outside")
```

### Analysis approach & data type

To investigate whether the data type affects the analysis approach, we used a Poisson regression to model the relationship between three different analysis approaches (inference, prediction, and description) and data type.  We used a likelihood ratio test comparing a saturated Poisson regression modeling cell counts with a model containing only the main effects of data type and analysis approach (but no interaction).  The independence model (no interatctions) was overdispersed.  However, model residuals from the Poisson independence model (Fig. S??) showed the same pattern as the randomization test (Fig. ??), suggesting that the interpretation of the results is robust to using different statistical tests, and the overdispersion in the independence model is not severe enough to make the model uninformative.  

A Chi-squared likelihood ratio test comparing the saturated Poisson regression model  to a reduced model with only main effects for analysis approach and data type rejected the null hypothesis that the reduced model is as good as the full model (*p* = `r round(dt_an_mod_independence_test$'Pr(>Chi)'[2], 2)`), suggesting that the data type does influence or constrain the analysis approach.  

```{r print_data_type_analysis_contingency_table}
if(diag_present) {
  kable(data.frame(dt_an_df_wide), 
        col.names = c("Data Type", "Results - descriptive only", 
                      "Results - inference", "Results - prediction"))
}
```

```{r print_data_type_analysis_interaction_test, include = F}
# testing null hypothesis that the reduced model is as good as the full model.
# This rejects the null.  So we can say that the effect of the data type on the
# number of studies depends on analysis approach. 
dt_an_mod_independence_test # (same as below)
```

```{r data_type_analysis_assumption_check}
# following this: https://newonlinecourses.science.psu.edu/stat504/node/169/

# plot predicted v. observed for independence model
plot(dt_an_df$predicted ~ dt_an_df$n) # looks ok
abline(0, 1)

# for independence model:
summary(dt_an_mod_ind)
22.655/8 # resid dev / df - A lot bigger than 1.  Overdispersed
1-pchisq(dt_an_mod_ind$deviance, dt_an_mod_ind$df.residual) # (same as chisq test above)
# p = 0.003.  So reject null hypothesis that model is correctly specified.  
# However, http://thestatsgeek.com/2014/04/26/deviance-goodness-of-fit-test-for-poisson-regression/
# warns that this test can often wrongly indicate that the model is incorrectly
# specified.  But that seems to be more of a problem when expected means are low.
# When that post used means between 20 and 55, the pchisq test worked ok.  My
# observed cell count values are generally 10 or 15-ish up to a hundred.  So I
# am right on the border of having "large" means.  

# for saturated model:
summary(dt_pd_mod_full)
# (the residual deviance/df value would have division by 0.  Not meaningful. Of course fit is perfect in a saturated model.)
```

```{r ata_type_analysis_approach_nb_model}
# fit saturated model
dt_an_nb_full <- try(glm.nb(n ~ 1 + analysis_approach + data_type + 
                        analysis_approach:data_type, data = dt_an_df, 
                      init.theta = 1, link = identity, 
                      start = dt_an_mod_full$coefficients))
# fit model assuming independence between analysis approach and data type
dt_an_nb_ind <- glm.nb(n ~ 1 + analysis_approach + data_type, 
                     data = dt_an_df, 
                     init.theta = 1, link = identity, 
                     start = dt_an_mod_ind$coefficients)


# fit saturated model - quasipoisson
dt_an_qp_full <- glm(n ~ 1 + analysis_approach + data_type + 
                        analysis_approach:data_type, data = dt_an_df, 
                      family = quasipoisson(link = log))
summary.glm(dt_an_qp_full)
summary.glm(dt_an_qp_full)$dispersion
# fit model assuming independence between analysis approach and data type
dt_an_qp_ind <- glm(n ~ 1 + analysis_approach + data_type, 
                     data = dt_an_df, family = quasipoisson(link = log))
summary.glm(dt_an_qp_ind)
summary.glm(dt_an_qp_ind)$dispersion

dt_an_qp_independence_test <- try(anova(dt_an_qp_ind, dt_an_qp_full, 
                                        test = "Chisq"))
# that doesn't work, but regardless, the residual deviance from the 
# quasipoisson and poisson models are identical.  
```

```{r print_data_type_analysis_method_heatmap_table_poisson}
ggplot(data = dt_an_df, 
       aes(x = factor(analysis_approach, 
                      levels = c("descriptive.only", "inference", "prediction"), 
                      labels = c("descriptive\nonly", 
                                 "inference", "prediction")), 
           y = factor(data_type,
                      levels = c("multiple.datasets.integrated.for.analysis", 
                                 "sampling.effort.known", 
                                 "non.detection",
                                 "visit.specific.covariates",
                                 "organized.data.collection.scheme"), 
                      labels = c("multiple\ndatasets", 
                                 "sampling effort", 
                                 "non detection", 
                                 "visit-specific\ncovariates",
                                 "organized\nscheme")))) + 
  geom_tile(aes(fill = resids)) + 
  xlab("Analysis Approach") + 
  ylab("Data Type") + 
  geom_text(aes(label = round(resids, digits = 1))) + # write the values
  scale_fill_gradient2(low = "blue", mid = "white", high = "orange") + 
  theme(panel.grid.major.x=element_blank(), #no gridlines
        panel.grid.minor.x=element_blank(), 
        panel.grid.major.y=element_blank(), 
        panel.grid.minor.y=element_blank(),
        panel.background=element_rect(fill="white"), # background=white
        axis.text.x = element_text(angle=30, hjust = 1, vjust=1),
        axis.text.y = element_text(angle=45)) + 
  labs(fill = "Poisson\nresiduals") + 
  ggtitle("Residuals from Poisson\nindependence model")
```

# Appendix 2 - Instructions for Coders
[ Attach pdf of Instructions for Coders ]

# Appendix 3 - Search Terms
The search terms used with each search engine to find studies for this review are listed below, along with the date of the search.

Web of Science, searched 15 June 2018 with search terms: `TS=(("biological record*" OR "citizen science" OR atlas OR gbif OR "Global Biodiversity Information Facility" OR eBird OR "National Biodiversity Data Centre" OR "NBN" OR "National Biodiversity Network" OR "Centre for Environmental Data and Recording") AND ("Great Britain" OR Ireland OR Scotland OR Wales OR "Northern Ireland" OR "British Isles" OR UK OR "United Kingdom") NOT (cancer OR disease OR palliative OR "cross-sectional" OR "cross sectional" OR "New South Wales" OR Pyle OR marmoset))`

Scopus, searched 15 June 2018 with search terms: `TITLE-ABS-KEY (("biological record*" OR "citizen science" OR (atlas AND species) OR gbif OR "Global Biodiversity Information Facility" OR eBird OR "National Biodiversity Data Centre" OR "NBN" OR "National Biodiversity Network" OR "Centre for Environmental Data and Recording") AND ("Great Britain" OR Ireland OR Scotland OR Wales OR "Northern Ireland" OR "British Isles" OR UK OR "United Kingdom") AND NOT (cancer OR disease OR palliative OR "cross-sectional" OR "cross sectional" OR "New South Wales" OR Pyle OR marmoset))`

ProQuest, searched 15 June 2018 with search terms: Full-text results with terms `noft("citizen science") AND noft("Ireland")`

Google Scholar searched through Publish or Perish (@Harzing2007) over multiple days.

- 8 June 2018: Publish or Perish search terms: all of the words: "biological records"‚ Ireland; none of the words: book, "new report", "new species", "new record"; years: 2014 to 2018
- 8 June 2018: Publish or Perish search term: all of the words: "citizen science", species, Ireland; none of the words: disease, book, "new report", "new species", "new record"; years: 2014 to 2018
- 8 June 2018: Publish or Perish search terms: all of the words: "citizen science", species, "United Kingdom"; none of the words: disease, book, "new report", "new species", "new record"; years: 2014 to 2018
- 12 June 2018: PoP search terms: all of the words: "biological records", "United Kingdom"; none of the words: book‚ "new report", "new species", "new record"

# Save plots
```{r save_plots_main_text}
t_size_save <- 25

ggsave("Fig1.svg", temp_extent_plot + 
         theme(text = element_text(size = t_size_save)), 
       width = 25, height = 25, units = "cm", 
       device = "svg")


ggsave("Fig1.jpg", temp_extent_plot + 
         theme(text = element_text(size = t_size_save)), 
       width = 25, height = 25, units = "cm", 
       device = "jpg")

```

```{r save_plots_appendices}


```


# References





</div>