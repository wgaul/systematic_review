---
title: "Biological Records Systematic Review - Preliminary Results"
csl: ecology.csl
output:
  html_document: default
  fig_caption: yes
  html_notebook: default
  word_document: null
bibliography: PhD_references.bib
---

**TODO:**
**Add table w/kripp values to the start of each analysis results section!!**

- Methods
    - *Study eligibiligy* [DONE]
    - Article coding
        - Data Type
        - Study Questions
        - *Analysis approach* [DONE]
        - *Agreement* [DONE]
    - *Temporal Extent* [DONE]
    - Authors and Data Providing Institutions
    - Study Question
        - *Paradigms* [DONE]
        - Specific focus
    - Analysis approach & data type
- Results - Statistical
    - Search results
        - n returned, n not returned by multiple searche methods (represents incompleteness of search), eligibility, n vars coded, n studies coded, n double coded
    - Reader agreement
    - *Temporal extent* [DONE]
    - Authors and Data Providing Institutions
    - Study Question
        - *Paradigm* [DONE]
        - Data type by question paradigm (add stats)
    - Analysis approach & data type
- Results - Descriptive
    - Spatial extent
    - *Specific focus* [DONE]
    - Author associated with data provider
    - Spatial bias correction
    - Taxonomic group
    - Role of biological records
    - Prediction performance measure
    - Tabulate analysis methods
- Discussion
    - Article coding
        - Agreement
    - Temporal extent
    - Study question
    - Analysis approach & data type
    - Bio recs as predictors, facilitative roles (horizon scanning)
 

```{r, echo=F}
knitr::opts_chunk$set(echo=F, message=F, warning=T, cache=F)
```

```{r, load_data, cache = F, results='hide', warning = F}
setwd("~/Documents/Data_Analysis/UCD/systematic_review/")
library(wgutil)
library(Hmisc)
library(boot)
library(irr)
library(captioner)
library(knitr)
library(pander)
library(ggridges)
library(bestglm)
library(car)
library(GGally)
library(tidyverse)

diag <- T # produce exploratory & diagnostics plots & results
diag_present <- T # produce diagnostics for report to Dina

if(diag == F) { # turn off warnings when not running diagnostics
  knitr::opts_chunk$set(echo=F, message=F, warning=F, cache=F)
}

source("./clean_data_syst_rev.R")
```

```{r numberingPrep}
# make functions for adding numbered captions to figures 
figs <- captioner(prefix = "Fig.") 
tabs <- captioner(prefix = "Table")
```

# Preliminary structure of analysis and results
This document states every question I plan to ask and shows methods, figures, tables and results text that I expect to use to answer each question in the biological records systematic review. Specific results and interpretations will change when I code the rest of the articles.

# Methods
We did a systematic review of original research published since 2014 that used biological records from Ireland and the UK.  We generated a list of relevant articles from Web of Science, Scopus, ProQuest, GoogleScholar, and the Global Biodiversity Information Facility (GBIF) website.  One researcher evaluated each article for inclusion eligibility and coded information on 46 variables describing characteristics of each article.  A second reader coded all variables for a subset of 20% of the articles.  Agreement between the two readers was evaluated using Krippendorff's alpha (Krippendorff 2004).  Variables that did not meet minimum standards for reader agreement were not used in subsequent analyses.  

## Study eligibility
Studies were deemed eligible if they presented original research (no reviews or idea papers) published in the English language, used opportunistic biological data collected with non-standardized or semi-standardized designs, included (but were not necessarily limited to) data from Ireland or the UK, and the full text of the study was available through the University College Dublin library online platform, Google Scholar, Google search results, or ResearchGate.  Grey literature was included and therefore peer review was not required.  Studies using semi-standardized data collection schemes (e.g. UK Butterfly Monitoring Scheme) were included as long as data collection included some opportunistic elements (e.g. locations chosen opportunistically by volunteers).  Publications of data (e.g. atlases or data papers) were not considered eligible unless they included analysis of the data.  Only studies using a sample size of greater than 20 were included; this sample size was chosen arbitrarily, mainly to exclude studies in which re-examination of museum specimens resulted in a taxonomic identification revision for one or a few specimens, thereby changing the known range of a species to include or exclude Ireland and the UK.  Studies using museum data were considered eligible when the museum data used was similar in format to biological records data (e.g. “what, where, when” data); studies that used museum specimens only for taxonomic, genetic or morphology studies were excluded.  Studies using only fossil records were not included.  Studies using data from phenology networks were included; for the purposes of this review such data are considered biological records data with associated additional visit-specific data (e.g. the flowering status of plants).  Studies for which all data was collected by the study authors were not considered biological records data for the purposes of this review and were excluded.  The minimum required information in the data was a taxonomic name, a location, and date (“what, where, when”); additional information was permissible.  

## Article coding

For a list of the variables along with a description of each, see Appendix **[??]** *Instructions for Coders*.  Here we provide a brief summary of the broad types of information we coded.

### Data Type
We coded thirteen variables describing aspects of the type and structure of the biological records data.  The data type categories are not mutually exclusive.  

For most analyses, we grouped the data types "physical specimen", "photo", "audio", and "video" into a "voucher specimen" group, but we coded them individually in order to identify emerging trends in how vouchers are collected.

For statistical analyses, we kept data type variables in models based on *a-priori* expectations about the variables' influence on data analysis strategy and based on expectations about different levels of complexity and effort involved in collecting, recording, and storing the data.  The data type variables we kept in models were: what, where, when only; sampling effort reported; abundance; non-detection; organized monitoring scheme; visit-specific covariates; multiple datasets integrated for analysis; life stage information; and voucher specimen. 

### Study Questions
We described the main focus(es) of each study in terms of whether the study was asking questions about individual species, ecological communities, or about methodology.

### Analysis approach
We classified the broad analysis approach used by each study into one of three categories: 1) inference; 2) prediction; or 3) descriptive only.  For each study, we evaluated the analysis approach only for analyses that used biological records data.

We considered studies estimating parameters and reporting some measure of uncertainty (e.g. *p*-values, confidence intervals, posterior probability distributions) to be using inference.  We considered studies that built models and made predictions to be using an analysis strategy of prediction, even if no uncertainty or inference was reported with the predictions.  Finally, we considered the analysis approach to be descriptive only if results were descriptive with no prediction or inference.  Descriptive results could be narrative, graphical, or quantitative as long as no inference or uncertainty was reported (e.g. point estimates of descriptive statistics without any confidence interval or *p*-values).  Studies could use both inference and prediction as analysis strategies and so those categories were not mutually exclusive, but studies were only categorized as using a descriptive analysis approach if all results were descriptive only and the study used no inference or prediction.  

### Agreement between two readers
We used Krippendorff's *alpha* [@Krippendorff2004] to evaluate the agreement between two readers for each of `r ncol(er)-6` variables that were coded by two people.  [**Dina, any thoughts about Krippendorff's alpha or other measures of agreement?**]. We calculated Krippendorff's *alpha* using the `kripp.alpha` function in the `irr` package [@irr2012].  Krippendorff's alpha measures agreement between multiple coders while accounting for aggrement by chance.  Values of Krippendorff's alpha range from **[??]** to 1, where 1 represents perfect agreement.  Rule-of-thumb guidelines suggest that Krippendorff's alpha values below 0.67 indicate poor agreement such that the data cannot be trusted for analysis; values between 0.67 and 0.8 suggest that the data are moderately reliable and may be suitable for only tentative interpretation, and values above 0.8 indicate that the data are reliable.  We removed variables with Krippendorff's alpha values below 0.67 from all analyses; all other variables were kept, but we make note of variables for which Krippendorff's alpha values indicate only moderate reliability of the data. 


```{r inter_coder_agreement_subset_dfs, warning=TRUE}
## ellie's results ----------------------------------------------------
# remove columns that aren't coded variables 
er <- er[, which(colnames(er) %nin% c("link", "qualifies", "authors", 
                                      "publication", "doi", "year", 
                                      "keywords", "coding.DONE"))]
er <- er[order(er$title), ] # order rows
er <- er[, order(colnames(er))] # order columns
# put title column first
er <- er[, c(which(colnames(er) == "title"), 
                       which(colnames(er) != "title"))]
## end reorder ellie's results ------------------------------------------------

## reorder my results  --------------------------------------------------------
# subset my coded results to titles and columns coded by ellie
wg_dbl_cd <- wg[which(wg$title %in% er$title), which(colnames(wg) %in% colnames(er))]
# order rows
wg_dbl_cd <- wg_dbl_cd[order(wg_dbl_cd$title), ] 
# order cols
wg_dbl_cd <- wg_dbl_cd[, order(colnames(wg_dbl_cd))] 
wg_dbl_cd <- wg_dbl_cd[, c(which(colnames(wg_dbl_cd) == "title"), 
             which(colnames(wg_dbl_cd) != "title"))] # put title column first
## end reorder my results  ----------------------------------------------------

## make sure all titles are present
if(any(wg_dbl_cd$title %nin% er$title) | 
   any(er$title %nin% wg_dbl_cd$title)) {
  warning("er and wg_dbl_cd don't have the same titles. One or both of those data frames will be subsetted.")
  wg_dbl_cd <- wg_dbl_cd[which(wg_dbl_cd$title %in% er$title), ]
  er <- er[which(er$title %in% wg_dbl_cd$title), ]
}

## make sure df rows and cols are in the same order
if(!identical(colnames(er), colnames(wg_dbl_cd))) {
  stop("Columns in er and wg_dbl_cd must be identical and in the same order. They must be in the same order for creating the variable data frames that will be used to calculate Krippendorf's alpha. ")
}
```

```{r make_dfs_of_double_coded_variables_all_studies}
# each row is codings from one person
# each column is a study
dbl_coded_list <- list()
if(colnames(er)[1] != "title" | colnames(wg_dbl_cd)[1] != "title") {
  stop("The first column of er and wg must be the study title")}

for(i in 2:ncol(er)) {
  dbl_coded_list[[i-1]] <- data.frame(matrix(
    data = c(er[, i], wg_dbl_cd[, i]), 
    nrow = 2, ncol = nrow(er), byrow = T))
}
names(dbl_coded_list) <- colnames(er)[2:ncol(er)]
```

```{r krippendorfs_alpha_calc, warning=FALSE, message=FALSE}
do_kripp.alpha <- function(x, method) {
  # function to call kripp.alpha while converting dataframe x to a matrix
  # ARGS: x - data frame in which rows are observers and columns are studies
  #       method - character string of method to pass to kripp.alpha
  kripp.alpha(as.matrix(x), method = method)
}

kripp_all_dbl_coded <- lapply(dbl_coded_list, FUN = do_kripp.alpha, 
                              method = "nominal")
```

--------------------------

## Temporal Extent Analysis

$H_a$: The mean temporal extent of studies using only what, where, when data is longer than the mean temporal extent of studies using richer data types. 

$H_0$: The mean temporal extent of studies using what, where, when-only data is the same as the mean temporal extent of studies using richer data types. 

*Proposed Test*: Linear regression with natural-log transformed time (in years) as the outcome and data types as predictors.

We used linear regression with natural-log transformed time (in years) as the response variable.  We used nine binary predictor variables indicating whether the study used at least some data that had the following characteristics or information: 1) only what, where, when information, 2) known sampling effort, 3) abundance, 4) explicit non-detection information, 5) data came from an organized monitoring scheme, 6) visit-specific covariates, 7) multiple different biological records datasets were integrated for analyses, 8) life stage information, and 9) voucher specimens available.  While the predictor variables are correlated, we kept all variables in the multiple regression model because we are interested in the effect of each additional type of information *after accounting for other information types* (e.g. is there an additional effect of having abundance data after adjusting for whether the data came from an organized monitoring scheme).

Because the variables are not mutually exclusive, each category is treated as an individual variable, rather than treating each variable as a dummy variable representing a single level of a categorical "data type" variable.  

The intercept-only model in this analysis is the mean temporal extent of all studies.  
```{r calc_temp_extent}
# calculate number of years covered by study
wg$temp_extent <- as.numeric(wg$end.year) - as.numeric(wg$start.year)
# individually assign values to studies with temporal extent < 1 year
if(diag) paste0("The following article has a temporal extent of zero: ", 
                wg$title[which(wg$temp_extent == 0)])
wg$temp_extent[which(
  grepl("An assessment of bumblebee .* land use and floral.*", 
        wg$title))] <- 0.33
```

```{r time_assess_correlation_predictors, fig.height=8, fig.width = 8}
### assess correlation between data type predictors ----------------------
dt_df <- wg[, which(
  colnames(wg) %in% 
    c("data.structure...organized.data.collection.scheme", 
      "data.structure...sampling.effort.known", 
      "data.structure...non.detection", 
      "data.structure...multiple.datasets.integrated.for.analysis", 
      "data.type...what.where.when.only", 
      "data.type...abundance", 
      "data.type...visit.specific.covariates", 
      "data.type...life.stage", 
      "data.type...voucher.of.some.kind.necessary.for.analysis"))]
for(i in 1:ncol(dt_df)) {
  dt_df[, i] <- as.factor(as.character(data.frame(dt_df)[, i]))
}

if(diag_present) {
  print(ggpairs(dt_df, columns = 1:ncol(dt_df), 
              upper = list(discrete = "blank"),
               lower = list(discrete = "ratio"), 
              diag = list(discrete = "barDiag"), 
              columnLabels = c("organized scheme", 
                               "sampling effort",
                               "non detection",
                               "multiple datasets", 
                               "what where when only", 
                               "abundance",
                               "visit specific covariates", 
                               "life stage", 
                               "voucher used"), 
              labeller = label_wrap_gen(width = 10)) + 
        ggtitle("Correlation of data type predictor variables") + 
        theme(axis.text.x = element_text(angle = 50, hjust = 1)))
}

```


```{r prepare_temporal_extent_df}
temp_extent <- select(
  wg, c(title, temp_extent, 
        data.type...what.where.when.only, 
        data.structure...sampling.effort.known, 
        data.type...abundance,
        data.structure...non.detection,
        data.structure...organized.data.collection.scheme, 
        data.type...visit.specific.covariates, 
        data.structure...multiple.datasets.integrated.for.analysis,
        data.type...life.stage, 
        data.type...voucher.of.some.kind.necessary.for.analysis)) %>%
  gather(key = "data_type", value = "dt_value", 
         data.type...what.where.when.only:data.type...voucher.of.some.kind.necessary.for.analysis, 
         factor_key = T) %>%
  filter(dt_value == T) %>%
  group_by(data_type)

# make names prettier
temp_extent$data_type <- gsub("data.type...", "", temp_extent$data_type)
temp_extent$data_type <- gsub("data.structure...", "", temp_extent$data_type)

temp_extent$data_type <- factor(as.character(temp_extent$data_type), 
                            levels = c("what.where.when.only", 
                                       "sampling.effort.known",
                                       "abundance",
                                       "non.detection", 
                                       "organized.data.collection.scheme", 
                                       "visit.specific.covariates", 
                                       "multiple.datasets.integrated.for.analysis", 
                                       "life.stage", 
                                       "voucher.of.some.kind.necessary.for.analysis"), 
                            labels = c("only what where when", 
                                       "sampling effort known",
                                       "abundance",
                                       "non detection", 
                                       "organized scheme", 
                                       "visit specific covariates", 
                                       "multiple datasets", 
                                       "life stage", 
                                       "voucher used"))
```

Evaluate normality of raw data:
```{r temp_range_distribution}
if(diag_present) {
  # distribution of all data
 # hist(wg$temp_extent)
 # hist(wg$temp_extent[which(wg$temp_extent <= quantile(wg$temp_extent, 
 #                                                        0.90, na.rm = T))])
  ## data are not balanced
  print(table(temp_extent$data_type))
  print("Data are not balanced.")
  
  ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
    geom_boxplot() +
    theme_bw() + 
    ggtitle("All data") + 
    theme(axis.text.x = element_text(angle = 330, vjust = 0.3))
  
  ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
    ylim(c(0, 800)) + 
    geom_boxplot() +
    theme_bw() + 
    ggtitle("Temporal Duration after\ncutting off the 8000 year study") + 
    theme(axis.text.x = element_text(angle = 330, vjust = 0.3))
  
  ggplot(data = temp_extent, aes(x = temp_extent, y = data_type)) + 
    xlim(c(0, 800)) + 
    geom_density_ridges() +
    ggtitle("Temporal Duration after\ncutting off the 8000 year study") + 
    theme_bw()
  
  ## normality -------------------------------------
  # data very non-normal (above) but log transformation kinda fixes this
  # (reciprocal and sqrt transformations did not help)
  print(ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
          geom_boxplot(varwidth = T) +
          scale_y_log10() + 
          theme_bw() + 
          ggtitle("log transformation of year\nmakes normality better") + 
          ylab("Temporal extent (years)") + 
          theme(axis.text.x = element_text(angle = 40, vjust = 0.9, 
                                           hjust = 1)))
  
  print(ggplot(data = temp_extent, aes(x = temp_extent, y = data_type)) + 
          scale_x_log10() + 
          geom_density_ridges(stat = "binline", alpha = 0.4) +
          geom_density_ridges(alpha = 0.4) + 
          xlab("Temporal extent (years)") + 
          theme_bw())
  
  if(diag) {
    for(i in unique(temp_extent$data_type)) { 
      # distribution of log(years) by data type
      #hist(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
      qqnorm(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)], 
             main = i)
      qqline(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
    }
  }
}
```

Fit full model and evaluate residuals for assumptions of linear regression.
```{r fit_temp_extent_lm}
## fit linear regression with log-transformed years as response variable
## and each data type as a separate variable 
# fit full model
wg$log_years <- log(wg$temp_extent)
# variables added based on a-priori expectation of most to least important
time_lm_full <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                   data = wg, 
                   na.action = na.exclude) 

time_resids <- rstandard(time_lm_full) # calculate residuals
if(diag_present) {
  print("Diagnostics for full model log_years ~ all 9 data types")
  plot(time_lm_full)
  # hist(time_resids, 
  #      main = "standardized residuals of full model for temporal extent", 
  #      xlab = "standardized residuals")
  boxplot(time_resids, ylab = "standardized residuals")
}
time_summary <- summary(time_lm_full)
if(diag) time_summary

## to interpret outcome on original scale:
# for a one-unit increase in x, the predicted value of y 
# changes by (e^beta1 - 1)*100 percent
```


```{r time_data_type_full_model_sig_test}
# Test significance of full model
m0 <- lm(log_years ~ 1, data = wg) # fit null model 
m_time_full_anova <- anova(m0, time_lm_full)
```

```{r time_data_type_test_main_effects, include = F}
## test effect of each data type predictor by comparing the full model to a
# model with that predictor removed
# 
# TODO: Does this need multiple comparison correction?

m_no_whwhwh <- lm(log_years ~ 1 + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                  data = wg)
m_no_effort <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                  data = wg)
m_no_abund <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                 data = wg)
m_no_nonDet <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                  data = wg)
m_no_scheme <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                  data = wg)
m_no_visitCovs <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                     data = wg)
m_no_multData <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                    data = wg)
m_no_lifeStage <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                     data = wg)
m_no_voucher <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage,
                   data = wg)

indiv_mods_time <- list(m_no_whwhwh, m_no_effort, m_no_abund, m_no_nonDet, 
                        m_no_scheme, m_no_visitCovs, m_no_multData, 
                        m_no_lifeStage, m_no_voucher)
```


## Analysis of Study Questions

### What is the most popular study question paradigm for studies using biological records?

$H_a$: Some broad study question paradigms are more common than others in studies using biological records.

$H_0$: The number of studies pursuing each of the broad study question paradigms is the same.

*Proposed Test*: Bootstrap confidence intervals of number of studies within each study question paradigm.

We categorized study questions into three broad question types: individual species questions, community questions, and other questions (which included methodological questions and others).  
We counted the number of studies that asked questions within each of three broad study question categories.  Some studies ask multiple questions, and those questions may be of different types, so it is possible for a study to appear in the count for multiple categories.  We estimated 95% confidence intervals around the number of studies asking each type of study question using bias-corrected accelerated bootstrap confidence intervals @Efron1993. 

```{r make_other_paradigm_category}
# make an "other paradigm" column for all studies that are not community or 
# individual species studies (this will be a catch-all category for methodology
# studies and whatever else)
wg$other_paradigm <- wg$community.question == F & 
  wg$individual.species.question == F
```

```{r paradigm_bootstrap}
count_fun <- function(x, indices) {
  # ARGS: x - a vector of TRUE/FALSE values, for which the TRUES will be counted
  #       indices - a vector of indices as required by boot
  x_b <- x[indices]
  sum(as.numeric(x_b), na.rm = TRUE)
}

boot_paradigm <- function(x) {
  b_o <- boot(x, count_fun, R = 10000, stype = "i")
  ci_o <- boot.ci(b_o, conf = 0.95, type = "bca")
  to_return <- list(obs = x, 
                    boot_obj = b_o, 
                    ci_obj = ci_o)
  to_return
}

paradigm_list <- lapply(list(community = wg$community.question, 
                             individual_sp = wg$individual.species.question, 
                             other = wg$other_paradigm), 
                        FUN = boot_paradigm)
```


### Specific Study Focus (TODO)

### What data types are used for each study question paradigm? (TODO)

---------------------

## Authors and Data Providing Institutions

**Who choses to use biological records data?**

$H_a$: The primary authors of a study are more likely to use data held by their own institution than by other institutions.

$H_0$: The number of studies with a lead author from the institution that provided the data is the same as expected if each lead author chose a co-lead author and data providing institution at random from the population of lead authors and data providing institutions that appear on a study in this review.

*Another sub-question - Does data with more structure tend to be analyzed in-house more frequently than data w/less structure?  See @Pearce-Higgins2018*
**TODO Here 22 Nov**
$H_a$: The proportion of studies for which a lead author is associated with the data providing institution is higher for studies using organized monitoring scheme data than for studies using data that is not from an organized monitoring scheme.  

$H_0$: The proportion of studies for which a lead author is associated with the data provider is not different for studies using data from an organized monitoring scheme and studies not using data from an organized monitoring scheme. 

*Proposed Test*: Permutation test to generate a null hypothesis representing no relationship between the first author and last authors or between either author and the institution that provided the data.  

```{r author_inst}
# create a variable that indicates whether either the 1st or last author is from
# the data providing organization
wg$lead_author_from_data_inst <- mapply(FUN = function(a, b, c) {
  any(c(unlist(a), unlist(b)) %in% unlist(c))}, 
  strsplit(wg$institution.of.first.author, split = ";"), 
  strsplit(wg$institution.of.last.author, split = ";"), 
  strsplit(wg$proximate.data.source, split = ";"))
```

```{r permute_author_inst}
# permute to break all associations between first author, last author, and
# data providing institution
perm_inst <- function(index, df) {
  # permute data source column
  df$proximate.data.source <- sample(df$proximate.data.source, replace = F)
  df$institution.of.first.author <- sample(df$institution.of.first.author, 
                                           replace = F)
  # find whether a lead author is from the data source institution
  matches <- mapply(FUN = function(a, b, c) {
    any(c(unlist(a), unlist(b)) %in% unlist(c))}, 
    strsplit(df$institution.of.first.author, split = ";"), 
    strsplit(df$institution.of.last.author, split = ";"), 
    strsplit(df$proximate.data.source, split = ";"))
  # count how many authors are from the data source institution
  sum(matches)
}

# set true data value
n_author_from_inst_real <- sum(wg$lead_author_from_data_inst)
# get null distribution
perm_auth <- sapply(c(1:9999), FUN = perm_inst, df = wg)
# append true value as a permutation in the null distribution
perm_auth <- c(perm_auth, n_author_from_inst_real)

# calculate empirical p-value from permutations
p_auth_perm <- min(length(which(perm_auth >= n_author_from_inst_real)), 
                   length(which(perm_auth <= n_author_from_inst_real))) / 
                     length(perm_auth)

# hist(perm_n_author_from_inst, 
#      xlim = c(min(c(perm_n_author_from_inst, n_author_from_inst_real)), 
#               max(c(perm_n_author_from_inst, n_author_from_inst_real))))
# abline(v = n_author_from_inst_real)
```

```{r boot_author_institution}
# get bootstrap CIs for proportion of studies with a lead author from the data
# providing institution and for the proportion of studies with any author from
# the data providing institution
lead_auth_inst_boot <- boot(wg$lead_author_from_data_inst, 
                            statistic = function(x, ind) {
                              sum(x[ind]) / length(x)}, 
                            R = 999)
any_auth_inst_boot <- boot(wg$author.associated.with.proximate.data.provider, 
                           statistic = function(x, ind) {sum(x[ind]) / length(x)}, 
                           R = 999)
```

**TODO: Here 21 Nov.  **

____________________________________________________________________________

# Results

```{r caption_kripp_summary}
kripp_sum_cap <- figs(name = "kripp_sum_cap", 
                      paste0("Reader agreement for ", 
                             length(which(colnames(wg) != "title")), 
                             " categories measured using Krippendorff's alpha. The ??? group shows agreement for all articles (n = ", 
                             nrow(wg), 
                             ").  Dotted lines show values of alpha of 0.667 and 0.8, which are recommended minimum and preferred values at which variables can be relied upon for analysis."))
```

## Search results 
The search returned `r nrow(elig)` potentially relevant studies, of which we have evaluated `r sum(!is.na(elig$qualifies))` for eligibility, and judged `r length(which(elig$qualifies == TRUE))` to be eligible for inclusion in the scoping review.  ??? percent of potentially relevant studies were returned by more than one search method, while ??? percent of potentially relevant studies were returned by only one search method.  This suggests that additional searching is likely to be less rewarding as most studies identified by the later searches had already been identified by earlier searches.  

One reader has coded `r nrow(wg)` articles and a second reader has coded `r nrow(er)` of those. 

This preliminary analysis uses the `r nrow(wg)` articles coded so far.  


## Reader agreement

Krippendorff's alpha values for the variables that have been double coded indicate substantial disagreement between the two readers, and show few variables for which codings are reliable enough to be used in analysis (`r figs("kripp_sum_cap", display = "cite")`).  However, the number of articles that have been evaluated by two readers is probably too small for reliable estimation of Krippendorff's alpha given the proportions of the TRUE and FALSE classes for most variables (`r tabs("kripp_samp_size_cap", display = "cite")`).

The poor agreement between the two readers for any given category could be due to: 1) poor catgegory definitions or poor instructions about how to evaluate the category; 2) true ambiguity in how a study should be coded; 3) insufficient knowledge or skill of the reader(s); 4) accidental error, perhaps due in part to skimming articles rather than reading them in their entirety.  

Possible solutions are: 1) refine definitions and instructions, when possible specifying specific words or phrases that can be searched for in the article; 2) removing from analysis variables for which many articles are difficult to definitively categorize; 3) requiring a higher level of skill or knowledge of the readers, or treating the less-skilled readers' categorizations as a trigger for a "tie breaker" 3rd review to reveal obvious errors by the first reader, but not as reliable codings in their own right; 4) require readers to read the entire article rather than skimming it, or using disagreements as a trigger for a 3rd "tie breaker" review.



```{r krippendorfs_alpha_summary, fig.cap = kripp_sum_cap}
kripp_summary_long <- data.frame(
  variable = names(kripp_all_dbl_coded), 
  krip_alpha = sapply(kripp_all_dbl_coded, 
                        FUN = function(x) {x$value}))

if(diag_present) {
  print(
    ggplot(data = kripp_summary_long, 
           mapping = aes(y = krip_alpha)) + 
      geom_boxplot() + 
      geom_point(aes(x = 0)) + 
      geom_hline(aes(yintercept = 0.667), lty = "dotted") + 
      geom_hline(aes(yintercept = 0.8), lty = "dashed") + 
      ggtitle("Reader agreement using Krippendorff's alpha") + 
      xlab("") +
      ylab("Krippendorff's alpha") + 
      theme_bw() + 
      theme(axis.text.x = element_blank())
  )
}
```

We could use my codings (either original or corrected) for analysis, and view the second reader's role as highlighting cases of sloppy errors or careless oversight on my part.  This doesn't address the reproducability question of whether my categories are clear enough that an independent researcher trying to do the same study would come up with the same conclusions.  In order to answer that, we would need a second reader with adequate ability to code each category.  Lacking such a person, we'll have to use the second reader as an error catcher rather than an independent second opinion.

Page 240 of the Krippendorf (2004) book has recommendations about sample size needed based on how many cases there are of the least common class.  `r tabs("kripp_samp_size_cap", display = "cite")` shows which variables have enough of the minority class that Krippendorff's alpha can be calculated with significance level of 0.1 and minimum acceptable Krippendorff's alpha of 0.667.

```{r kripp_samp_size_cap}
kripp_samp_size_cap <- tabs(name = "kripp_samp_size_cap", 
                            paste0("Recommended sample size for accurate estimation of Krippendorff's alpha given the proportion of the minority class in the results, and a desired confidence level of 0.1 for a minimum Krippendorff's alpha value of 0.667 (Krippendorf 2004).  So far there are ", nrow(wg), " double-coded studies."))
```

`r if(diag_present) tabs("kripp_samp_size_cap", display = "full")`

```{r calculate_proportion_T}
## This calculates the probability of the smallest class for each variable, for 
## use in calculating minimum sample size needed for Kripp's alpha for that 
## variable.  
## This treats all variables as factor variables.
prob_of_values <- lapply(
  wg[, which(colnames(wg) %in% names(kripp_all_dbl_coded))], 
  FUN = function(x) {
    x <- factor(x)
    mn <- 1
    for(j in 1:length(levels(x))) {
      prob <- length(which(x == levels(x)[j])) / length(which(!is.na(x)))
      if(prob < mn) mn <- prob
    }
    mn
  })

prob_of_values <- data.frame(variable = names(prob_of_values), 
                             prob_small_class = unlist(prob_of_values), 
                             row.names = NULL)
prob_of_values$kripp_alpha <- NA
for(i in 1:nrow(prob_of_values)) {
  # add krippendorff's alpha for codings
  prob_of_values$kripp_alpha[i] <- kripp_all_dbl_coded[[which(
    names(kripp_all_dbl_coded) == prob_of_values$variable[i])]]$value
}
prob_of_values <- prob_of_values[order(prob_of_values$kripp_alpha, 
                                       decreasing = T), ]

# On p 240 of Krippendorff book, it says that if acceptable alpha is 0.667 and 
# desired significance level is 0.1, then sample size needed is as follows:
prob_of_values$n_25_adequate <- prob_of_values$prob_small_class > 0.25
prob_of_values$n_30_adequate <- prob_of_values$prob_small_class > 0.2
prob_of_values$n_40_adequate <- prob_of_values$prob_small_class >= 0.143
# TODO add column for sample size of 50

if(diag_present == TRUE) {
  kable(prob_of_values[, -2], digits = 2)
  
  # list only those for which sample size is adequate
  samp_adequate <- prob_of_values[which(prob_of_values$n_40_adequate == T), ]
  kable(samp_adequate[order(
  samp_adequate$kripp_alpha, decreasing = T), -2], digits = 2)
  rm(samp_adequate)
}
```

```{r plot_kripp_for_adequate_sample_size}
# plot krippendorf's alpha values for only variables that have adequate sample size
ggplot(data = prob_of_values[which(prob_of_values$n_40_adequate == T), ], 
       aes(y = kripp_alpha)) +
  geom_boxplot() + 
  geom_point(aes(x = 0)) + 
  geom_hline(aes(yintercept = 0.667), lty = "dotted") + 
  geom_hline(aes(yintercept = 0.8), lty = "dashed") + 
  ggtitle("Krippendorf's alpha for the variables for\nwhich we have adequate sample size") +
  ylab("Krippendorf's Alpha value") + 
  xlab("") +
  theme_bw() + 
  theme(axis.text.x = element_blank())
```



## Temporal extent of studies
Coder agreement for variables in this analysis:
```{r vars_temp_extent}
if(diag_present) {
  vs <- c("start.year", "end.year", "data.type...what.where.when.only",
          "data.structure...sampling.effort.known", "data.type...abundance", 
          "data.structure...non.detection",
          "data.structure...organized.data.collection.scheme", 
          "data.type...visit.specific.covariates", 
          "data.structure...multiple.datasets.integrated.for.analysis", 
          "data.type...life.stage", 
          "data.type...voucher.of.some.kind.necessary.for.analysis")
  kable(prob_of_values[which(prob_of_values$variable %in% vs), 
                       c("variable", "kripp_alpha", "n_40_adequate")], digits = 2)
  rm(vs)
}
```

```{r print_time_anova}
m_time_full_anova
```

```{r print_time_individual_predictor_significance}
if(diag) {
  for(i in 1:length(indiv_mods_time)) { 
    print(anova(indiv_mods_time[[i]], time_lm_full))
  }
}
```

The overall model using supplementary data types to predict the natural log of temporal extent of study (in years) was (not) significant ($F_{`r format(round(m_time_full_anova$Df[2], digits = 2), scientific = F)`, `r format(round(m_time_full_anova$Res.Df[2], digits = 2), scientific = F)`}$ = `r format(round(m_time_full_anova$F[2], digits = 2), scientific = F)`, p = `r format(round(m_time_full_anova[6][[1]][2], digits = 2), scientific = F)`, Adjusted $R^2%$ = `r format(round(time_summary$adj.r.squared, digits = 2), scientific = F)`).  

If the overall model is significant, I will individually test whether each variable has a significant effect after accounting for the other variables.  I will interpret the effect direction and effect size of all variables in the context of the full model even if the variables are not significant on their own.  Example of how results will be written:
 
The expected temporal extent covered by a study that uses only "what, where, when" data is `r round(exp((time_lm_full$coefficients[1] + time_lm_full$coefficients[2])), digits = 1)` years when the data have no other data types.  When the data are "what, where, when" only but multiple datasets were integrated for analysis, the expected temporal extent covered by a study is `r round(exp((time_lm_full$coefficients[1] + time_lm_full$coefficients[2] + time_lm_full$coefficients[8])), digits = 1)` years. 

Studies using data that includes sampling effort information have a shorter temporal extent after accounting for other data types that the data has (*p* = `r format(round(time_summary$coefficients[2, 4], digits = 3), scientific = F)`).  The effects of additional information on detection / non-detection and abundance were not significant after accounting for the effects of the other data types.  The signs of the effects of sampling effort information and detection / non-detection information were both negative, as expected.  However, the sign of the effect for abundance information was unexpectedly positive.  This may be because abundance is correlated with detection/non-detection and sampling effort, and so the effect of abundance data is positive after correcting for those other data types.  When I fit a model with only the intercept and the abundance data type, the direction of the effect of abundance data was negative as expected.    

```{r time_extent_results_plot}
## TODO: need to hand-annotate R2 and p-value in plot
temp_extent$sig_different_from_WhWhWh <- temp_extent$data_type %in% 
                                       c("[names of sig. diff. vars. here")
print(ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + #, color = sig_different_from_WhWhWh
        geom_boxplot(varwidth = T) +
        scale_y_log10() + 
        xlab("Data Type") + 
        ylab("Temporal Extent of Study (years)") + 
        annotate("text", x = 3, y = 2600, 
                 label = "paste(\"Adjusted \", 
                 italic(R) ^ 2, \" = ???\")", 
                parse = TRUE) + 
  annotate("text", x = 3, y = 1200, 
           label = "paste(\"Overall model significance \", 
                 italic(p), \" = ??\")", 
           parse = TRUE) +
        theme_bw() + 
        scale_color_discrete(name = "Significantly Different from\nWhat, Where, When-only\nstudies (alpha = 0.1 level)") + 
        theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) 
)
```

```{r time_extent_results_plot_for_presentation_slide, eval = F, include = F}
## This code is to be run in the console for making plot to be exported as jpg for slides
## TODO: need to hand-annotate R2 and p-value in plot
t_size = 22 # plot text size
ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + #, color = sig_different_from_WhWhWh
  geom_boxplot(varwidth = T) +
  scale_y_log10() + 
  xlab("Data Type") + 
  ylab("Temporal Extent of Study (years)") + 
  # annotate("text", x = 3, y = 140, 
  #          label = "paste(\"Adjusted \", 
  #                italic(R) ^ 2, \" = 0.27\")", 
  #          parse = TRUE, 
  #          size = t_size - 15) + 
  annotate("text", x = 3, y = 1400, 
           label = "paste(\"Overall model significance \", 
                 italic(p), \" = 0.012\")", 
           parse = TRUE, 
           size = t_size - 14) +
  theme_bw() + 
  scale_color_discrete(name = "Significantly Different from\nWhat, Where, When-only\nstudies (alpha = 0.1 level)") + 
  theme(axis.text.x = element_text(angle = 330, vjust = 1, hjust = 0), 
        text = element_text(size = t_size)) 
```

-----------------------------------------------

```{r paradigm_table_caption}
paradigm_table_caption <- tabs("paradigm_table_caption", 
                               paste0("The number of studies published between 2014 and 2018 addressing different broad types of questions using biological records data.  The 'other' category includes methodology studies and other studies that did not focus specifically on questions about individual species or communities."))
```

```{r paradigm_plot_cap}
paradigm_plot_cap <- figs("paradigm_plot_cap", 
                          "The number of studies published between 2014 and 2018 addressing different broad types of questions using biological records data.  The 'other' category includes methodology studies and other studies that did not focus specifically on questions about individual species or communities.  Vertical lines show 95% bootstrap confidence intervals.")
```

## Analysis of Study Questions

### What is the most popular study question paradigm?

$H_a$: Some broad study question paradigms are more common than others in studies using biological records.

$H_0$: The number of studies pursuing each of the broad study question paradigms is the same.

Coder agreement for variables in this analysis:
```{r vars_paradigm}
# TODO Here 23 Nov
if(diag_present == T) {
  vs <- c("community.question", "individual.species.question")
  kable(prob_of_values[which(prob_of_values$variable %in% vs), 
                       c("variable", "kripp_alpha", "n_40_adequate")], digits = 2)
  rm(vs)
}
```

The number of studies asking questions about individual species (`r paradigm_list$individual_sp$ci_obj$t0` studies, 95% bootstrap CI [`r paradigm_list$individual_sp$ci_obj$bca[4]`, `r paradigm_list$individual_sp$ci_obj$bca[5]`]) was significantly higher than the number of studies asking questions about communities (`r paradigm_list$community$ci_obj$t0` studies, 95% bootstrap CI [`r paradigm_list$community$ci_obj$bca[4]`, `r paradigm_list$community$ci_obj$bca[5]`]) or asking other types of questions (`r paradigm_list$other$ci_obj$t0` studies, 95% bootstrap CI [`r paradigm_list$other$ci_obj$bca[4]`, `r paradigm_list$other$ci_obj$bca[5]`]) (`r tabs("paradigm_table_caption", display = "cite")`, `r figs("paradigm_plot_cap", display = "cite")`). 

`r tabs("paradigm_table_caption", display = "full")`

```{r print_study_question_paradigm_table}
paradigm_count_df <- data.frame(matrix(nrow = length(paradigm_list), 
                                        ncol = 4))
colnames(paradigm_count_df) <- c("paradigm", "n_studies", "l_95ci", "u_95ci")

# fill paradigm counts data frame using the list of paradigm bootstrap results
for(i in 1:length(paradigm_list)) {
  paradigm_count_df[i, "paradigm"] <- names(paradigm_list)[i]
  paradigm_count_df[i, "n_studies"] <- paradigm_list[[i]]$ci_obj$t0
  paradigm_count_df[i, "l_95ci"] <- paradigm_list[[i]]$ci_obj$bca[4]
  paradigm_count_df[i, "u_95ci"] <- paradigm_list[[i]]$ci_obj$bca[5]
}

kable(paradigm_count_df, col.names = c("broad type\nof study question", 
                                       "number of studies", 
                                       "lower 95%\nbootstrap CI", 
                                       "upper 95%\nbootstrap CI"))
```


```{r plot_paradigm_counts, fig.cap=paradigm_plot_cap}
# plot number of studies in each paradigm, with 95% CIs
print(ggplot(data = paradigm_count_df, 
             aes(x = factor(paradigm, 
                            levels = c("individual_sp", 
                                       "community", 
                                       "other"), 
                            labels = c("individual species questions", 
                                       "community questions", 
                                       "other types of questions")), 
                 y = n_studies)) + 
        geom_point() + 
        geom_linerange(aes(x = factor(paradigm, 
                            levels = c("individual_sp", 
                                       "community", 
                                       "other"), 
                            labels = c("individual species questions", 
                                       "community questions", 
                                       "other types of questions")), 
                      ymin = l_95ci, 
                      ymax = u_95ci)) + 
        xlab(element_blank()) + 
        ylab("Number of Studies") + 
        theme_bw())
```

**Dina - Do I need to do some sort of explicit test to see if the number of studies in each category is different, or is it enough to just say that the 95% bootstrap CIs don't overlap?  A Poisson regression w/counts as outcome and question types as predictors doesn't make sense here, as I don't have replicate observations of counts, right (I only have a single set of studies, with a count of number of studies using each focus)**

### Specific Study Focus

```{r specific_question_by_paradigm_bar_plot}
qt_df <- select(wg, title, 
                community.question, individual.species.question, 
                response.variable...species.richness, 
                response.variable...diversity, 
                response.variable...distribution, 
                response.variable...abundance, response.variable...phenology,
                trends.over.time, methodology.development.or.analysis) 
names(qt_df)[4:10] <- c("species.richness", "diversity", 
                        "distribution", "abundance", "phenology", 
                        "temporal.trends", "methodology")

qt_df <- gather(qt_df, key = "paradigm", value = "parad_val", 
                community.question, individual.species.question,  
                factor_key = T) %>%
  filter(parad_val == TRUE) %>%
  gather(key = "focus", value = "foc_val", 
         species.richness, diversity, distribution, abundance, phenology, 
         temporal.trends, methodology, 
         factor_key = T) %>%
  filter(foc_val == TRUE) %>%
  group_by(paradigm, focus) %>%
  summarise(count = n()) %>% 
  # filter odd combinations of paradigm and focus
  filter(!(paradigm == "community.question" & 
             focus %in% c("distribution", "abundance", "phenology"))) %>%
  filter(!(paradigm == "individual.species.question" & 
             focus %in% c("species.richness", "diversity"))) 

ggplot(data = qt_df, 
       aes(x = factor(paradigm, 
                      levels = c("community.question", 
                                 "individual.species.question"), 
                      labels = c("community", "individual species")), 
           y = count, 
           fill = factor(focus, 
                      levels = c("species.richness", "diversity", 
                                 "distribution", "phenology", "abundance", 
                                 "temporal.trends", "methodology"), 
                      labels = c("species richness", "diversity", 
                                 "distribution", "phenology", "abundance", 
                                 "temporal trends", "methodology")))) + 
  geom_bar(position = "dodge", stat = "identity") + 
  scale_fill_brewer(type = "qual", palette = 2) + 
  #scale_fill_grey(end = 0.7) + 
  xlab("Study Focus") + 
  ylab("Number of Studies") + 
  theme_bw() + 
  theme(legend.title = element_blank())
```


[ Descriptive stats for studies focusing on alien species, testing ecological theory, and testing methods of data correction. ]

### What data types are used for each study question paradigm?

```{r make_study_question_paradigm_by_dataType_tibble, eval = F, include = F}
## I don't think a table is a good way to present this.  Too hard to see.
## But if I include this again later, should add all data type variables.
paradigm_data_df <- select(wg, title, individual.species.question, 
                      community.question, data.type...what.where.when.only, 
                      data.structure...sampling.effort.known, 
                      data.structure...non.detection, 
                      data.type...abundance) %>%
  gather(key = "paradigm", value = "parad_used", individual.species.question:
           community.question) %>%
  filter(parad_used == TRUE) %>%
  gather(key = "data_type", value = "dat_used", 
         data.type...what.where.when.only:data.type...abundance) %>%
  filter(dat_used == TRUE)

paradigm_data_df$data_type <- gsub("data.type...what.where.when.only", 
                                   "what, where, when only", 
                                   paradigm_data_df$data_type)
paradigm_data_df$data_type <- gsub("data.type...sampling.effort.known", 
                                   "sampling effort", 
                                   paradigm_data_df$data_type)
paradigm_data_df$data_type <- gsub("data.structure...non.detection", 
                                   "detection / non-detection", 
                                   paradigm_data_df$data_type)
paradigm_data_df$data_type <- gsub("data.type...abundance", 
                                   "abundance", 
                                   paradigm_data_df$data_type)
paradigm_data_df$paradigm <- gsub("community.question", 
                                   "community", 
                                   paradigm_data_df$paradigm)
paradigm_data_df$paradigm <- gsub("individual.species.question", 
                                   "individual species", 
                                   paradigm_data_df$paradigm)

kable(table(paradigm_data_df$data_type, paradigm_data_df$paradigm))
```


```{r data_type_by_paradigm_barplot}
### plot data type by study question paradigm ----------------------
dt_pd_df <- select(wg, data.type...what.where.when.only, 
      data.structure...sampling.effort.known, 
      data.type...abundance, 
      data.structure...non.detection, 
      data.structure...organized.data.collection.scheme, 
      data.structure...multiple.datasets.integrated.for.analysis, 
      data.type...life.stage, 
      data.type...voucher.of.some.kind.necessary.for.analysis,
      community.question, 
      individual.species.question) %>%
  gather(key = "data_type", value = "dt_val", data.type...what.where.when.only:
           data.type...voucher.of.some.kind.necessary.for.analysis) %>%
  filter(dt_val == TRUE) %>%
  gather(key = "paradigm", value = "pd_val", community.question, 
         individual.species.question) %>%
  filter(pd_val == TRUE) %>%
  count(paradigm, data_type) %>%
  complete(paradigm, data_type, fill = list(n = 0))

ggplot(dt_pd_df, 
       aes(x = factor(paradigm, 
                      levels = c("community.question", 
                                 "individual.species.question"), 
                      labels = c("community", "individual species")), 
           y = n, 
           fill = factor(data_type, 
                         levels = c("data.type...what.where.when.only", 
                           "data.structure...sampling.effort.known", 
                           "data.type...abundance", 
                           "data.structure...non.detection", 
                           "data.structure...organized.data.collection.scheme", 
                           "data.structure...multiple.datasets.integrated.for.analysis",
                           "data.type...life.stage", 
                           "data.type...voucher.of.some.kind.necessary.for.analysis"), 
                         labels = c("what where when only", 
                                    "sampling effort", 
                                    "abundance", 
                                    "non detection", 
                                    "organized scheme", 
                                    "multiple datasets", 
                                    "life stage", 
                                    "voucher available")))) + 
  geom_bar(position = position_dodge(preserve = "single"), stat = "identity") + 
  ggtitle("Data use based on study question paradigm") + 
  scale_fill_discrete(name = "Data Type") + 
  xlab("Study question paradigm") + 
  ylab("Number of studies") + 
  scale_y_continuous(breaks = seq(0, max(dt_pd_df$n), 5)) + 
  theme_bw() 
```

**Jon - Is there any useful statistical work to be done here? TODO: Yes, fit full model w/interaction and w/out interactin to see if distribution of bars is different in community v. individual sp studies.**  I think the story is that data use is roughly similar between question paradigms, except maybe that voucher data seems to be used more often in individual species studies.  Also integrating multiple datasets for analyses is common in both question paradigms, and so it is worth making an effort to ensure that published datasets are in standardized, compatible formats.  

## Analysis approach & data type
[**TODO**]

-------------------------------

## Authors and Data Providing Institutions (TODO)

**Who choses to use biological records data?**

$H_a$: The primary author is more likely to use data held by their own institution than by other institutions.

$H_0$: The number of studies with a lead author from the institution that provided the data is the same as expected if each lead author chose a co-lead author and data providing institution at random from the population of lead authors and data providing institutions that appear on a study in this review.

Lead authors (the first and/or last authors on a study) were significantly more likely to use biological records data provided by their own institution (empirical permutation *p*-value = `r p_auth_perm`).  Over a quarter of the studies `r lead_auth_inst_boot$t0`, 95% bootstrap CI [`r boot.ci(lead_auth_inst_boot)$bca[4]`, `r boot.ci(lead_auth_inst_boot)$bca[5]`]) used data provided by the institution of one of the lead authors, and two-thirds of the studies (proportion = `r any_auth_inst_boot$t0`, 95% bootstrap CI [`r boot.ci(any_auth_inst_boot)$bca[4]`, `r boot.ci(any_auth_inst_boot)$bca[5]`]) had at least one author associated with the data providing institution.


_____________________________________________________________________
# Discussion

## Reader Agreement
Consider adding a step after the agreement calculation where I summarize studies for which there was a lot of disagreement overall - ie disagreement on many categories.  These may be particularly perplexing, novel, or creative studies that could be removed from the formal analyses because they are poorly characterized by the existing categories.  But report number removed, and discuss them separately in discussion.  

The seemingly poor agreement between WG's and CP's codings either reflects true ambiguity in how to code the variables or errors by one of the readers.  My review of all cases of disagreement revealed many cases that I believe to be obvious errors or oversights.  Of the obviously erroneous values, most were CP's, though some were mine.  This suggest that either I have given poor instructions to CP or CP is carrying out the instructions poorly.  It is probably not possible to figure out which of these things is happening without having a third reader, and preferable one with a higher skill level.  In some cases, my instructions were clearly insufficient.  I have now edited the instructions and hopefully solved those problems for the 12 variables CP double coded (though similar inadequate instructions probably still exist for the other variables).  The best test now would be to test these same 12 variables with another reader with a higher level of ability.  If that reader also has substantial disagreement, then either the categories themselves or the instructions will need to be majorly revised.  If that $3^{rd}$ higher-ability reader agrees with my original codings better, then it indicates that a highly skilled reader is required for the second readings.  

The technique of having me do a "tie breaker" evaluation of all disagreement cases is a partial solution.  This method would basically treat the $2^{nd}$ reader's codings as a check for error's in my coding that trigger a tie breaker review.  Cases in which the $2^{nd}$ readers coding is judged to be obviously erroneous would not be of much concern, and would be attributed to insufficient knowledge (e.g. not knowing that "fungarium" is a collection of physical specimens) or to skimming the articles quickly rather than reading them (CP seemed to spend less than 30 minutes per article).  Cases in which my original coding is judged to be obviously erroneous would be of concern.  Those values could be corrected for analysis, and/or the overall reliability of my original codings would need to be evaluated in some way to determine if a $2^{nd}$ check is needed for all studies and variables or whether my errors are few enough that they introduce an acceptable amount of error/uncertainty into the analysis. The ideal soluttion would be to have a highly-skilled second reader and to calculate Krippendorff's alpha for agreement between the two readers.  However, lacking such a person, I think it would be acceptable to use a less skilled second reader as a trigger for a 3rd tie breaker review done by me.  I do need to decide whether the trigger/tie breaker approach requires checking all studies and variables, or whether there is an adequate way to estimate the reliability of my original codings. I am not sure whether Ellie would be knowledgable enough to be a highly-skilled second reader.  Perhaps not. 

A final solution that is perhaps worth pursuing is only coding and analyzing variables for which coding can be done based entirely or largely on character string matching.  This would provide greater reliability, but at the cost of less interesting, informative, and insightful analysis (Krippendorff 2004, Section 11.1).

There are two goals of having two readers.  First, to estimate how replicatable the analysis is.  If another researcher could code the articles in the same way I do, then it provides confidence that the analysis is revealing some sort of true characteristics of the literature.  Second, to improve the quality of this specific analysis by identifying and correcting sloppy errors on my part.  This is not necessarily replicatable, though, as I am doing the correcting, so the categories basically still show "Willson's inner vision of the literature" rather than some more widely true characteristics. 

## Who uses biological records?
While the number of studies that used data provided by the instituion of one of the lead authors was higher than expected by chance, the majority of studies did not have a lead author associated with the data providing institution.  If the lead authors are primarily the people who concieve of and design a study, then these results suggest that authors frequently use data that they are not directly involved in curating, perhaps because they do not already have the data necessary to answer their research question.  This suggests that data providing institutions perform an important data sharing role, and the majority of research that uses biological records data could not have been done solely using the data resources of the lead authors.  

Most studies in this review (`r any_auth_inst_boot$t0`) did have at least one author associated with the data providing institution.  Co-authorship may be a prefered way for data providers to recieve credit for their role in the research pipeline, especially when the data form a "non-trivial" contribution to the study (@Roche2014, @Whitlock2011).  This could reflect [**do I have some reference about the messy nature of bio recs?**].

While standards for citing datasets are developing [**need citations**], there still seems to be some uncertainty about when and how to cite data resources [**I have a citation for this, right?**] which may motivate data providers to seek recognition through co-authorship.  

## Assorted bits to add
[[@Follett2015] found that methodology was focus of 17% of studies in a systematic review of cit. sci. use in publications. ]

[[@Follett2015] found that "the public preferred to publish their outcomes in" non peer reviewed places.  This is part of why I include grey literature. ] 

[[@Pearce-Higgins2018] Some data collection / providing schemes have a business model of getting funding to do research.  This might be particularly important for designed monitoring schemes, because the study authors can influence data collection methods.  ]

[[@Pearce-Higgins2018] warn of risks of free and open data being used without data providers knowledge and input.  Risks include inappropriate analysis or interpretation.  "Although not commonly regarded as a major issue in other disciplines where PDA is the norm, the complexities of ecological data can make archiving for independent use particularly challenging (Kenall et al. 2014, Mills et al. 2015)" [@Pearce-Higgins2018]. ]

[In a review of ecological and environmental citizen science projects, Pocock et al. [-@Pocock2017] distinguished between 'mass participation' projects that tended to have lower data quality, less detailed data, but cover larger geographic areas, and 'systematic monitoring' projects that tended to collect more complex and possible higher-quality data (e.g. counts instead of presence/absence observations) but covered smaller geographic areas.]




# References







