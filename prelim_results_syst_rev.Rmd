---
title: "Biological Records Systematic Review - Preliminary Results"
csl: ecology.csl
output:
  html_document: default
  fig_caption: yes
  html_notebook: default
  word_document: null
bibliography: phd_sources_Aug2018.bib
---

**TODO: As of 10 Sep, I have removed columns from the 'systematic_review_coded_results.ods' spreadsheet.  Need to check to make sure that this all still runs appropriately (e.g. that columns are not accessed by number position, etc).**

**TODO:**

- Methods
    - *Study eligibiligy* [DONE]
    - Article coding
        - Data Type
        - Study Questions
        - *Analysis approach* [DONE]
        - *Agreement* [DONE]
    - *Temporal Extent* [DONE]
    - Authors and Data Providing Institutions
    - Study Question
        - Paradigms
        - Specific focus
    - Analysis approach & data type
- Results - Statistical
    - Search results
        - eligibility, n vars coded, n studies coded, n double coded
    - Reader agreement
    - *Temporal extent* [DONE]
    - Authors and Data Providing Institutions
    - Study Question
        - *Paradigms* [DONE]
    - Analysis approach & data type
- Results - Descriptive
    - Spatial extent
    - *Study Question* [DONE]
        - *Data type by question paradigm* [DONE]
        - *Specific focus* [DONE]
    - Author associated with data provider
    - Spatial bias correction
    - Taxonomic group
    - Role of biological records
    - Prediction performance measure
    - Tabulate analysis methods
- Discussion
    - Article coding
        - Agreement
    - Temporal extent
    - Study question
    - Analysis approach & data type
 

```{r, echo=F}
knitr::opts_chunk$set(echo=F, message=F, warning=T, cache=F)
```

```{r, load_data, cache = F, results='hide', warning = F}
setwd("~/Documents/Data_Analysis/UCD/systematic_review/")
library(wgutil)
library(Hmisc)
library(irr)
library(captioner)
library(knitr)
library(pander)
library(ggridges)
library(bestglm)
library(car)
library(GGally)
library(tidyverse)

diag <- F # produce exploratory & diagnostics plots & results
diag_present <- T # produce diagnostics for report to Dina

if(diag == F) { # turn off warnings when not running diagnostics
  knitr::opts_chunk$set(echo=F, message=F, warning=F, cache=F)
}

source("./clean_data_syst_rev.R")
```

```{r calc_temp_extent}
# calculate number of years covered by study
rev$temp_extent <- as.numeric(rev$end.year) - as.numeric(rev$start.year)
# individually assign values to studies with temporal extent < 1 year
if(diag) rev$title[rev$temp_extent == 0] 
rev$temp_extent[which(
  grepl("An assessment of bumblebee .* land use and floral.*", 
        rev$title))] <- 0.33
```

```{r voucher_data_to_whwhwh, include = F, eval = F}
## TODO: eventually I can delete this chunk as I am not using it and don't
## expect to in the future.
## 
## This chunk could be turned on if I decide that:
## For the purposes of analysis, if there was a specimen available but the 
# specimen was not used at all for analysis, then, if there are no other richer
# data types, I will consider the data what, where, when-only.  I think this
# best represents the use of physical.specimen data.  I do not think this would 
# make sense
# for other data types (e.g. if there is abundance data but the analysis only
# uses it as what, where, when) because that is a choice to strip usually 
# meaningful information.  The physical specimen is likely not meaningful for
# many analyses that use physical.specimens as a source of wh, wh, wh data.

## But as of 23 August 2018 I think I will keep the physical.specimen data as 
## "voucher available" and not transform the whwhwh column.  

print("Modifying what, where, when data type to TRUE if voucer specimen was not used.")
for(i in 1:nrow(rev)) {
  if(rev$data.type...physical.specimen[i] == T | rev$data.type...photo[i] == T | 
     rev$data.type...audio[i] == T | rev$data.type...video[i] == T) {
    # If a voucher is available
    if(rev$data.type...abundance[i] == F & 
       rev$data.type...sampling.effort.reported[i] == F & 
       rev$data.type...organized.data.collection.scheme[i] == F & 
       rev$data.type...visit.specific.covariates[i] == F & 
       rev$data.type...life.stage[i] == F) {
      # If there are no other rich data types
      if(rev$voucher.of.some.kind.necessary.for.analysis[i] == F) {
        # If the voucer was not used
        rev$data.type...what.where.when.only[i] <- TRUE
      }
    }
  }
} 
```

```{r numberingPrep}
# make functions for adding numbered captions to figures 
figs <- captioner(prefix = "Fig.") 
tabs <- captioner(prefix = "Table")
```

# Preliminary structure of analysis and results
This document states every question I plan to ask and shows methods, figures, tables and results text that I expect to use to answer each question in the biological records systematic review. Specific results and interpretations will change when I code the rest of the articles.

# Methods
We did a systematic review of original research published since 2014 that used biological records from Ireland and the UK.  We generated a list of relevant articles from Web of Science, Scopus, ProQuest, GoogleScholar, and the Global Biodiversity Information Facility (GBIF) website.  I evaluated each article for inclusion eligibility and coded information on [**??**] characteristics for each eligible article.  This coding was validated for a subset of articles by a second reader.  Agreement between the two readers was evaluated using Krippendorff's alpha (Krippendorff 2004).

## Study eligibility
Studies were eligible for inclusion if they met all of the following criteria:

1) original research
2) English language
3) used opportunistic biological data collected with non-standardized or semi-standardized protocols
4) included (but were not necessarily limited to) data from Ireland or the UK
5) the full text of the study was available through the UCD library online platform, Google, GoogleScholar, or ResearchGate.  
6) performs at least one analysis of the data (data papers and biotic atlases were not eligible)
7) sample size greater than 20
8) not restricted to fossil records

Grey literature was included and therefore peer review was not required.  Studies using semi-standardized data collection schemes (e.g. UK Butterfly Monitoring Scheme) were included as long as they included some opportunistic elements (e.g. locations chosen opportunistically by volunteers). Studies for which all data was collected by the study authors were excluded.  

## Article coding
### Data Type
We coded twelve variables describing aspects of data type: what, where, when only; sampling effort reported; abundance; detection / non-detection; organized monitoring scheme; visit-specific covariates; multiple datasets integrated for analysis; life stage information; physical specimen; photo; audio; video. Data types are not mutually exclusive except fot the "what where when only" data type, which cannot be true if any other data type is true.  We considered "what, where, when" as the baseline data type and considered all other data types supplementary additions to that basic data type.

For most analyses, we grouped the data types "physical specimen", "photo", "audio", and "video" into a "voucher specimen" group, but we coded them individually in order to identify emerging trends in how vouchers are collected.

For statistical analyses, we kept data type variables in models based on *a-priori* expectations about the variables' influence on data analysis strategy and based on expectations about different levels of complexity and effort involved in collecting, recording, and storing the data.  The data type variables we kept in models were: what, where, when only; sampling effort reported; abundance; detection / non-detection; organized monitoring scheme; visit-specific covariates; multiple datasets integrated for analysis; life stage information; voucher specimen. 

### Study Question

### Analysis approach
We classified the broad analysis approach used by each study into one of three categories: 1) inference; 2) prediction; or 3) descriptive only.  For each study, we evaluated the analysis approach only for analyses within the study that used biological records data.  

We considered studies estimating parameters and reporting some measure of uncertainty (e.g. *p*-values, confidence intervals, posterior probability distributions) to be using inference.  We considered studies that built models and made predictions to be using an analysis strategy of prediction, even if no uncertainty or inference was reported with the predictions.  Finally, we considered the analysis approach to be descriptive only if results were descriptive with no prediction or inference.  Descriptive results could be narrative, graphical, or quantitative as long as no inference or uncertainty was reported (e.g. point estimates of descriptive statistics without any confidence interval or *p*-values).  Studies could use both inference and prediction as analysis strategies and so those categories were not mutually exclusive, but studies were only categorized as using a descriptive analysis approach if all results were descriptive only and the study used no inference or prediction.  

### Agreement between two readers
I used Krippendorff's *alpha* [@Krippendorff2004] to evaluate the agreement between two readers for each of `r ncol(cp)-1` variables that had been coded by two people.  [**Dina, any thoughts about Krippendorff's alpha or other measures of agreement?**]. I calculated Krippendorff's *alpha* using the `kripp.alpha` function in the `irr` package [@irr2012].  For each case of disagreement between the two readers, I reviewed the article.  When the disagreement seemed to be due to an oversight or an obvious error (e.g. when one reader coded "results type - inference" as FALSE but the article reported 95% confidence intervals around estimates), I corrected the erroneous value.  In cases where the disagreement was not the result of an obvious oversight, I did not change either value.  I calculated Krippendorff's alpha to assess agreement both before and after I corrected obviously erroneous codings. 

Krippendorff's *alpha* for binary data is calculated as:

$\alpha = 1 - D_O/D_E$

Where $D_o$ is the observed disagreement between readers and $D_e$ is the disagreement expected by chance.  The number of agreements (and disagreements) expected by chance is calculated using the observed values by calculating a coincidence matrix of values expected by chance:

|   |    0     |     1    |
|---|----------|----------| 
| 0 | $E_{00}$ | $E_{01}$ | 
| 1 | $E_{10}$ | $E_{11}$ | 

where $E_{00} = n_0 \times\ (n_0 - 1)/(n-1)$

$E_{01} = E_{10} = n_0 \times\ n_1/(n - 1)$

$E_{11} = n_1 \times\ (n_1 - 1) / (n - 1)$

$n$ is the number of observations (twice the number of coded units when there are two observers), $n_0$ is the number of zeros in all observed pairs, and $n_1$ is the number of ones in all observed pairs.

The coincidence matrix for observed values is:

|   |    0     |     1    |
|---|----------|----------|
| 0 | $O_{00}$ | $O_{01}$ | 
| 1 | $O_{10}$ | $O_{11}$ | 

"Coincidences sum contingencies and their inverses, thereby omitting the references to the individual observers..." @Krippendorff2004



```{r inter_coder_agreement_subset_dfs, warning=TRUE}
## casey's pre-calibration results ------------------------------------------
# remove columns that aren't coded variables
cp_pre_cal <- cp_pre_cal[, which(colnames(cp_pre_cal) %nin% 
                                   c("link", "qualifies", "authors", 
                                     "publication", "doi", "year", 
                                     "coding.DONE"))]
# rename the old column names used in casey's 1st coding
colnames(cp_pre_cal)[grep(".*known.effort", colnames(cp_pre_cal))] <- 
  "data.type...sampling.effort.reported"
colnames(cp_pre_cal)[grep(".*semi.structured.*", colnames(cp_pre_cal))] <- 
  "data.type...organized.data.collection.scheme"

cp_pre_cal <- cp_pre_cal[order(cp_pre_cal$title), ] # order rows
cp_pre_cal <- cp_pre_cal[, order(colnames(cp_pre_cal))] # order columns
# put title column first
cp_pre_cal <- cp_pre_cal[, c(which(colnames(cp_pre_cal) == "title"), 
                             which(colnames(cp_pre_cal) != "title"))]
## end casey's pre-calibration results ---------------------------------------

## casey's post-calibration results before error correction ----------------------
# remove columns that aren't coded variables from casey's df
cp_post_cal <- cp_orig[, which(colnames(cp_orig) %nin% 
                             c("link", "qualifies", "authors", "publication", 
                               "doi", "year", "coding.DONE"))]
cp_post_cal <- cp_post_cal[order(cp_post_cal$title), ] # order rows
cp_post_cal <- cp_post_cal[, order(colnames(cp_post_cal))] # order columns
# put title column first
cp_post_cal <- cp_post_cal[, c(which(colnames(cp_post_cal) == "title"), 
                       which(colnames(cp_post_cal) != "title"))]
## end casey's post-calibration results ------------------------------------

## all casey's results before error correction --------------------------------
# remove columns that aren't coded variables from casey's df
cp_orig <- cp_orig[, which(colnames(cp_orig) %nin% 
                             c("link", "qualifies", "authors", "publication", 
                               "doi", "year", "coding.DONE"))]
cp_orig <- cp_orig[order(cp_orig$title), ] # order rows
cp_orig <- cp_orig[, order(colnames(cp_orig))] # order columns
# put title column first
cp_orig <- cp_orig[, c(which(colnames(cp_orig) == "title"), 
                       which(colnames(cp_orig) != "title"))]

## end casey's results before error correction --------------------------------

## my results before correction  ------------------------------------------------
# subset my coded results to titles and columns coded by casey
mult_coded_rev_orig <- rev_orig[which(rev_orig$title %in% cp_orig$title), 
                      which(colnames(rev_orig) %in% colnames(cp_orig))]
# order rows
mult_coded_rev_orig <- mult_coded_rev_orig[order(mult_coded_rev_orig$title), ] 
# order cols
mult_coded_rev_orig <- mult_coded_rev_orig[, order(colnames(mult_coded_rev_orig))] 
mult_coded_rev_orig <- mult_coded_rev_orig[, c(
  which(colnames(mult_coded_rev_orig) == "title"), 
  which(colnames(mult_coded_rev_orig) != "title"))] # put title column first
### end my results before correciton -----------------------------------------

## casey's results after error correction -----------------------------------
# remove columns that aren't coded variables 
cp <- cp[, which(colnames(cp) %nin% 
                             c("link", "qualifies", "authors", "publication", 
                               "doi", "year", "coding.DONE"))]
cp <- cp[order(cp$title), ] # order rows
cp <- cp[, order(colnames(cp))] # order columns
# put title column first
cp <- cp[, c(which(colnames(cp) == "title"), 
                       which(colnames(cp) != "title"))]
## end casey's results after error correction ---------------------------------

## my results after error correction ----------------------------------------
# subset my coded results to titles and columns coded by casey
mult_coded_rev <- rev[which(rev$title %in% cp$title), 
                      which(colnames(rev) %in% colnames(cp))]
# order rows
mult_coded_rev <- mult_coded_rev[order(mult_coded_rev$title), ] 
# order cols
mult_coded_rev <- mult_coded_rev[, order(colnames(mult_coded_rev))] 
mult_coded_rev <- mult_coded_rev[, c(
  which(colnames(mult_coded_rev) == "title"), 
  which(colnames(mult_coded_rev) != "title"))] # put title column first
## end my results after error correction --------------------------------------


## make sure all titles are present
if(any(cp_pre_cal$title %nin% mult_coded_rev_orig$title)) {
  warning("Some titles in cp_pre_cal are not in mult_coded_rev_orig. cp_pre_cal will be subsetted.")
  cp_pre_cal <- cp_pre_cal[which(cp_pre_cal$title %in% 
                                   mult_coded_rev_orig$title), ]
}
if(any(cp_orig$title %nin% mult_coded_rev_orig$title)) {
  warning("Some titles in cp_orig are not in mult_coded_rev_orig. cp_orig will be subsetted.")
  cp_orig <- cp_orig[which(cp_orig$title %in% mult_coded_rev_orig$title), ]
}
if(any(mult_coded_rev_orig$title %nin% cp_orig$title)) {
  warning("Some titles in mult_coded_rev_orig are not in cp_orig. mult_coded_rev_orig will be subsetted.")
  mult_coded_rev_orig <- mult_coded_rev_orig[which(mult_coded_rev_orig$title %in%
                                           cp_orig$title), ]
}
if(any(mult_coded_rev$title %nin% cp$title) | 
   any(cp$title %nin% mult_coded_rev$title)) {
  warning("cp and mult_coded_rev don't have the same titles. One or both of those data frames will be subsetted.")
  mult_coded_rev <- mult_coded_rev[which(mult_coded_rev$title %in% cp$title), ]
  cp <- cp[which(cp$title %in% mult_coded_rev$title), ]
}

## make sure df rows and cols are in the same order
if(!identical(cp_orig$title, mult_coded_rev_orig$title)) {
  stop("Study titles in cp_orig and mult_coded_rev_orig are either not the same or are not in the same order.  They must be in the same order for creating the variable data frames that will be used to calculate Krippendorf's alpha. ")
}
if(!identical(colnames(cp_orig), colnames(mult_coded_rev_orig))) {
  stop("Columns in cp_orig and mult_coded_rev_orig must be identical and in the same order. ")
}
if(!identical(colnames(cp_pre_cal), colnames(mult_coded_rev_orig))) {
  stop("Columns in cp_pre_cal and mult_coded_rev_orig must be identical and in the same order. ")
}
if(!identical(colnames(cp), colnames(mult_coded_rev))) {
  stop("Columns in cp and mult_coded_rev must be identical and in the same order. ")
}
```

```{r define_calibration_set}
calibration_set <- cp_pre_cal$title
```

```{r make_dfs_of_pre_calibration_double_coded_articles}
pre_cal_list <- list()

rev_pre_cal <- mult_coded_rev_orig[which(mult_coded_rev_orig$title %in% calibration_set), ]

if(colnames(cp_pre_cal)[1] != "title") {stop("The first column of cp_pre_cal and mult_coded_rev_orig must be the study title.")}

for(i in 2:ncol(cp_pre_cal)) { # awkward subsetting is b/c of tibble format
  pre_cal_list[[i-1]] <- data.frame(matrix(data = c(cp_pre_cal[, i][[1]],
                                                 rev_pre_cal[, i][[1]]),
                                        nrow = 2, ncol = nrow(cp_pre_cal), 
                                       byrow = T))
}
names(pre_cal_list) <- colnames(cp_pre_cal)[2:ncol(cp_pre_cal)]
```

```{r make_dfs_of_post_calibration_double_coded_articles}
post_cal_list <- list()

cp_post_cal <- cp_orig[which(cp_orig$title %nin% calibration_set), ]
rev_post_cal <- mult_coded_rev_orig[which(mult_coded_rev_orig$title %nin% calibration_set), ]

if(colnames(cp_post_cal)[1] != "title") {stop("The first column of cp_post_cal and mult_coded_rev_orig must be the study title.")}

for(i in 2:ncol(cp_post_cal)) { # awkward subsetting is b/c of tibble format
  post_cal_list[[i-1]] <- data.frame(matrix(data = c(cp_post_cal[, i][[1]], 
                                                     rev_post_cal[, i][[1]]),
                                            nrow = 2, ncol = nrow(cp_post_cal), 
                                            byrow = T))
}
names(post_cal_list) <- colnames(cp_post_cal)[2:ncol(cp_post_cal)]
```

```{r make_dfs_of_double_coded_variables_all_studies_pre_correction}
## make a data frame for each double-coded variable
# each row is codings from one person
# each column is a study
pre_correct_list <- list()

if(colnames(cp_orig)[1] != "title") {stop("The first column of cp_orig and mult_coded_rev_orig must be the study title.")}

for(i in 2:ncol(cp_orig)) { # awkward subsetting is b/c of tibble format
  pre_correct_list[[i-1]] <- data.frame(matrix(data = c(cp_orig[, i][[1]], 
                                                 mult_coded_rev_orig[, i][[1]]),
                                        nrow = 2, ncol = nrow(cp_orig), byrow = T))
}
names(pre_correct_list) <- colnames(cp_orig)[2:ncol(cp_orig)]
```

```{r make_dfs_of_double_coded_variables_all_studies_post_correction}
# each row is codings from one person
# each column is a study
post_correct_list <- list()
if(colnames(cp)[1] != "title" | colnames(mult_coded_rev)[1] != "title") {
  stop("The first column of cp and mult_coded_rev must be the study title")}

for(i in 2:ncol(cp)) {
  post_correct_list[[i-1]] <- data.frame(matrix(
    data = c(cp[, i][[1]], mult_coded_rev[, i][[1]]), 
    nrow = 2, ncol = nrow(cp), byrow = T))
}
names(post_correct_list) <- colnames(cp)[2:ncol(cp)]
```

```{r krippendorfs_alpha_calc, warning=FALSE, message=FALSE}
do_kripp.alpha <- function(x, method) {
  # function to call kripp.alpha while converting dataframe x to a matrix
  # ARGS: x - data frame in which rows are observers and columns are studies
  #       method - character string of method to pass to kripp.alpha
  kripp.alpha(as.matrix(x), method = method)
}

kripp_all_pre_correct <- lapply(pre_correct_list, FUN = do_kripp.alpha, 
                               method = "nominal")
kripp_all_post_correct <- lapply(post_correct_list, FUN = do_kripp.alpha, 
                                 method = "nominal")
kripp_pre_cal <- lapply(pre_cal_list, FUN = do_kripp.alpha, method = "nominal")
kripp_post_cal <- lapply(post_cal_list, FUN = do_kripp.alpha, 
                         method = "nominal")
```

```{r calc_kripp_wg_before_and_after_correction, include = F, eval = F}
## This is a bit dumb, as I didn't do a 3rd coding for all articles, so many of
## the coded values are carried directly over from the 1st coding and are thus
## very correlated.  I don't think this is a very worthwhile way to look at 
## things, and I should probably abandon it.
# calculate agreement between my codings before and after I corrected them based
# on CP's codings.  This will perhaps show how reliable my original codings are.
# The assumption would be that if my original codings are reliable compared to
# my double-checking when prompted, then we can at least have confidence that
# my coding is consistent and I'm not making too many sloppy errors.
wg_comparison_list <- list()

if(colnames(mult_coded_rev)[1] != "title" | colnames(mult_coded_rev_orig)[1] != "title") {
  stop("The first columns of mult_coded_rev_orig and mult_coded_rev must be the study title.")}
if(nrow(mult_coded_rev) != nrow(mult_coded_rev_orig)) {
  warning("There are different numbers of rows in mult_coded_rev and mult_coded_rev_orig. This may be due to studies that were disqualified in the data cleaning script after review because of differing reader scorings.")
  mult_coded_rev_orig <- mult_coded_rev_orig[which(mult_coded_rev_orig$title %in%
                                                     mult_coded_rev$title), ]
  mult_coded_rev <- mult_coded_rev[which(mult_coded_rev$title %in%
                                           mult_coded_rev_orig$title), ]
}

for(i in 2:ncol(mult_coded_rev)) {
  wg_comparison_list[[i-1]] <- data.frame(matrix(
    data = c(mult_coded_rev_orig[, i][[1]], mult_coded_rev[, i][[1]]), 
    nrow = 2, ncol = max(nrow(mult_coded_rev), nrow(mult_coded_rev_orig)), 
    byrow = T))
}
names(wg_comparison_list) <- colnames(cp)[2:ncol(cp)]

## calculate Krippendorff's alpha
kripp_wg_comparison <- lapply(wg_comparison_list, FUN = do_kripp.alpha, 
                              method = "nominal")
```





--------------------------

## Temporal Extent

$H_a$: The mean temporal extent of studies using only what, where, when data is longer than the mean temporal extent of studies using richer data types. 

$H_0$: The mean temporal extent of studies using what, where, when-only data is the same as the mean temporal extent of studies using richer data types. 

*Proposed Test*: Linear regression with natural-log transformed time (in years) as the outcome and data types as predictors.

We used linear regression with natural-log transformed time (in years) as the response variable.  We used nine binary predictor variables indicating whether the data had the following information: 1) only what where when data, 2) sampling effort, 3) abundance, 4) detection / non-detection, 5) organized monitoring scheme, 6) visit-specific covariates, 7) multiple different datasets, 8) life stage information, and 9) voucher specimens.  While the predictor variables are correlated, we kept all variables in the multiple regression model because we are interested in the effect of each additional type of information *after accounting for other information types* (e.g. is there an additional effect of having abundance data after adjusting for whether the data came from an organized monitoring scheme).

Note that because the variables are not mutually exclusive these are each individual variables - these are not dummy variables representing a single categorical factor level "data type" variable.  

The intercept-only model in this analysis is the mean temporal extent of all studies.  


```{r time_assess_correlation_predictors, fig.height=8, fig.width = 8}
### assess correlation between data type predictors ----------------------
dt_df <- rev[, which(
  colnames(rev) %in% 
    c("data.type...what.where.when.only", 
      "data.type...sampling.effort.reported", 
      "data.type...abundance", 
      "data.type...detection...non.detection", 
      "data.type...organized.data.collection.scheme", 
      "data.type...visit.specific.covariates", 
      "data.type...multiple.datasets.integrated.for.analysis", 
      "data.type...life.stage", 
      "data.type...voucher.available"))]
for(i in 1:ncol(dt_df)) {
  dt_df[, i] <- as.factor(as.character(data.frame(dt_df)[, i]))
}

print(ggpairs(dt_df, columns = 1:ncol(dt_df), 
              upper = list(discrete = "ratio"),
              lower = list(discrete = "facetbar"), 
              diag = list(discrete = "barDiag"), 
              columnLabels = c("what where when only", 
                               "sampling effort",
                               "abundance",
                               "non detection", 
                               "organized scheme", 
                               "visit specific covariates", 
                               "multiple datasets", 
                               "life stage", 
                               "voucher available"), 
              labeller = label_wrap_gen(width = 10)) + 
        ggtitle("Correlation of data type predictor variables") + 
        theme(axis.text.x = element_text(angle = 50, hjust = 1)))
```


```{r prepare_temporal_extent_df}
temp_extent <- select(rev, 
                      c(title, temp_extent, 
                        data.type...what.where.when.only, 
                        data.type...sampling.effort.reported, 
                        data.type...abundance,
                        data.type...detection...non.detection,
                        data.type...organized.data.collection.scheme, 
                        data.type...visit.specific.covariates, 
                        data.type...multiple.datasets.integrated.for.analysis,
                        data.type...life.stage, 
                        data.type...voucher.available)) %>%
  gather(key = "data_type", value = "dt_value", 
         data.type...what.where.when.only:data.type...voucher.available, 
         factor_key = T) %>%
  filter(dt_value == T) %>%
  group_by(data_type)

# make names prettier
temp_extent$data_type <- gsub("data.type...", "", temp_extent$data_type)

temp_extent$data_type <- factor(as.character(temp_extent$data_type), 
                            levels = c("what.where.when.only", 
                                       "sampling.effort.reported",
                                       "abundance",
                                       "detection...non.detection", 
                                       "organized.data.collection.scheme", 
                                       "visit.specific.covariates", 
                                       "multiple.datasets.integrated.for.analysis", 
                                       "life.stage", 
                                       "voucher.available"), 
                            labels = c("what where when only", 
                                       "sampling effort reported",
                                       "abundance",
                                       "non detection", 
                                       "organized scheme", 
                                       "visit specific covariates", 
                                       "multiple datasets", 
                                       "life stage", 
                                       "voucher available"))
```

Evaluate normality of raw data:
```{r temp_range_distribution}
if(diag_present) {
  # distribution of all data
 # hist(rev$temp_extent)
 # hist(rev$temp_extent[which(rev$temp_extent <= quantile(rev$temp_extent, 
 #                                                        0.90, na.rm = T))])
  ## data are not balanced
  print(table(temp_extent$data_type))
  print("Data are not balanced.")
  
  ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
    geom_boxplot() +
    theme_bw() + 
    ggtitle("All data") + 
    theme(axis.text.x = element_text(angle = 330, vjust = 0.3))
  
  ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
    ylim(c(0, 800)) + 
    geom_boxplot() +
    theme_bw() + 
    ggtitle("Temporal Duration after\ncutting off the 8000 year study") + 
    theme(axis.text.x = element_text(angle = 330, vjust = 0.3))
  
  ggplot(data = temp_extent, aes(x = temp_extent, y = data_type)) + 
    xlim(c(0, 800)) + 
    geom_density_ridges() +
    ggtitle("Temporal Duration after\ncutting off the 8000 year study") + 
    theme_bw()
  
  ## normality -------------------------------------
  # data very non-normal (above) but log transformation kinda fixes this
  # (reciprocal and sqrt transformations did not help)
  print(ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
          geom_boxplot(varwidth = T) +
          scale_y_log10() + 
          theme_bw() + 
          ggtitle("log transformation of year\nmakes normality better") + 
          ylab("Temporal extent (years)") + 
          theme(axis.text.x = element_text(angle = 40, vjust = 0.9, 
                                           hjust = 1)))
  
  print(ggplot(data = temp_extent, aes(x = temp_extent, y = data_type)) + 
          scale_x_log10() + 
          geom_density_ridges(stat = "binline", alpha = 0.4) +
          geom_density_ridges(alpha = 0.4) + 
          xlab("Temporal extent (years)") + 
          theme_bw())
  
  if(diag) {
    for(i in unique(temp_extent$data_type)) { 
      # distribution of log(years) by data type
      hist(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
      qqnorm(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
      qqline(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
    }
  }
}
```

Fit full model and evaluate residuals for assumptions of linear regression.
```{r fit_temp_extent_lm}
## fit linear regression with log-transformed years as response variable
## and each data type as a separate variable 
# fit full model
rev$log_years <- log(rev$temp_extent)
# variables added based on a-priori expectation of most to least important
time_lm_full <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.type...sampling.effort.reported + 
                     data.type...abundance +
                     data.type...detection...non.detection + 
                     data.type...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.type...multiple.datasets.integrated.for.analysis + 
                     data.type...life.stage + 
                     data.type...voucher.available,
                   data = rev, 
                   na.action = na.exclude) 

time_resids <- rstandard(time_lm_full) # calculate residuals
if(diag_present) {
  print("Diagnostics for full model log_years ~ all 9 data types")
  plot(time_lm_full)
  # hist(time_resids, 
  #      main = "standardized residuals of full model for temporal extent", 
  #      xlab = "standardized residuals")
  boxplot(time_resids, ylab = "standardized residuals")
}
time_summary <- summary(time_lm_full)
if(diag) time_summary

## to interpret outcome on original scale:
# for a one-unit increase in x, the predicted value of y 
# changes by (e^beta1 - 1)*100 percent
```

Fitted vs. residuals plot shows some fanning.  

The coefficient estimate for what, where, when data does not produce a good prediction (expected value is lower than any value actual temp extent for those data).  I think there are a couple issues here.  1) the estimate of the effect of what, where, when is *after correcting for multiple datasets used in analysis*, which tends to have long studies.  2) many temp extents are NA, which means sample size is quite small (7-ish or less for wh,wh,wh == T).  So I don't think anything is going wrong with the model.

```{r time_data_type_full_model_sig_test}
# Test significance of full model
m0 <- lm(log_years ~ 1, data = rev) # fit null model 
m_time_full_anova <- anova(m0, time_lm_full)
```

```{r time_data_type_test_main_effects, include = F}
## test effect of each data type predictor by comparing the full model to a
# model with that predictor removed
# 
# TODO: Does this need multiple comparison correction?

m_no_whwhwh <- lm(log_years ~ 1 + 
                    data.type...sampling.effort.reported + 
                    data.type...abundance +
                    data.type...detection...non.detection + 
                    data.type...organized.data.collection.scheme + 
                    data.type...visit.specific.covariates + 
                    data.type...multiple.datasets.integrated.for.analysis + 
                    data.type...life.stage + 
                    data.type...voucher.available,
                  data = rev)
m_no_effort <- lm(log_years ~ 1 + 
                    data.type...what.where.when.only + 
                    data.type...abundance +
                    data.type...detection...non.detection + 
                    data.type...organized.data.collection.scheme + 
                    data.type...visit.specific.covariates + 
                    data.type...multiple.datasets.integrated.for.analysis + 
                    data.type...life.stage + 
                    data.type...voucher.available,
                  data = rev)
m_no_abund <- lm(log_years ~ 1 + 
                   data.type...what.where.when.only + 
                   data.type...sampling.effort.reported + 
                   data.type...detection...non.detection + 
                   data.type...organized.data.collection.scheme + 
                   data.type...visit.specific.covariates + 
                   data.type...multiple.datasets.integrated.for.analysis + 
                   data.type...life.stage + 
                   data.type...voucher.available,
                 data = rev)
m_no_nonDet <- lm(log_years ~ 1 + 
                    data.type...what.where.when.only + 
                    data.type...sampling.effort.reported + 
                    data.type...abundance +
                    data.type...organized.data.collection.scheme + 
                    data.type...visit.specific.covariates + 
                    data.type...multiple.datasets.integrated.for.analysis + 
                    data.type...life.stage + 
                    data.type...voucher.available,
                  data = rev)
m_no_scheme <- lm(log_years ~ 1 + 
                    data.type...what.where.when.only + 
                    data.type...sampling.effort.reported + 
                    data.type...abundance +
                    data.type...detection...non.detection + 
                    data.type...visit.specific.covariates + 
                    data.type...multiple.datasets.integrated.for.analysis + 
                    data.type...life.stage + 
                    data.type...voucher.available,
                  data = rev)
m_no_visitCovs <- lm(log_years ~ 1 + 
                       data.type...what.where.when.only + 
                       data.type...sampling.effort.reported + 
                       data.type...abundance +
                       data.type...detection...non.detection + 
                       data.type...organized.data.collection.scheme + 
                       data.type...multiple.datasets.integrated.for.analysis + 
                       data.type...life.stage + 
                       data.type...voucher.available,
                     data = rev)
m_no_multData <- lm(log_years ~ 1 + 
                      data.type...what.where.when.only + 
                      data.type...sampling.effort.reported + 
                      data.type...abundance +
                      data.type...detection...non.detection + 
                      data.type...organized.data.collection.scheme + 
                      data.type...visit.specific.covariates + 
                      data.type...life.stage + 
                      data.type...voucher.available,
                    data = rev)
m_no_lifeStage <- lm(log_years ~ 1 + 
                       data.type...what.where.when.only + 
                       data.type...sampling.effort.reported + 
                       data.type...abundance +
                       data.type...detection...non.detection + 
                       data.type...organized.data.collection.scheme + 
                       data.type...visit.specific.covariates + 
                       data.type...multiple.datasets.integrated.for.analysis + 
                       data.type...voucher.available,
                     data = rev)
m_no_voucher <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.type...sampling.effort.reported + 
                     data.type...abundance +
                     data.type...detection...non.detection + 
                     data.type...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.type...multiple.datasets.integrated.for.analysis + 
                     data.type...life.stage,
                   data = rev)

indiv_mods_time <- list(m_no_whwhwh, m_no_effort, m_no_abund, m_no_nonDet, 
                        m_no_scheme, m_no_visitCovs, m_no_multData, 
                        m_no_lifeStage, m_no_voucher)
```

## Study Question

**What is the most popular study question paradigm?**

$H_a$: Some broad study question paradigms are more common than others in studies using biological records.

$H_0$: The number of studies pursuing each of the broad study question paradigms is the same.

*Proposed Test* One Poisson regression with study question paradigms (community, individual species, or other) as predictors. 


```{r make_study_question_paradigm_tibble}
paradigm_df <- select(rev, title, individual.species.analysis, 
                      community.analysis) %>%
  gather(key = "paradigm", value = "used", individual.species.analysis:
           community.analysis) %>%
  filter(used == TRUE)

paradigm_counts_df <- group_by(paradigm_df, paradigm) %>%
  summarise(num_studies = sum(used))
paradigm_counts_df[nrow(paradigm_counts_df) + 1, "paradigm"] <- "other"
paradigm_counts_df[which(paradigm_counts_df$paradigm == "other"), 2] <- length(
    which(rev$community.analysis == F & rev$individual.species.analysis == F))
```

```{r question_paradigm_poisson_regression}
## fit poisson regression with number of studies as response and question 
## paradigm as predictor

## check unconditional mean and variance for possible overdispersion
if(diag) {
  mean(paradigm_counts_df$num_studies)
  var(paradigm_counts_df$num_studies) # doesn't look good
}

qp_mod <- glm(num_studies ~ 1 + paradigm, data = paradigm_counts_df, 
              family = "poisson") # full model

# test significance of paradigm as a predictor for number of studies
# Need to look at type-3 sums of squares (function from car package)
qp_full_anova <- Anova(qp_mod, type = 3)  

# check for overdispersion after model fit
if(diag) qp_mod$deviance / qp_mod$df.residual # um, not sure how this works when I have only 1 obs for each predictor

```



---------------------

## Authors and Data Providing Institutions (TODO)

**Who choses to use biological records data?**

$H_a$: The first author is more likely to use data held by their own institution than by other institutions.

$H_0$: The number of studies with a lead author from the institution that provided the data is the same as expected by chance.

*Proposed Test*: Permutation test of lead author institutions.  

----------------------

____________________________________________________________________________

# Results

```{r caption_kripp_summary}
kripp_sum_cap <- figs(name = "kripp_sum_cap", 
                      paste0("Reader agreement for ", 
                             length(which(colnames(mult_coded_rev) != "title")), 
                             " categories measured using Krippendorff's alpha.  The pre-calibration results are for only articles before a meeting to calibrate expectations and definitions (n = ", 
                             length(calibration_set), 
                             ").  Post-calibration results are for only articles coded after the calibration meeting (n = ", 
                             length(which(mult_coded_rev$title %nin% 
                                            calibration_set)), 
                             "). The all_pre_correct group shows results for the original codings for all articles (both those coded before and after the callibration meeting, n = ", 
                             nrow(mult_coded_rev_orig), 
                             ").  The post-correct group shows agreement for all articles (n = ", 
                             nrow(mult_coded_rev), 
                             ") after I evaluated each individual case of disagreement and corrected obvious errors. Dotted lines show values of alpha of 0.667 and 0.8, which are recommended minimum and preferred values at which variables can be relied upon for analysis."))
```

## Search results 
The search returned `r nrow(elig)` potentially relevant studies, of which we have evaluated `r sum(!is.na(elig$qualifies))` for eligibility, and judged `r length(which(elig$qualifies == TRUE))` to be eligible for inclusion in the scoping review.  One reader has coded `r nrow(rev)` articles and a second reader has coded `r nrow(cp_orig)` of those. This preliminary analysis uses the `r nrow(rev)` articles coded so far.


## Reader agreement

Krippendorff's alpha values for the variables that have been double coded indicate substantial disagreement between the two readers, and show few variables for which codings are reliable enough to be used in analysis (`r figs("kripp_sum_cap", display = "cite")`).  However, the number of articles that have been evaluated by two readers is probably too small for reliable estimation of Krippendorff's alpha given the proportions of the TRUE and FALSE classes for most variables (`r tabs("kripp_samp_size_cap", display = "cite")`).

The poor agreement between the two readers for any given category could be due to: 1) poor catgegory definitions or poor instructions about how to evaluate the category; 2) true ambiguity in how a study should be coded; 3) insufficient knowledge or skill of the reader(s); 4) accidental error, perhaps due in part to skimming articles rather than reading them in their entirety.  

Possible solutions are: 1) refine definitions and instructions, when possible specifying specific words or phrases that can be searched for in the article; 2) removing from analysis variables for which many articles are difficult to definitively categorize; 3) requiring a higher level of skill or knowledge of the readers, or treating the less-skilled readers' categorizations as a trigger for a "tie breaker" 3rd review to reveal obvious errors by the first reader, but not as reliable codings in their own right; 4) require readers to read the entire article rather than skimming it, or using disagreements as a trigger for a 3rd "tie breaker" review.

During the third "tie breaker" evaluation of disagreement cases, I judged that the disagreement was due to an obvious error and therefore changed the original (erroneous) value for `r length(which(mult_coded_rev != mult_coded_rev_orig[which(mult_coded_rev_orig$title %in% mult_coded_rev$title), ]))` of Willson's original codings and `r length(which(cp != cp_orig[which(cp_orig$title %in% cp$title), ]))` of CP's original codings.  Disagreements were not a result of obvious errors and values were therefore not changed in `r length(which(mult_coded_rev != cp))` cases.


```{r krippendorfs_alpha_summary, fig.cap = kripp_sum_cap}
kripp_summary_precorrect_long <- data.frame(
  set = c(rep("pre calibration", length(kripp_pre_cal)), 
          rep("post calibration", length(kripp_post_cal)), 
          rep("all_pre_correct", length(kripp_all_pre_correct)), 
          rep("all_post_correct", length(kripp_all_post_correct))), 
  variable = c(names(kripp_pre_cal), 
               names(kripp_post_cal), 
               names(kripp_all_pre_correct), 
               names(kripp_all_post_correct)), 
  krip_alpha = c(sapply(kripp_pre_cal, 
                        FUN = function(x) {x$value}), 
                 sapply(kripp_post_cal, 
                        FUN = function(x) {x$value}), 
                 sapply(kripp_all_pre_correct, 
                        FUN = function(x) {x$value}), 
                 sapply(kripp_all_post_correct, 
                        FUN = function(x) {x$value})))

kripp_summary_precorrect_long$set <- factor(kripp_summary_precorrect_long$set, 
                                 levels = c("pre calibration", 
                                            "post calibration", 
                                            "all_pre_correct", 
                                            "all_post_correct"))

if(diag_present) {
  print(
    ggplot(data = kripp_summary_precorrect_long, 
           mapping = aes(y = krip_alpha, x = set)) + 
      geom_boxplot() + 
      geom_point() + 
      geom_line(aes(group = variable)) + 
      geom_hline(aes(yintercept = 0.667), lty = "dotted") + 
      geom_hline(aes(yintercept = 0.8), lty = "dotted") + 
      ggtitle("Reader agreement using Krippendorff's alpha") + 
      xlab("") +
      ylab("Krippendorff's alpha") + 
      theme_bw()
  )
}
```

```{r plot_kripp_by_nTRUE}
# this is exploratory to see if krippendorff's alpha is varying wildly b/c
# of insufficient variation
alpha_df_all <- data.frame(
  set = rep("all", length(names(kripp_pre_cal))), 
  variable = names(kripp_all_pre_correct),  
  alpha = sapply(kripp_all_pre_correct, FUN = function(x) {x$value}),
  n_wg_T = sapply(names(kripp_all_pre_correct), 
                  FUN = function(x, df = mult_coded_rev_orig) {
                    df <- data.frame(df)
                    col_vec <- df[, which(colnames(df) == x)]
                    sum(col_vec)}), 
  stringsAsFactors = FALSE,
  row.names = 1:length(names(kripp_all_pre_correct))
)

alpha_df_pre_cal <- data.frame(
  set = rep("pre calibration", length(names(kripp_pre_cal))), 
  variable = names(kripp_pre_cal), 
  alpha = sapply(kripp_pre_cal, FUN = function(x) {x$value}), 
  n_wg_T = sapply(names(kripp_pre_cal), 
                  FUN = function(x, df = mult_coded_rev_orig) {
                    df <- data.frame(df)
                    col_vec <- df[, which(colnames(df) == x)]
                    sum(col_vec)}),
  stringsAsFactors = FALSE, 
  row.names = 1:length(names(kripp_pre_cal))
)

alpha_df_post_cal <- data.frame(
  set = rep("post calibration", length(names(kripp_post_cal))), 
  variable = names(kripp_post_cal), 
  alpha = sapply(kripp_post_cal, FUN = function(x) {x$value}), 
  n_wg_T = sapply(names(kripp_post_cal), 
                  FUN = function(x, df = mult_coded_rev_orig) {
                    df <- data.frame(df)
                    col_vec <- df[, which(colnames(df) == x)]
                    sum(col_vec)}), 
  stringsAsFactors = FALSE,
  row.names = 1:length(names(kripp_post_cal))
)

alpha_df <- bind_rows(list(alpha_df_all, alpha_df_pre_cal, alpha_df_post_cal))

if(diag) {
  print(
    ggplot(alpha_df, aes(x = n_wg_T, y = alpha, color = set)) + 
      geom_point() + 
      geom_hline(aes(yintercept = 0.667), lty = "dotted") + 
      geom_hline(aes(yintercept = 0.8), lty = "dotted") + 
      ggtitle("Krippendorff's alpha by the number\nof TRUEs in Willson's coding\nOriginal Codings Before Correcting Errors") + 
      xlab("Number of TRUE values in Willson's coding") + 
      theme_bw()
  )
}
```

```{r kripp_agreement_wg_before_and_after_corrections, include = F, eval = F}
# show agreement measure of how well my corrections based on looking at double
# codings agree with my original codings.
if(diag) {
  hist(sapply(kripp_wg_comparison, FUN = function(x) {x$value}))
  summary(sapply(kripp_wg_comparison, FUN = function(x) {x$value}))
  sapply(kripp_wg_comparison, FUN = function(x) {x$value})
}
```

We could use my codings (either original or corrected) for analysis, and view the second reader's role as highlighting cases of sloppy errors or careless oversight on my part.  This doesn't address the reproducability question of whether my categories are clear enough that an independent researcher trying to do the same study would come up with the same conclusions.  In order to answer that, we would need a second reader with adequate ability to code each category.  Lacking such a person, we'll have to use the second reader as an error catcher rather than an independent second opinion.

Page 240 of the Krippendorf (2004) book has recommendations about sample size needed based on how many cases there are of the least common class.  `r tabs("kripp_samp_size_cap", display = "cite")` shows which variables have enough of the minority class that Krippendorff's alpha can be calculated with significance level of 0.1 and minimum acceptable Krippendorff's alpha of 0.667.

```{r kripp_samp_size_cap}
kripp_samp_size_cap <- tabs(name = "kripp_samp_size_cap", 
                            paste0("Recommended sample size for accurate estimation of Krippendorff's alpha given the proportion of the minority class in the results, and a desired confidence level of 0.1 for a minimum Krippendorff's alpha value of 0.667 (Krippendorf 2004).  So far there are ", nrow(mult_coded_rev), " double-coded studies."))
```

`r if(diag_present) tabs("kripp_samp_size_cap", display = "full")`

```{r calculate_proportion_T}
## This calculates the proportion of TRUE values using wg's post-correction
# codings for each variable.  Even though we are interested in kripp's alpha
# pre-correction, I use the post-correction TRUEs as the best estimate of the
# true proportion of TRUE values for each variable, and that best estimate of
# the proportion is what I need to figure out required sample size for Kripp's a.
prob_of_values <- lapply(
  mult_coded_rev[, 2:ncol(mult_coded_rev)], 
  FUN = function(x) {sum(x, na.rm = T) / length(which(!is.na(x)))})

prob_of_values <- data.frame(variable = names(prob_of_values), 
                             prob_T = unlist(prob_of_values), 
                             row.names = NULL)
prob_of_values$prob_small_class <- NA
prob_of_values$kripp_pre_correct <- NA
for(i in 1:nrow(prob_of_values)) {
  if(prob_of_values$prob_T[i] <= 0.5) { # calc prob of minority class
    prob_of_values$prob_small_class[i] <- prob_of_values$prob_T[i]
  } else prob_of_values$prob_small_class[i] <- 1 - prob_of_values$prob_T[i]
  
  # add krippendorff's alpha for codings before corrections
  prob_of_values$kripp_pre_correct[i] <- kripp_all_pre_correct[[which(
    names(kripp_all_pre_correct) == prob_of_values$variable[i])]]$value
}

# On p 240 of Krippendorff book, it says that if acceptable alpha is 0.667 and 
# desired significance level is 0.1, then sample size needed is as follows:
prob_of_values$n_25_adequate <- prob_of_values$prob_small_class > 0.25
prob_of_values$n_30_adequate <- prob_of_values$prob_small_class > 0.2
prob_of_values$n_40_adequate <- prob_of_values$prob_small_class >= 0.143

if(diag_present) kable(prob_of_values[, -2], digits = 2)
```


#### Notes about agreement 
(*italics* means I've corrected it, **bold** means I need to deal with it.)

**As of 2 Sep I haven't updated this list to include the last few articles that I coded to get up to speed with CP.**

- biological records in facilitative role: generally, it looks like CP codes this as TRUE much more often than I do.
    - *"An assessment of bumblebee (Bombus spp) land use..." Casey coded this as T, but the species observation data were clearly used as a response variable.  Perhaps the confusion is because survey participants were identifed from a list of people who had participated in a previous survey.  Regardless, this should be FALSE.*
    - *"Modelling and mapping UK emissions..." Casey coded TRUE, but to me this is a pretty clear case of biological records as a predictor (used to modify the response variable of emissions).  I think this is an error from lack of understanding of what it would look like to have bio recs as a predictor variable.  I don't think there's actually much ambiguity about how this article would be coded, I think perhaps there is ambiguity in my instructions about this category.  So, I changed this to FALSE.*
    - *"The role of historical environmental information in perceptions..." Casey coded TRUE but the occurrence records were used as a response variable to create the presence trajectory type categories that were used for trait analysis.  See p 27 of the study.  This is an error and I will change it to FALSE.*
    - *Edit instructions to make clear what bio recs as a predictor would look like from a general perspective, not just explicitly in a regression equation or something.*
    - *"Light pollution: spatial analysis..." CP put T, I put F.  They did actually look at the bio recs data, and results include a map of records.  They don't do much in the way of analysis, except a kinda descriptive analyses in which they note that conservation concern species have been recorded in an areay with increased night lighting. So I think this is an actual descriptive analysis of bio recs data, not just using it facilitatively.  I will change CP's value to FALSE.*
- cross validation
    - *The only disagreement was on "Nature protection areas of Europe are insufficient..." which I have already disqualified b/c it doesn't use UK/IE records.*
- data type - physical specimen: 
    * *Casey missed "Congruence in fungal phenology patterns...", perhaps because methods say "fungarium" data, and he may not have known the word or not realized that those are probably physical specimens in a collection.*
    - **I think Casey missed "Evaluating promotional approaches for citizen science biological recording..." which states that all data used were records for which there was either a photo or specimen.  But I don't know if this is clear enough to be an error that I correct, or whether this article is a bit ambiguous.  I will not change it for now.** 
    - *Perhaps I need to change name from "museum" to "physical specimen" for this category.*  
    * *For "Explaining European fungal fruiting phenology..." it looks like Casey correctly had TRUE for museum and I just missed the 1st sentence of methods - a bad oversight on my part.*
    * *For "Nature protection areas are insufficient..." Casey had T for museum, I had F, but on re-reading I think this doesn't even qualify at all - even though UK was in their original study extent, there were no records of the beetle in UK or IE, so it doesn't actually meet my criteria of using UK/IE biological records.*
    * *"Big data integration: Pan-European fungal species..." CP put T, I put F.  The authors of this study got data from museums, among others, and used that data to produce the meta-database, which was then used for the analyses.  By my definition for the physical specimen data type category that I revised 11 Sep, any study that uses the meta-database would not be using physical specimen data, as the data would be acquired as whwhwh data from the proximate data provider (the metadatabase).  But in the case of this study, b/c the metadatabase didnt' extist, and the authors collected the data directly from other sources and then analyzed it to demonstrate their new data repository, the data in the analysis did come from museums as the proximate data source, and so this should be TRUE for physical specimen.  Not entirely easy to see, but a fairly clear decision when my new strict definition for this category is applied.  Therefore my original coding is in error, and I will change my value to TRUE.*
- data type - organized monitoring scheme:  
    * *For "Evaluating promotional approaches for citizen science biological recording..." Casey coded T for monitoring scheme.  However, despite records coming from named "schemes" (UK Ladybird Recording Scheme & BeeWatch), those named schemes are actually just data collection portals/websites for opportunistic records.  There is no organized monitoring effort, no regular survey protocol, locations, or times of surveys.  So the correct coding is FALSE for that article.*
    - *Need to edit instructions to make it clear that this is only if the actual act of recording is standardized in methods, locations, or effort, not just having a central repository.*
    * *For "Nature protection areas of Europe are insufficient..." Casey put TRUE for organized scheme, but I find no record of that in the article.  Seems data were agregated from many databases, but no scheme.  Don't know why he put that.  Either misunderstanding or oversight.  Regardless, the article is now disqualified (see above).*
    * *"Big data ingegration: Pan-European fungal..." CP put T, I put F. I don't easily find mention of organized schemes.  They might be there, though.  So I will not change this for now.*
    * **"Congruency in fungal phenology..."**
    * **"N81 Tullow Footbridges Scheme..." Casey put T, I put F. **
- sampling effort reported
    - *"British phenological records indicate high diversity..." Casey coded T, I coded FALSE, but I was wrong, as they used data from UK Butterfly Monitoring Scheme (among other data sources).  This was an error by me.*
    - *"Congruency in fungal phenology..." Casey coded TRUE, I coded FALSE.  This is not obviously an error by either of us, but is due to ambiguity in the study.  The local dataset used for Switzerland is from surveys in quadrats, and seems to be the result of a specific study.  I considered this not bio recs data but rather other data that was used in addition to bio recs data.  However, one could view this as bio recs data in the sense that bio recs are sometimes derived by searching published literature.  So it is a legitimate interpretation to call that bio recs data, in which case sampling effort = TRUE for this study.  I will not change anything as this is not an obvious error.*
    - *"Evaluating promotional approaches for citizen science..." Casey coded TRUE.  But despite records coming from named "schemes" (UK Ladybird Recording Scheme & BeeWatch), those named schemes are actually just data collection portals/websites for opportunistic records.  There is no organized monitoring effort, no regular survey protocol, locations, or times of surveys, and, as far as I can tell from looking at those scheme websites, no record of survey effort.  This is an error by CP and I will change it to FALSE.*
    - *"Large reorganizations in butterfly communities..." CP coded FALSE but the study uses UKBMS data which is collected on transects of known length.  This is an error by CP and I will change it to TRUE.*
    - *"Setting temporal baselines for biodiversity..." CP coded F, I coded T.  I looked at DaEuMon website and browsed individual monitoring schemes.  The Jersey Butterfly Monitoring scheme uses transects and was entered into EuMon pre-2009, so presumably at least that scheme was included in this study and therefore at least some sampling effort is reported.  This is therefore an error by CP (though one that took a lot of digging to get correct) and I will change CP's coding to TRUE.*
    - *Edit instructions to make clear that what is being asked is whether the sampling effort is reported with the data, not whether sampling effort is reported in the study or article.  So a study that uses UK Butterfly Monitoring Scheme data is TRUE for sampling effort reported, because it is reported with the data, even if the study doesn't report that sampling effort (e.g. transect length).*
    - *With this and all data type variables, I need to clarify and make explicit whether I am asking whether the data type is available, or whether it is used.  I am asking whether it is available.  It could not be used either b/c the authors 1) chose not to use it or 2) it wasn't relevant to the question.  However, trying to determine whether it is relevant to the questions is a slippery slope, b/c questions may be changed to match the available data, and it's also not always clear whether the data is actually needed for the question.  So, I intend to make this ask about whether the data has those data types available at all, regardless of whether they were used.*
- diversity as focus: 
    - CP seems to be coding TRUE way more often than I am.  I think the main issue here is that I didn't define clearly enough that by "diversity" I mean a narrow definition of either alpha/beta diversity (turnover) or a combination of species richness and evenness.  I am not considering species richness alone or narrative descriptions of communities as "diversity" as a study focus.
    - *"An assessment of bumblebee (Bombus spp) land use...".  This looks like clearly not diversity as an outcome to me.  This is an error by CP and I will change TRUE to FALSE.*
    - *"An invertebrate survey of Scragh Bog...".  I don't see any explicity study of diversity (in the strict sense) in this.  This is an error by CP and I will change TRUE to FALSE.*
    - *"British phenological records indicate high diversity...".  Despite the title, this study uses species richness as the outcome measure, and is therefore not a diversity study by my strict definition.  Therefore I consider CP's TRUE to be an error, and I will change it to FALSE.*
    - *"Ocean current connectivity propelling...".  Don't know why CP coded this as T.  That is an error and I will change this to FALSE.* 
    - *"The interplay of climate and land use change affects...".  Doesn't measure diversity in the strict sense.  So CP's TRUE is an error and I will change it to FALSE.*
    - *Edit instructions to make clear that this is about beta diversity or about some combination of species richness and evenness.*
    - **"Predicting the likely impact of urbanisation..."**
- results type - descriptive only
    - *"Ocean current connectivity...". CP coded F, I coded T. I don't see any non-descriptive analyses that use the bio recs data, but it's a little unclear which data were used for which analyses, so this article is probably ambiguous, which means the codings are not in error.  I will not change them.*
- inference: looks pretty good.
    * *Casey missed "Explaining European fungal fruiting..." that he marked as FALSE but that had bootstrap CIs on one plot.*
    * *On "Ocean current connectivity..." I put no inference b/c all the analyses using bio recs didn't use inference, while Casey put inference b/c there were other analyses with non-bio recs data that used inference.*
- testing using independent dataset
    - *"Population variability in species can be deduced...". CP coded F, I coded T.  The study compares results using UKBMS and a separate dataset of opportunistic records.  Therefore CP's coding is an error and I will change it to TRUE.*
    - *"Potential for coupling the monitoring of bush-crickets...". CP coded F, I coded T.  The introduction says "We validated these methods using data from intensive survey sites", so I think CP's code is an error and I will change it to TRUE.*
    - **"Light pollution: spatial analysis..."**
    - *"Using geographic profiling..." I coded T, CP coded F. However, this study clearly uses 3 independent datasets and runs the analysis separately using 2 of the datasets as training data separately.  For both those analyses, the data are tested on a 3rd dataset (known dens) which were not included as training data.  So CP's coding is an error and I am changing it to T.*


## Temporal extent of studies

```{r print_time_anova}
m_time_full_anova
```

```{r print_time_individual_predictor_significance}
if(diag) {
  for(i in 1:length(indiv_mods_time)) { 
    print(anova(indiv_mods_time[[i]], time_lm_full))
  }
}
```

The overall model using supplementary data types to predict the natural log of temporal extent of study (in years) was (not) significant ($F_{`r format(round(m_time_full_anova$Df[2], digits = 2), scientific = F)`, `r format(round(m_time_full_anova$Res.Df[2], digits = 2), scientific = F)`}$ = `r format(round(m_time_full_anova$F[2], digits = 2), scientific = F)`, p = `r format(round(m_time_full_anova[6][[1]][2], digits = 2), scientific = F)`, Adjusted $R^2%$ = `r format(round(time_summary$adj.r.squared, digits = 2), scientific = F)`).  

If the overall model is significant, I will individually test whether each variable has a significant effect after accounting for the other variables.  I will interpret the effect direction and effect size of all variables in the context of the full model even if the variables are not significant on their own.  Example of how results will be written:
 
The expected temporal extent covered by a study that uses only "what, where, when" data is `r round(exp((time_lm_full$coefficients[1] + time_lm_full$coefficients[2])), digits = 1)` years when the data have no other data types.  When the data are "what, where, when" only but multiple datasets were integrated for analysis, the expected temporal extent covered by a study is `r round(exp((time_lm_full$coefficients[1] + time_lm_full$coefficients[2] + time_lm_full$coefficients[8])), digits = 1)` years. 

Studies using data that includes sampling effort information have a shorter temporal extent after accounting for other data types that the data has (*p* = `r format(round(time_summary$coefficients[2, 4], digits = 3), scientific = F)`).  The effects of additional information on detection / non-detection and abundance were not significant after accounting for the effects of the other data types.  The signs of the effects of sampling effort information and detection / non-detection information were both negative, as expected.  However, the sign of the effect for abundance information was unexpectedly positive.  This may be because abundance is correlated with detection/non-detection and sampling effort, and so the effect of abundance data is positive after correcting for those other data types.  When I fit a model with only the intercept and the abundance data type, the direction of the effect of abundance data was negative as expected.    

```{r time_extent_results_plot}
## TODO: need to hand-annotate R2 and p-value in plot
temp_extent$sig_different_from_WhWhWh <- temp_extent$data_type %in% 
                                       c("sampling effort reported")
print(ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + #, color = sig_different_from_WhWhWh
        geom_boxplot(varwidth = T) +
        scale_y_log10() + 
        xlab("Data Type") + 
        ylab("Temporal Extent of Study (years)") + 
        annotate("text", x = 3, y = 2600, 
                 label = "paste(\"Adjusted \", 
                 italic(R) ^ 2, \" = ???\")", 
                parse = TRUE) + 
  annotate("text", x = 3, y = 1200, 
           label = "paste(\"Overall model significance \", 
                 italic(p), \" = ??\")", 
           parse = TRUE) +
        theme_bw() + 
        scale_color_discrete(name = "Significantly Different from\nWhat, Where, When-only\nstudies (alpha = 0.1 level)") + 
        theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) 
)
```

```{r time_extent_results_plot_for_presentation_slide, eval = F, include = F}
## This code is to be run in the console for making plot to be exported as jpg for slides
## TODO: need to hand-annotate R2 and p-value in plot
t_size = 22 # plot text size
ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + #, color = sig_different_from_WhWhWh
  geom_boxplot(varwidth = T) +
  scale_y_log10() + 
  xlab("Data Type") + 
  ylab("Temporal Extent of Study (years)") + 
  # annotate("text", x = 3, y = 140, 
  #          label = "paste(\"Adjusted \", 
  #                italic(R) ^ 2, \" = 0.27\")", 
  #          parse = TRUE, 
  #          size = t_size - 15) + 
  annotate("text", x = 3, y = 1400, 
           label = "paste(\"Overall model significance \", 
                 italic(p), \" = 0.012\")", 
           parse = TRUE, 
           size = t_size - 14) +
  theme_bw() + 
  scale_color_discrete(name = "Significantly Different from\nWhat, Where, When-only\nstudies (alpha = 0.1 level)") + 
  theme(axis.text.x = element_text(angle = 330, vjust = 1, hjust = 0), 
        text = element_text(size = t_size)) 
```

-----------------------------------------------

## Study Question Paradigm

### What is the most popular study question paradigm?

$H_a$: Some broad study question paradigms are more common than others in studies using biological records.

$H_0$: The number of studies pursuing each of the broad study question paradigms is the same.

**Jon - does a test here even make sense when I only have one observation of counts for each predictor?  Because no replicates, have no estimate of variance, so how can tell if means are different?  Assuming variance equals mean?  But do we have any evidence for that?  Can we assume that? Use bootstrapping to estimate variance to check assumptions?**  I'm not sure that the inference here is telling us much, either with the significance or parameter estimates.  Especially because that inference is based on an assumption about variance that we have no evidence for.  I think we actually have just as much confidence in the results by looking at the table as by looking at the inference (because difference is big, and assumptions of inferential model are questionable). 

```{r print_paradigm_model_results}
if(diag_present) {
  qp_full_anova
  summary(qp_mod)
}
```

```{r print_study_question_paradigm_table}
kable(paradigm_counts_df, col.names = c("paradigm", "number of studies"))
```

## Specific Study Focus

```{r specific_question_by_paradigm_bar_plot}
qt_df <- select(rev, "title", 
                "community.analysis", "individual.species.analysis", 
                "species.richness", "diversity", 
                "distribution", "abundance", "phenology",
                "trends.over.time", "methodology.development.or.analysis") 
names(qt_df)[4:10] <- c("species.richness", "diversity", 
                        "distribution", "abundance", "phenology", 
                        "temporal.trends", "methodology")
# make an "other" column indicating studies that are not community or individual
# qt_df$paradigm.other <- rev$community.analysis == F & 
#   rev$ individual.species.analysis == F

qt_df <- gather(qt_df, key = "paradigm", value = "parad_val", 
                community.analysis, individual.species.analysis,  
                factor_key = T) %>%
  filter(parad_val == TRUE) %>%
  gather(key = "focus", value = "foc_val", 
         species.richness, diversity, distribution, abundance, phenology, 
         temporal.trends, methodology, 
         factor_key = T) %>%
  filter(foc_val == TRUE) %>%
  group_by(paradigm, focus) %>%
  summarise(count = n()) %>% 
  # filter odd combinations of paradigm and focus
  filter(!(paradigm == "community.analysis" & 
             focus %in% c("distribution", "abundance", "phenology"))) %>%
  filter(!(paradigm == "individual.species.analysis" & 
             focus %in% c("species.richness", "diversity"))) 

ggplot(data = qt_df, 
       aes(x = factor(paradigm, 
                      levels = c("community.analysis", 
                                 "individual.species.analysis"), 
                      labels = c("community", "individual species")), 
           y = count, 
           fill = factor(focus, 
                      levels = c("species.richness", "diversity", 
                                 "distribution", "phenology", "abundance", 
                                 "temporal.trends", "methodology"), 
                      labels = c("species richness", "diversity", 
                                 "distribution", "phenology", "abundance", 
                                 "temporal trends", "methodology")))) + 
  geom_bar(position = "dodge", stat = "identity") + 
  scale_fill_brewer(type = "qual", palette = 2) + 
  #scale_fill_grey(end = 0.7) + 
  xlab("Study Focus") + 
  ylab("Number of Studies") + 
  theme_bw() + 
  theme(legend.title = element_blank())

# qt_df <- gather(qt_df, key = "study_question", value = "v_2", 
#                 species.richness, diversity, distribution, abundance, 
#                 phenology,
#                 factor_key = T) %>%
#   filter(v_2 == TRUE) %>%
#   group_by(study_question) %>%
#   summarise(count = n()) %>%
#   mutate(percent = (count/sum(count)) * 100) %>%
#   mutate(paradigm = str_replace_all(study_question, 
#                                     c("species.richness|diversity" = 
#                                         "community", 
#                                       "distribution|abundance|phenology" = 
#                                         "individual species"))) 
# 
# ggplot(data = qt_df, 
#        aes(x = factor(study_question, 
#                       levels = c("species.richness", "diversity", 
#                                  "distribution", "phenology", "abundance"), 
#                       labels = c("species richness", "diversity", 
#                                  "distribution", "phenology", "abundance")), 
#            y = percent, 
#            fill = paradigm)) + 
#   geom_bar(stat = "identity") + 
#  # scale_fill_brewer() + 
#   scale_fill_grey(end = 0.7) + 
#   xlab("Study Focus") + 
#   ylab("Percent of Studies") + 
#   theme_bw()
```

```{r specific_question_multi_paradigm_bar_plot, eval = F, include = F}
## as of 4 Sep 2018, don't need this anymore - this is incorporated in the 
## above plot
qt_df <- select(rev, "title", "methodology.development.or.analysis", 
                "testing.macro.or.ecological.theory", "trends.over.time") 
names(qt_df)[2:4] <- c("methodology", "testing.theory", "temporal.trends")

qt_df <- gather(qt_df, key = "study_question", value = "v_2", 
                temporal.trends,testing.theory, methodology,  
         factor_key = T) %>%
  filter(v_2 == TRUE) %>%
  group_by(study_question) %>%
  summarise(count = n()) %>%
  mutate(percent = (count/sum(count)) * 100)

ggplot(data = qt_df, 
       aes(x = factor(study_question, 
                      levels = c("temporal.trends", "methodology",
                                 "testing.theory"), 
                      labels = c("temporal trends", "methodology",
                                 "testing theory")), 
           y = percent)) + 
  geom_bar(stat = "identity") + 
  scale_fill_brewer() + 
  xlab("Study Focus") + 
  ylab("Percent of Studies") + 
  theme_bw()
```

[ Descriptive stats for studies focusing on policy or alien species, testing ecological theory, and testing methods of data correction. ]

### What data types are used for each study question paradigm?

```{r make_study_question_paradigm_by_dataType_tibble, eval = F, include = F}
## I don't think a table is a good way to present this.  Too hard to see.
## But if I include this again later, should add all data type variables.
paradigm_data_df <- select(rev, title, individual.species.analysis, 
                      community.analysis, data.type...what.where.when.only, 
                      data.type...sampling.effort.reported, 
                      data.type...detection...non.detection, 
                      data.type...abundance) %>%
  gather(key = "paradigm", value = "parad_used", individual.species.analysis:
           community.analysis) %>%
  filter(parad_used == TRUE) %>%
  gather(key = "data_type", value = "dat_used", 
         data.type...what.where.when.only:data.type...abundance) %>%
  filter(dat_used == TRUE)

paradigm_data_df$data_type <- gsub("data.type...what.where.when.only", 
                                   "what, where, when only", 
                                   paradigm_data_df$data_type)
paradigm_data_df$data_type <- gsub("data.type...sampling.effort.reported", 
                                   "sampling effort", 
                                   paradigm_data_df$data_type)
paradigm_data_df$data_type <- gsub("data.type...detection...non.detection", 
                                   "detection / non-detection", 
                                   paradigm_data_df$data_type)
paradigm_data_df$data_type <- gsub("data.type...abundance", 
                                   "abundance", 
                                   paradigm_data_df$data_type)
paradigm_data_df$paradigm <- gsub("community.analysis", 
                                   "community", 
                                   paradigm_data_df$paradigm)
paradigm_data_df$paradigm <- gsub("individual.species.analysis", 
                                   "individual species", 
                                   paradigm_data_df$paradigm)

kable(table(paradigm_data_df$data_type, paradigm_data_df$paradigm))
```

```{r data_type_by_paradigm_pairs_plot, fig.height=8, fig.width = 8, eval = F}
### plot data type by study question paradigm ----------------------
### I think this is not how I want to visualize this.  Trying a new approach in following chunk
dt_pd_df <- rev[, which(
  colnames(rev) %in% 
    c("data.type...what.where.when.only", 
      "data.type...sampling.effort.reported", 
      "data.type...abundance", 
      "data.type...detection...non.detection", 
      "data.type...organized.data.collection.scheme", 
      "data.type...visit.specific.covariates", 
      "data.type...multiple.datasets.integrated.for.analysis", 
      "data.type...life.stage", 
      "data.type...voucher.available", 
      "community.analysis", 
      "individual.species.analysis"))]
for(i in 1:ncol(dt_pd_df)) {
  dt_pd_df[, i] <- as.factor(as.character(data.frame(dt_pd_df)[, i]))
}

# set attributes of columns
attr(dt_pd_df, "data types") <- c(
  "data.type...what.where.when.only", 
  "data.type...sampling.effort.reported", 
  "data.type...abundance", 
  "data.type...detection...non.detection", 
  "data.type...organized.data.collection.scheme", 
  "data.type...visit.specific.covariates", 
  "data.type...multiple.datasets.integrated.for.analysis", 
  "data.type...life.stage", 
  "data.type...voucher.available")
attr(dt_pd_df, "study paradigm") <- c("community.analysis", 
                                      "individual.species.analysis")

data_type_vars <- attr(dt_pd_df, "data type")
paradigm_vars <- attr(dt_pd_df, "study paradigm")

print(ggduo(dt_pd_df, data_type_vars, paradigm_vars, 
            title = "testing", 
            columnLabelsX = c("what where when only", 
                               "sampling effort",
                               "abundance",
                               "non detection", 
                               "organized scheme", 
                               "visit specific covariates", 
                               "multiple datasets", 
                               "life stage", 
                               "voucher available"), 
            columnLabelsY = c("community", "individual species"), 
            labeller = label_wrap_gen(width = 10)) + 
        theme(axis.text.x = element_text(angle = 50, hjust = 1), 
              axis.text.y = element_text(angle = 50, hjust = 0)))
```

```{r data_type_by_paradigm_barplot}
### plot data type by study question paradigm ----------------------
dt_pd_df <- select(rev, data.type...what.where.when.only, 
      data.type...sampling.effort.reported, 
      data.type...abundance, 
      data.type...detection...non.detection, 
      data.type...organized.data.collection.scheme, 
      data.type...multiple.datasets.integrated.for.analysis, 
      data.type...life.stage, 
      data.type...voucher.available,
      community.analysis, 
      individual.species.analysis) %>%
  gather(key = "data_type", value = "dt_val", data.type...what.where.when.only:
           data.type...voucher.available) %>%
  filter(dt_val == TRUE) %>%
  gather(key = "paradigm", value = "pd_val", community.analysis, 
         individual.species.analysis) %>%
  filter(pd_val == TRUE) %>%
  count(paradigm, data_type) %>%
  complete(paradigm, data_type, fill = list(n = 0))

ggplot(dt_pd_df, 
       aes(x = factor(paradigm, 
                      levels = c("community.analysis", 
                                 "individual.species.analysis"), 
                      labels = c("community", "individual species")), 
           y = n, 
           fill = factor(data_type, 
                         levels = c("data.type...what.where.when.only", 
                           "data.type...sampling.effort.reported", 
                           "data.type...abundance", 
                           "data.type...detection...non.detection", 
                           "data.type...organized.data.collection.scheme", 
                           "data.type...multiple.datasets.integrated.for.analysis",
                           "data.type...life.stage", 
                           "data.type...voucher.available"), 
                         labels = c("what where when only", 
                                    "sampling effort", 
                                    "abundance", 
                                    "non detection", 
                                    "organized scheme", 
                                    "multiple datasets", 
                                    "life stage", 
                                    "voucher available")))) + 
  geom_bar(position = position_dodge(preserve = "single"), stat = "identity") + 
  ggtitle("Data use based on study question paradigm") + 
  scale_fill_discrete(name = "Data Type") + 
  xlab("Study question paradigm") + 
  ylab("Number of studies") + 
  scale_y_continuous(breaks = seq(0, max(dt_pd_df$n), 5)) + 
  theme_bw() 
```

**Jon - Too many data types to interpret well?  Drop life stage?  Is there any useful statistical work to be done here?  I don't think so - would require lots of data, not sure it adds much to understanding.**  I think the story is that data use is roughly similar between question paradigms, except maybe that voucher data seems to be used more often in individual species studies.  Also integrating multiple datasets for analyses is common in both question paradigms, and so it is worth making an effort to ensure that published datasets are in standardized, compatible formats.  

## Analysis approach & data type
_____________________________________________________________________
# Discussion

## Reader Agreement
The seemingly poor agreement between WG's and CP's codings either reflects true ambiguity in how to code the variables or errors by one of the readers.  My review of all cases of disagreement revealed many cases that I believe to be obvious errors or oversights.  Of the obviously erroneous values, most were CP's, though some were mine.  This suggest that either I have given poor instructions to CP or CP is carrying out the instructions poorly.  It is probably not possible to figure out which of these things is happening without having a third reader, and preferable one with a higher skill level.  In some cases, my instructions were clearly insufficient.  I have now edited the instructions and hopefully solved those problems for the 12 variables CP double coded (though similar inadequate instructions probably still exist for the other variables).  The best test now would be to test these same 12 variables with another reader with a higher level of ability.  If that reader also has substantial disagreement, then either the categories themselves or the instructions will need to be majorly revised.  If that $3^{rd}$ higher-ability reader agrees with my original codings better, then it indicates that a highly skilled reader is required for the second readings.  

The technique of having me do a "tie breaker" evaluation of all disagreement cases is a partial solution.  This method would basically treat the $2^{nd}$ reader's codings as a check for error's in my coding that trigger a tie breaker review.  Cases in which the $2^{nd}$ readers coding is judged to be obviously erroneous would not be of much concern, and would be attributed to insufficient knowledge (e.g. not knowing that "fungarium" is a collection of physical specimens) or to skimming the articles quickly rather than reading them (CP seemed to spend less than 30 minutes per article).  Cases in which my original coding is judged to be obviously erroneous would be of concern.  Those values could be corrected for analysis, and/or the overall reliability of my original codings would need to be evaluated in some way to determine if a $2^{nd}$ check is needed for all studies and variables or whether my errors are few enough that they introduce an acceptable amount of error/uncertainty into the analysis. The ideal soluttion would be to have a highly-skilled second reader and to calculate Krippendorff's alpha for agreement between the two readers.  However, lacking such a person, I think it would be acceptable to use a less skilled second reader as a trigger for a 3rd tie breaker review done by me.  I do need to decide whether the trigger/tie breaker approach requires checking all studies and variables, or whether there is an adequate way to estimate the reliability of my original codings. I am not sure whether Ellie would be knowledgable enough to be a highly-skilled second reader.  Perhaps not. 

A final solution that is perhaps worth pursuing is only coding and analyzing variables for which coding can be done based entirely or largely on character string matching.  This would provide greater reliability, but at the cost of less interesting, informative, and insightful analysis (Krippendorff 2004, Section 11.1).

There are two goals of having two readers.  First, to estimate how replicatable the analysis is.  If another researcher could code the articles in the same way I do, then it provides confidence that the analysis is revealing some sort of true characteristics of the literature.  Second, to improve the quality of this specific analysis by identifying and correcting sloppy errors on my part.  This is not necessarily replicatable, though, as I am doing the correcting, so the categories basically still show "Willson's inner vision of the literature" rather than some more widely true characteristics. 

## Variables to abandon


_________________________________________________________________________

# Old versions below here

## Types of questions  
Analyses may appear in this table or graph more than once.  For example, a study proposing a new method for estimating species richness and then using that method to test predictions about a productivity/richness relationship would appear in the counts in three columns.

```{r questions_by_data_type}
qt_df <- select(rev, "title", "year", "methodology.development.or.analysis", 
                "distribution", "abundance", "phenology", 
                "testing.macro.or.ecological.theory", "trends.over.time", 
                "data.type...what.where.when.only", 
                "data.type...detection...non.detection", 
                "data.type...abundance", "data.type...sampling.effort.reported", 
                "data.type...organized.data.collection.scheme") 
names(qt_df)[3:8] <- c("methodology", "distribution", "abundance",
                        "phenology", "testing.theory", "temporal.trends")

qt_df <- gather(qt_df, key = "study_question", value = "v_1", 
                methodology:temporal.trends, factor_key = T) %>%
  filter(v_1 == TRUE) %>%
  gather(key = "data.type", value = "v_2", 
         data.type...what.where.when.only:
           data.type...organized.data.collection.scheme, 
         factor_key = T) %>%
  filter(v_2 == TRUE)


kable(table(qt_df$data.type, qt_df$study_question), 
      caption = "The types of questions addressed by studies that use biological records, according to what type of data they use.")

### testing stuff
chisq.test(table(qt_df$data.type, qt_df$study_question), simulate.p.value = T)
```

```{r}
qt_df <- select(rev, "title", "year", "methodology.development.or.analysis", 
                "individual.species.analysis", "community.analysis", 
                "compiled.individual.species.analysis", "distribution", 
                "abundance", "phenology", "testing.macro.or.ecological.theory", 
                "trends.over.time") 
names(qt_df)[3:10] <- c("methodology", "individual.species", "community", 
                        "compiled.individual", "distribution", "abundance",
                        "phenology", "testing.theory")

qt_df <- gather(qt_df, key = "population", value = "v_1", individual.species, 
                compiled.individual, community, factor_key = T) %>%
  filter(v_1 == TRUE) %>%
  gather(key = "study_question", value = "v_2", distribution, abundance, 
         phenology, testing.theory, methodology, trends.over.time, 
         factor_key = T) %>%
  filter(v_2 == TRUE)


kable(table(qt_df$study_question, qt_df$population) / length(unique(qt_df$title)), digits = 2, 
      caption = "The types of questions addressed by studies that use biological records.  Values are the proportion of all studies.  Study populations are columns, topic of the study is in rows.")
```




## Taxonomic Group
```{r}
taxa <- select(rev, title, year, taxonomic.group, distribution, abundance, 
               phenology, testing.macro.or.ecological.theory, trends.over.time)

tax_names <- unlist(strsplit(taxa$taxonomic.group, "; "))
tax_names <- unique(tax_names)
tax_names <- tax_names[which(!is.na(tax_names))]
tax_names <- tax_names[order(tax_names)]

temp_df <- data.frame(matrix(nrow = nrow(taxa), ncol = length(tax_names)))
colnames(temp_df) <- tax_names
taxa <- cbind(taxa, temp_df)
rm(temp_df) 

for(i in 1:nrow(taxa)) {
  groups <- taxa$taxonomic.group[i]
  groups <- unlist(strsplit(groups, "; "))
  taxa[i, which(colnames(taxa) %in% groups)] <- T
}

taxa <- gather(taxa, key = "tax_group", value = "value", 
               all:wasps) %>%
  filter(value == T) 

table(taxa$tax_group)[order(table(taxa$tax_group), decreasing = T)]
```

```{r}
if(diag) {
  taxa <- gather(taxa, key = "question", value = "v_2", 
                 distribution:trends.over.time, factor_key = T) %>%
    filter(v_2 == T)
  
  table(taxa$tax_group, taxa$question)
}
```
Above is good.  Need to simplify taxonomic groups.  Might not end up using this table, b/c the patterns it shows probably have more to do with the type of data (abundance, semi-structure) than taxonomic group.  So, butterflies don't lend themselves to abundance and phenology studies, but the monitoring scheme used for butterflies does lend itself to those types of analyses.


--------------------------------------------------------------------

## Results format by data type
$H_01$: Different types of biological records data are not analyzed with different broad analysis approaches.

$H_a1$: Different types of biological records data are analyzed with different broad analysis approaches.

$H_02$: wh,wh,wh-only data is not analyzed with a different broad analysis approach than other data types.

$H_a2$: wh,wh,wh-only data is analyzed with inference and/or prediction less often than richer data are.  

$H_03$: The ?rate? at which wh,wh,wh-only data is analyzed with prediction is not greater than for other data types.

$H_a3$: The ?rate? at which wh,wh,wh-only data is analyzed with prediction is greater than for richer data types.

```{r analysis_approach_data_prep}
rf <- select(rev, c(title, data.type...what.where.when.only:
                      results.type...descriptive.only)) %>%
  gather(key = "data_type", value = "v_1", 
         data.type...what.where.when.only:data.type...detection...non.detection, 
         data.type...abundance:data.type...physical.specimen, 
         factor_key = T) %>%
  filter(v_1 ==T) %>%
  gather(key = "results_format", value = v_2, 
         results.type...inference:results.type...descriptive.only, 
         factor_key = T) %>%
  filter(v_2 == T)

# make names prettier
rf$results_format <- gsub("results.type...", "", rf$results_format)
rf$data_type <- gsub("data.type...", "", rf$data_type)

rf$results_format <- factor(as.character(rf$results_format), 
                            levels = c("inference", "prediction", "descriptive.only"), 
                            labels = c("inference", "prediction", "descriptive only"),
                            ordered = T)
```

```{r analysis_approach_by_data_type_assumptions_poisson_reg}
if(diag) {
  table(rf$data_type, rf$results_format)
  hist(table(rf$data_type, rf$results_format)) # doesn't look poisson-y
  
  addmargins(table(rf$data_type, rf$results_format))
  
  # check unconditional mean and variance - doesn't look the same
  mean(as.numeric(table(rf$data_type, rf$results_format)))
  var(as.numeric(table(rf$data_type, rf$results_format))) 

  
}
```

```{r resultsFormatByDataType}


if(diag) {
  pander(table(rev$results.type...descriptive.only), caption = "Descriptive Only Results (need at least 10 events/non evenst per predictor)")
}

dt_table <- table(rf$data_type)
names(dt_table) <- c("What, Where,\nWhen", "Abundance", "Known\nEffort", 
                     "Semi-\nStructured", "Visit-specific\nCovariates", 
                     "physical\nspecimen")
# pander(dt_table, caption = "Table of the gathered data_type column (compare to number of studies included)", keep.line.breaks = T)


kable(table(rf$data_type, rf$results_format), 
      caption = "Results formats produced using different types of data (this is counting studies, not counting analyses).  Is there a chi-squared-type test for when grouping variable categories are not mutually exclusive?")

```




Above looks ok, but probably need to do a chi-squared test or something to compare expected v. observed counts.  Or do glm with inference & hypothesis testing (or H testing, inference, and prediction) as positive class, descriptive as negative class, and data type as categorical predictors?  

### $H_02$

```{r results_by_data}
rev$quantitative <- rev$results.type...hypothesis.testing == T |
  rev$results.type...inference == T | rev$results.type...prediction == T

long_rev <- select(rev, title, quantitative, data.type...what.where.when.only:
                     data.type...detection...non.detection, 
                   data.type...sampling.effort.reported:data.type...physical.specimen) %>%
  gather(key = "data_type", value = "v_1", 
         data.type...what.where.when.only:data.type...physical.specimen) %>%
  filter(v_1 == T)

long_rev$quantitative <- gsub("TRUE", "inference/prediction", as.character(long_rev$quantitative))
long_rev$quantitative <- gsub("FALSE", "descriptive", as.character(long_rev$quantitative))
kable(table(long_rev$data_type, long_rev$quantitative))

```

```{r, echo = T}
if(diag) {
  table(rev$data.type...what.where.when.only)
  table(rev$data.type...abundance)
  table(rev$data.type...detection...non.detection)
  table(rev$data.type...sampling.effort.reported)
  table(rev$data.type...organized.data.collection.scheme)
  table(rev$data.type...visit.specific.covariates)
  table(rev$data.type...physical.specimen)
}
```

```{r fit_glm_analysis_approach}
results_data_model <- glm(quantitative ~ 
                            as.factor(data.type...what.where.when.only) + 
                            as.factor(data.type...abundance) + 
                            as.factor(data.type...detection...non.detection) + 
                            as.factor(data.type...sampling.effort.reported) + 
                            as.factor(data.type...organized.data.collection.scheme) + 
                            as.factor(data.type...visit.specific.covariates) + 
                            as.factor(data.type...physical.specimen), 
                          family = binomial, 
                          data = rev)
```

```{r glm_results}
# one_cov_mod <- glm(quantitative ~ data.type...what.where.when.only + 
#                             data.type...abundance, 
#                           family = binomial, 
#                           data = rev)

# summary(results_data_model)
# library(car)
# vif(results_data_model)
```











