---
title: "Biological Records Systematic Review - Preliminary Methods and Results"
csl: ecology.csl
output:
  word_document:
    toc: true
    toc_depth: 4
  html_document:
    number_sections: true
    df_print: kable
  html_notebook: default
  fig_caption: yes
  pdf_document: default
bibliography: PhD_references.bib
---

<div style="line-height: 2em;">

# Preliminary structure of analysis and results
This document states every question I plan to ask and shows methods, figures, tables and results text that I expect to use to answer each question in the biological records systematic review. Specific results and interpretations will change when I code the rest of the articles.

# General Questions for Dina

- Do I need to use the author, institution of the author, or source of the data in some way to account for possible pseudo-replication?  For example, the UK Butterfly Monitoring Scheme data is used in multiple studies.  Is it appropriate to count each of those uses as an independent data point in analyses of e.g temporal extent of studies?  I think it might be ok, because many studies (about a third) seem to integrate multiple datasets, so that many of the things I test might not depend only on a single dataset.  Thoughts?

- Do I need some sort of multiple comparison correction for doing multiple tests with the same dataset?


TODO:

- auto-generate table of contents

- add cross-validation to all model evaluation?

- Methods
    - *Study eligibiligy* [DONE]
    - Article coding
        - Data Type
        - Study Questions
        - *Analysis approach* [DONE]
        - *Agreement* [DONE]
    - *Temporal Extent* [DONE]
    - Analysis of study questions
        - *Most common paradigms* [DONE]
        - Development of methodology across different types of study questions
        - Type of data used across study question paradigms
    - Analysis approach & data type
    - Authors and Data Providing Institutions
- Results - Statistical
    - Search results
        - n returned, n not returned by multiple searche methods (represents incompleteness of search), eligibility, n vars coded, n studies coded, n double coded
    - Reader agreement
    - *Temporal extent* [DONE]
    - Authors and Data Providing Institutions
    - Study Question
        - *Paradigm* [DONE]
        - Data type by question paradigm (add stats)
    - Analysis approach & data type
- Results - Descriptive
    - Spatial extent
    - *Specific focus* [DONE]
    - Author associated with data provider
    - Spatial bias correction
    - Taxonomic group
    - Role of biological records
    - Prediction performance measure
    - Tabulate analysis methods
- Discussion
    - Article coding
        - Agreement
    - Temporal extent
    - Study question
    - Analysis approach & data type
    - Bio recs as predictors, facilitative roles (horizon scanning)
 


```{r, echo=F}
knitr::opts_chunk$set(echo=F, message=F, warning=T, cache=F)
```

```{r, load_data, cache = F, results='hide', warning = F}
setwd("~/Documents/Data_Analysis/UCD/systematic_review/")
library(wgutil)
library(Hmisc)
library(boot)
library(irr)
library(ResourceSelection)
library(captioner)
library(pander)
library(knitr)
library(ggridges)
library(bestglm)
library(car)
library(GGally)
library(tidyverse)

diag <- F # produce exploratory & diagnostics plots & results
diag_present <- T # produce diagnostics for report to Dina
plot_text_size <- 15

if(diag == F) { # turn off warnings when not running diagnostics
  knitr::opts_chunk$set(echo=F, message=F, warning=F, cache=F)
}

source("./clean_data_syst_rev.R")
```

```{r numberingPrep}
# make functions for adding numbered captions to figures 
figs <- captioner(prefix = "Fig.") 
tabs <- captioner(prefix = "Table")
```


# Methods
We did a systematic review of original research published since 2014 that used biological records from Ireland and/or the United Kingdom (UK).  We generated a list of relevant articles from Web of Science, Scopus, ProQuest, GoogleScholar, and the Global Biodiversity Information Facility (GBIF) website.  One researcher evaluated each article for inclusion eligibility and coded information on 46 variables describing characteristics of each article.  A second reader coded all variables for a subset of 20% of the articles.  Agreement between the two readers was evaluated using Krippendorff's alpha (Krippendorff 2004).  Variables that did not meet minimum standards for reader agreement were not used in subsequent analyses.  

## Study eligibility
Studies were deemed eligible if they presented original research (no reviews or idea papers) published in the English language, used opportunistic biological data collected with non-standardized or semi-standardized designs, included (but were not necessarily limited to) data from Ireland or the UK, and the full text of the study was available through the University College Dublin library online platform, Google Scholar, Google search results, or ResearchGate.  Grey literature was included and therefore peer review was not required.  Studies using semi-standardized data collection schemes (e.g. UK Butterfly Monitoring Scheme) were included as long as data collection included some opportunistic elements (e.g. locations chosen opportunistically by volunteers).  Publications of data (e.g. atlases or data papers) were not considered eligible unless they included analysis of the data.  Only studies using a sample size of greater than 20 were included; this sample size was chosen arbitrarily, mainly to exclude studies in which re-examination of museum specimens resulted in a taxonomic identification revision for one or a few specimens, thereby changing the known range of a species to include or exclude Ireland and the UK.  Studies using museum data were considered eligible when the museum data used was similar in format to biological records data (e.g. “what, where, when” data); studies that used museum specimens only for taxonomic, genetic or morphology studies were excluded.  Studies using only fossil records were not included.  Studies using data from phenology networks were included; for the purposes of this review such data are considered biological records data with associated additional data (e.g. the flowering status of plants).  Studies for which all data was collected by the study authors were not considered biological records data for the purposes of this review and were excluded.  The minimum required information in the data was a taxonomic name, a location, and date (“what, where, when”); additional information was permissible.  

## Article coding

For a list of the variables along with a description of each, see Appendix 1 *Instructions for Coders*.  Here we provide a brief summary of the broad types of information we coded.

### Data Type
We coded thirteen binary variables describing aspects of the type and structure of the biological records data.  The data type categories are not mutually exclusive.  

For most analyses, we grouped the data types "physical specimen", "photo", "audio", and "video" into a "voucher specimen" group, but we coded them individually in order to identify emerging trends in how vouchers are collected.

For statistical analyses, we included data type variables in models based on *a-priori* expectations about the variables' influence on data analysis methods and based on expectations about different levels of complexity and effort involved in collecting, recording, and storing the data.  The data type variables we kept in models were: what, where, when only; sampling effort known; abundance; non-detection; organized monitoring scheme; visit-specific covariates; multiple datasets integrated for analysis; life stage information; and voucher specimen. 

### Study Questions
We described the main focus(es) of each study in terms of whether the study was asking questions about individual species, ecological communities, or about methodology.

### Analysis approach
We classified the broad analysis approach used by each study into one of three categories: 1) inference; 2) prediction; or 3) description.  For each study, we evaluated the analysis approach only for analyses that used biological records data.

We considered studies estimating parameters and reporting some measure of uncertainty (e.g. *p*-values, confidence intervals, posterior probability distributions) to be using inference.  We considered studies that built models and made predictions to be using an analysis strategy of prediction, even if no uncertainty or inference was reported with the predictions.  Finally, we considered the analysis approach to be descriptive only if results were descriptive with no prediction or inference.  Descriptive results could be narrative, graphical, or quantitative as long as no inference or uncertainty was reported (e.g. point estimates of descriptive statistics without any confidence interval or *p*-values).  Studies could use both inference and prediction as analysis strategies and so those categories were not mutually exclusive, but studies were only categorized as using a descriptive analysis approach if all results were descriptive only and the study used no inference or prediction.  

### Agreement between two readers
We used Krippendorff's *alpha* [@Krippendorff2004] to evaluate the agreement between two readers for each of `r ncol(er)-6` variables that were coded by two people.  [**Dina, any thoughts about Krippendorff's alpha or other measures of agreement?**]. We calculated Krippendorff's *alpha* using the `kripp.alpha` function in the `irr` package [@irr2012].  Krippendorff's alpha measures agreement between multiple coders while accounting for aggrement by chance.  Values of Krippendorff's alpha range from **[??]** to 1, where 1 represents perfect agreement.  Rule-of-thumb guidelines suggest that Krippendorff's alpha values below 0.67 indicate poor agreement such that the data cannot be trusted for analysis; values between 0.67 and 0.8 suggest that the data are moderately reliable and may be suitable for only tentative interpretation, and values above 0.8 indicate that the data are reliable.  We removed variables with Krippendorff's alpha values below 0.67 from analyses; all other variables were kept, but we make note of variables for which Krippendorff's alpha values indicate only moderate reliability of the data. 


```{r inter_coder_agreement_subset_dfs, warning=TRUE}
## ellie's results ----------------------------------------------------
# remove columns that aren't coded variables 
er <- er[, which(colnames(er) %nin% c("link", "qualifies", "authors", 
                                      "publication", "doi", "year", 
                                      "keywords", "coding.DONE"))]
er <- er[order(er$title), ] # order rows
er <- er[, order(colnames(er))] # order columns
# put title column first
er <- er[, c(which(colnames(er) == "title"), 
                       which(colnames(er) != "title"))]
## end reorder ellie's results ------------------------------------------------

## reorder my results  --------------------------------------------------------
# subset my coded results to titles and columns coded by ellie
wg_dbl_cd <- wg[which(wg$title %in% er$title), which(colnames(wg) %in% colnames(er))]
# order rows
wg_dbl_cd <- wg_dbl_cd[order(wg_dbl_cd$title), ] 
# order cols
wg_dbl_cd <- wg_dbl_cd[, order(colnames(wg_dbl_cd))] 
wg_dbl_cd <- wg_dbl_cd[, c(which(colnames(wg_dbl_cd) == "title"), 
             which(colnames(wg_dbl_cd) != "title"))] # put title column first
## end reorder my results  ----------------------------------------------------

## make sure all titles are present
if(any(wg_dbl_cd$title %nin% er$title) | 
   any(er$title %nin% wg_dbl_cd$title)) {
  warning("er and wg_dbl_cd don't have the same titles. One or both of those data frames will be subsetted.")
  wg_dbl_cd <- wg_dbl_cd[which(wg_dbl_cd$title %in% er$title), ]
  er <- er[which(er$title %in% wg_dbl_cd$title), ]
}

## make sure df rows and cols are in the same order
if(!identical(colnames(er), colnames(wg_dbl_cd))) {
  stop("Columns in er and wg_dbl_cd must be identical and in the same order. They must be in the same order for creating the variable data frames that will be used to calculate Krippendorf's alpha. ")
}
```

```{r make_dfs_of_double_coded_variables_all_studies}
# each row is codings from one person
# each column is a study
dbl_coded_list <- list()
if(colnames(er)[1] != "title" | colnames(wg_dbl_cd)[1] != "title") {
  stop("The first column of er and wg must be the study title")}

for(i in 2:ncol(er)) {
  dbl_coded_list[[i-1]] <- data.frame(matrix(
    data = c(er[, i], wg_dbl_cd[, i]), 
    nrow = 2, ncol = nrow(er), byrow = T))
}
names(dbl_coded_list) <- colnames(er)[2:ncol(er)]
```

```{r krippendorfs_alpha_calc, warning=FALSE, message=FALSE}
do_kripp.alpha <- function(x, method) {
  # function to call kripp.alpha while converting dataframe x to a matrix
  # ARGS: x - data frame in which rows are observers and columns are studies
  #       method - character string of method to pass to kripp.alpha
  kripp.alpha(as.matrix(x), method = method)
}

kripp_all_dbl_coded <- lapply(dbl_coded_list, FUN = do_kripp.alpha, 
                              method = "nominal")
```


## Temporal extent of studies

```{r correlation_cap}
correlation_cap <- figs(name = "correlation_cap", 
                        "Correlation of the binary data type predictor variables.   **Dina, how should I assess correlation for binary variables?  Variance Inflaction Factor (VIF) or Pearson's correlation coefficient?**")
```

We used linear regression with natural-log transformed time (in years) as the response variable.  We used nine binary predictor variables indicating whether the study used at least some data that had the following characteristics or information: 1) only what, where, when information, 2) known sampling effort, 3) abundance, 4) explicit non-detection information, 5) data came from an organized monitoring scheme, 6) visit-specific covariates, 7) multiple different biological records datasets integrated for analyses, 8) life stage information, and 9) voucher specimens.  Although some of the predictor variables may be correlated (`r figs("correlation_cap", display = "cite")`), we kept all variables in the regression model because we were interested in the effect of each additional type of information *after accounting for other information types* (e.g. is there an additional effect of having abundance data after adjusting for whether the data came from an organized monitoring scheme).  However, we assessed collinearity of predictor variables using variance inflation factors in order to determine if collinearity is likely to cause additional uncertainty in the the coefficient estimates for the effect of each data type [@Fox1992]. **TODO: implmenet VIF for this**

Because the variables are not mutually exclusive, each category is treated as an individual binary variable, rather than treating each variable as a factor level of a categorical "data type" variable.  

The intercept-only model in this analysis is the mean temporal extent of all studies. 

$H_a$: The mean temporal extent of studies using only "what, where, when" data is longer than the mean temporal extent of studies using richer data types. 

$H_0$: The mean temporal extent of studies using what, where, when-only data is the same as the mean temporal extent of studies using richer data types. 

*Proposed Test*: Linear regression with natural-log transformed time (in years) as the outcome and data types as predictors.
 
```{r calc_temp_extent}
# calculate number of years covered by study
wg$temp_extent <- as.numeric(wg$end.year) - as.numeric(wg$start.year)
# individually assign values to studies with temporal extent < 1 year
if(diag) paste0("The following article has a temporal extent of zero: ", 
                wg$title[which(wg$temp_extent == 0)])
wg$temp_extent[which(
  grepl("An assessment of bumblebee .* land use and floral.*", 
        wg$title))] <- 0.33
```

```{r prepare_temporal_extent_df}
temp_extent <- select(
  wg, c(title, temp_extent, 
        data.type...what.where.when.only, 
        data.structure...sampling.effort.known, 
        data.type...abundance,
        data.structure...non.detection,
        data.structure...organized.data.collection.scheme, 
        data.type...visit.specific.covariates, 
        data.structure...multiple.datasets.integrated.for.analysis,
        data.type...life.stage, 
        data.type...voucher.of.some.kind.necessary.for.analysis)) %>%
  gather(key = "data_type", value = "dt_value", 
         data.type...what.where.when.only:data.type...voucher.of.some.kind.necessary.for.analysis, 
         factor_key = T) %>%
  filter(dt_value == T) %>%
  group_by(data_type)

# make names prettier
temp_extent$data_type <- gsub("data.type...", "", temp_extent$data_type)
temp_extent$data_type <- gsub("data.structure...", "", temp_extent$data_type)

temp_extent$data_type <- factor(as.character(temp_extent$data_type), 
                            levels = c("what.where.when.only", 
                                       "sampling.effort.known",
                                       "abundance",
                                       "non.detection", 
                                       "organized.data.collection.scheme", 
                                       "visit.specific.covariates", 
                                       "multiple.datasets.integrated.for.analysis", 
                                       "life.stage", 
                                       "voucher.of.some.kind.necessary.for.analysis"), 
                            labels = c("what where when", 
                                       "sampling effort known",
                                       "abundance",
                                       "non detection", 
                                       "organized scheme", 
                                       "visit specific covariates", 
                                       "multiple datasets", 
                                       "life stage", 
                                       "voucher used"))
```


Fit full model and evaluate residuals for assumptions of linear regression.
```{r fit_temp_extent_lm}
## fit linear regression with log-transformed years as response variable
## and each data type as a separate variable 
# fit full model
wg$log_years <- log(wg$temp_extent)
# variables added based on a-priori expectation of most to least important
time_lm_full <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                   data = wg, 
                   na.action = na.exclude) 

time_resids <- rstandard(time_lm_full) # calculate residuals
```

```{r time_data_type_full_model_sig_test}
# Test significance of full model
m0 <- lm(log_years ~ 1, data = wg) # fit null model 
m_time_full_anova <- anova(m0, time_lm_full)
```

```{r time_data_type_test_main_effects, include = F}
## test effect of each data type predictor by comparing the full model to a
# model with that predictor removed
# 
# TODO: Does this need multiple comparison correction?

m_no_whwhwh <- lm(log_years ~ 1 + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                  data = wg)
m_no_effort <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                  data = wg)
m_no_abund <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                 data = wg)
m_no_nonDet <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                  data = wg)
m_no_scheme <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                  data = wg)
m_no_visitCovs <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                     data = wg)
m_no_multData <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                    data = wg)
m_no_lifeStage <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                     data = wg)
m_no_voucher <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage,
                   data = wg)

indiv_mods_time <- list(m_no_whwhwh, m_no_effort, m_no_abund, m_no_nonDet, 
                        m_no_scheme, m_no_visitCovs, m_no_multData, 
                        m_no_lifeStage, m_no_voucher)
```


## Analysis of Study Questions

### What is the most popular study question paradigm for studies using biological records?

We categorized study questions into three broad question types: individual species questions, community questions, and other questions (which included methodological questions and others).  We counted the number of studies that asked only individual species questions, only community questions, both individual species and community questions, and "other" questions that were about neither individual species or communities.  We used bias-corrected accelerated bootstrap confidence intervals [@Efron1993] to estimate 95% confidence intervals around the number of studies asking each type of study question.  We determined statistical significance of differences in the number of studies asking each type of question by assessing whether the 95% bootstrap confidence intervals overlapped.

$H_a$: Some broad study question paradigms are more common than others in studies using biological records.

$H_0$: The number of studies pursuing each of the broad study question paradigms is the same.

*Proposed Test*: Bootstrap confidence intervals of number of studies within each study question paradigm.

```{r make_other_paradigm_category}
# make an "other paradigm" column for all studies that are not community or 
# individual species studies (this will be a catch-all category for methodology
# studies and whatever else)
wg$other_paradigm <- wg$community.question == F & 
  wg$individual.species.question == F
```

```{r make_both_paradigms_category}
# make a new variable indicating whether a study has both community and 
# individual species questions
wg$community_question_only <- wg$community.question == T & 
  wg$individual.species.question == F
wg$individual_species_question_only <- wg$individual.species.question == T &
  wg$community.question == F
wg$both_ind_and_community_questions <- wg$individual.species.question == T & 
  wg$community.question == T
```

```{r paradigm_bootstrap}
count_fun <- function(x, indices) {
  # ARGS: x - a vector of TRUE/FALSE values, for which the TRUES will be counted
  #       indices - a vector of indices as required by boot
  x_b <- x[indices]
  sum(as.numeric(x_b), na.rm = TRUE)
}

get_boot_CI <- function(x) {
  b_o <- boot(x, count_fun, R = 10000, stype = "i")
  ci_o <- boot.ci(b_o, conf = 0.95, type = "bca")
  to_return <- list(obs = x, 
                    boot_obj = b_o, 
                    ci_obj = ci_o)
  to_return
}

paradigm_list <- lapply(
  list(community_only = wg$community_question_only, 
       individual_sp_only = wg$individual_species_question_only, 
       both = wg$both_ind_and_community_questions, 
       other = wg$other_paradigm), 
  FUN = get_boot_CI)
```

### Ecological question

Within the broad study question paradigms (individual species questions and community questions), we assessed which response variables were used.  We used bias-corrected accelerated bootstrap confidence intervals [@Efron1993] to estimate 95% confidence intervals around the number of studies using each response variable within each paradigm. We determined statistical significance of differences in the number of studies asking each type of question by assessing whether the 95% bootstrap confidence intervals overlapped.  For studies asking individual species-based questions, we identified three response variables, species distributions, abundance, and phenology, and an "other" category for response variables that did not fit within one of those three categories.  For studies asking community-based questions, we identified two response variables, species richness (*alpha*-diversity or the number of species present), and species turnover (*beta*-diversity or any measure that incorporated species identity and/or abundance in addition to the number of species), plus an "other" category for response variables that did not fit in either of those two categories.  

$H_0$: There is no difference in the number of studies using each response variable.  

$H_a$: Some response variables are more commonly used than others. 

*Proposed test*: Bootstrap CIs around number of studies using each response variable. 

```{r make_other_response_variable_columns}
# make columns indicating which studies analyze something that does not fit
# within the response variable categories I am using.  I do this by finding
# studies that do not have any of my response variables marked TRUE, which means
# they must have been doing something else.
resp_var_df <- select(wg, title, individual.species.question,
                      community.question, response.variable...species.richness,
                      response.variable...diversity, 
                      response.variable...distribution, 
                      response.variable...abundance,
                      response.variable...phenology) 
# make column indicating individual species studies that don't use one of the 
# 3 main response variables
resp_var_df$individual_other_resp <- 
  resp_var_df$individual.species.question == T & 
  resp_var_df$response.variable...distribution == F & 
  resp_var_df$response.variable...abundance == F & 
  resp_var_df$response.variable...phenology == F
# make column indicating community studies that don't use one of the 2 main
# response variables
resp_var_df$community_other_resp <- resp_var_df$community.question == T & 
  resp_var_df$response.variable...species.richness == F & 
  resp_var_df$response.variable...diversity == F
```

```{r boot_response_variable}
resp_var_list <- lapply(
  list(comm_sp_rich = resp_var_df$response.variable...species.richness[which(
    resp_var_df$community.question == T)], 
    comm_diversity = resp_var_df$response.variable...diversity[which(
      resp_var_df$community.question == T)], 
    comm_other = resp_var_df$community_other_resp[which(
      resp_var_df$community.question == T)],
    ind_dist = resp_var_df$response.variable...distribution[which(
      resp_var_df$individual.species.question == T)],
    ind_abund = resp_var_df$response.variable...abundance[which(
      resp_var_df$individual.species.question == T)], 
    ind_phen = resp_var_df$response.variable...phenology[which(
      resp_var_df$individual.species.question == T)], 
    ind_other = resp_var_df$individual_other_resp[which(
      resp_var_df$individual.species.question == T)]),
  FUN = get_boot_CI)
```

### Development of methodology across different study questions

To test for differences in the the number of methodology studies across the different study focuses, we used logistic regression with six binary predictor variables indicating the specific biological or ecological focus(es) of the study and a binary response variable indicating whether the study addressed a methodological question (e.g. development of a new method or analysis of the performance of existing methods).  To evaluate whether assumptions were met for using logistic regression, we evaluated the number of samples per predictor variable, checked that all levels of predictor variables had more than 5 cases, checked for multicollinearity using the variance inflation factors [@Fox1992], performed a Hosmer-Lemeshow goodness of fit test, looked at the confusion matrix, and identified influential points using Pearson and deviance residuals and leverage.  

(Note that if counts of diversity studies remain small we could lump "diversity" and "species richness" studies into a single category.)

$H_0$: There is no effect of the ecological focus of a study on the probability of the study addressing methodological questions.  

$H_a$: The ecological focus of a study affects the probability of the study addressing methodological questions.  

```{r methodology_by_study_focus_mod}
method_mod <- glm(factor(as.character(methodology.development.or.analysis), 
                         levels = c("FALSE", "TRUE"), 
                         labels = c("FALSE", "TRUE")) ~ 1 +
                    factor(response.variable...species.richness) + 
                    factor(response.variable...diversity) + 
                    factor(response.variable...distribution) + 
                    factor(response.variable...abundance) + 
                    factor(response.variable...phenology) + 
                    factor(trends.over.time), 
                  data = wg, family = binomial())
method_mod_null <- glm(factor(as.character(methodology.development.or.analysis), 
                         levels = c("FALSE", "TRUE"), 
                         labels = c("FALSE", "TRUE")) ~ 1, 
                  data = wg, family = binomial())
```


### What data types are used for each study question paradigm? 

We fit Poisson regression models of the counts (number of studies) for each combination of data type and study question paradigm, fitting both a saturated model and an independence model with only main effects.  We used a Chi-squared likelihood ratio test to compare the saturated and independence models. 

Saturated model: `n studies ~ data type + paradigm + data type:paradigm`

Independence model: `n studies ~ data type + paradigm`

The response is the cell counts. (Penn State tutorial 10.1.1).  The Poisson regression model assumes that the observations (cell counts) are independent.  I wonder about this for the different data type variables.  For example, it is possible that if a study uses data from an organized monitoring scheme, that data also has abundance information, so that the counts in the "organized monitoring scheme" cells and the "abundance" cells would go up together, making them not independent.  We checked for independence among the data type variables by using a permutation procedure to look for non-random patterns in the data types.  The permutation procedure involved, for each pair of data type variables, making a table in which each study is a row and there is a column for each of the two data type variables being assessed. **TODO**

$H_a$: Different broad study question paradigms use different types of biological records data.

$H_0$: The use of data types is the same for all study question paradigms.

*Proposed Test*: Poisson regression with Chi-squared likelihood ratio test of independence.

I think an assumption of the Poisson regression is that the cell counts are independent, meaning that the count in one cell doesn't go up or down with the count in another cell.  So I need to asses if there is any dependence between the levels of each categorical variable.  For example, does the count in the "sampling effort x community" cell go up in correlation with the count in the "abundance x community" cell?  Could I use some kind of permutation approach to test for independence of cell counts?  **Dina, how should I check to make sure cell counts are independent?  How how should I check other model assumptions for the Poisson regression?**

```{r data_type_paradigm_df_prepare}
# make data frame with counts of data type by study paradigm
dt_pd_df <- select(wg, data.structure...organized.data.collection.scheme, 
                   data.structure...sampling.effort.known,
                   data.structure...non.detection,
                   data.structure...multiple.datasets.integrated.for.analysis,
                   data.type...what.where.when.only,
                   data.type...abundance, 
                   data.type...visit.specific.covariates, 
                   data.type...life.stage, 
                   data.type...voucher.of.some.kind.necessary.for.analysis,
                   community_question_only, 
                   individual_species_question_only, 
                   both_ind_and_community_questions, 
                   other_paradigm) %>%
  gather(key = "data_type", value = "dt_val", 
         data.structure...organized.data.collection.scheme:
           data.type...voucher.of.some.kind.necessary.for.analysis) %>%
  filter(dt_val == TRUE) %>%
  gather(key = "paradigm", value = "pd_val", community_question_only, 
         individual_species_question_only, both_ind_and_community_questions, 
         other_paradigm) %>%
  filter(pd_val == TRUE) %>%
  count(paradigm, data_type) %>%
  complete(paradigm, data_type, fill = list(n = 0))

# spread for easy visualisation of 2-way table
dt_pd_df_wide <- spread(dt_pd_df, key = paradigm, value = n)
```

```{r data_type_paradigm_model}
dt_pd_mod_full <- glm(n ~ 1 + paradigm + data_type + paradigm:data_type, 
                      data = dt_pd_df, family = poisson())
dt_pd_mod_ind <- glm(n ~ 1 + paradigm + data_type, 
                     data = dt_pd_df, family = poisson())
# Do likelihood ratio test for full model vs. independence model
dt_pd_interaction_test <- anova(dt_pd_mod_full, dt_pd_mod_ind, test = "Chisq") 
```


## Analysis approach & data type

To investigate whether the data type affects the analysis approach, we used a Poisson regression to model the relationship between three different analysis approaches (inference, prediction, and description) and data type.  The data type categories included in models based on *a-priori* expectations of importance were (in order of expected importance): what, where, when only; sampling effort known; abundance; non-detection; organized monitoring scheme; visit-specific covariates; multiple datasets integrated for analysis; life stage information; and voucher specimen. 

$H1_a$: Different types of data are analysed with different broad analysis approaches.

$H1_0$: Different types of data are analysed with the same broad analysis approaches.

*Proposed Test*: A likelihood ratio test to compare a saturated Poisson regression modeling cell counts with a model with only the main effects of data type and analysis approach (but no interaction).  

```{r data_type_analysis_approach_df}
dt_an_df <- select(wg, data.structure...organized.data.collection.scheme, 
                   data.structure...sampling.effort.known,
                   data.structure...non.detection,
                   data.structure...multiple.datasets.integrated.for.analysis,
                   data.type...what.where.when.only,
                   data.type...abundance, 
                   data.type...visit.specific.covariates, 
                   data.type...life.stage, 
                   data.type...voucher.of.some.kind.necessary.for.analysis,
                   results.type...inference, 
                   results.type...prediction, 
                   results.type...descriptive.only) %>%
  gather(key = "data_type", value = "dt_val", 
         data.structure...organized.data.collection.scheme:
           data.type...voucher.of.some.kind.necessary.for.analysis) %>%
  filter(dt_val == TRUE) %>%
  gather(key = "analysis_approach", value = "an_val", results.type...inference,
         results.type...prediction, results.type...descriptive.only) %>%
  filter(an_val == TRUE) %>%
  count(analysis_approach, data_type) %>%
  complete(analysis_approach, data_type, fill = list(n = 0))

# spread for easy visualisation of 2-way table
dt_an_df_wide <- spread(dt_an_df, key = analysis_approach, value = n)
```

```{r data_type_analysis_approach_model}
# fit saturated model
dt_an_mod_full <- glm(n ~ 1 + analysis_approach + data_type + 
                        analysis_approach:data_type, data = dt_an_df, 
                      family = poisson())
# fit model assuming independence between analysis approach and data type
dt_an_mod_ind <- glm(n ~ 1 + analysis_approach + data_type, 
                     data = dt_an_df, family = poisson())
dt_an_mod_independence_test <- anova(dt_an_mod_full, dt_an_mod_ind, test = "Chisq")
```

---------------------

## Authors and Data Providing Institutions

**Who choses to use biological records data?**

We used a permutation approach to assess whether the number of lead authors associated with the data providing institution is different than expected by chance.  To break all associations between first author, last author, and the data providing institution, we randomly permuted the institutions of the first author and the data providing institutions.  We then counted how many cases there were in which the first and/or last author(s) were from the same institution that provided the data.  This created a distribution of counts under the null hypothesis that authors choose their main co-authors and the data source randomly from the population of other authors and data providers that appear on studies in this review.  We performed 9,999 random permutations and included the observed data as one of the permutations, for a total of 10,000 permutations.  We calculated a two-tailed empirical *p*-value by dividing the number of count values from the permuted datasets that were more extreme than the count from the observed data by the total number of permutations:

`p <- min(length(which(permuted_counts >= real_count)), length(which(permuted_counts <= real_count))) / 10000`

$H_a$: The primary authors of a study are more likely to use data held by their own institution than by other institutions.

$H_0$: The number of studies with a lead author from the institution that provided the data is the same as expected if each lead author chose a co-lead author and data providing institution at random from the population of lead authors and data providing institutions that appear on studies in this review. **Jon & Dina, this is very specific wording to describe the null hypothesis made by permutation.  Is there a better way to word this?**

*Proposed Test*: Permutation test to generate a null hypothesis representing no relationship between the first author and last authors or between either author and the institution that provided the data.  


```{r author_inst}
# create a variable that indicates whether either the 1st or last author is from
# the data providing organization
wg$lead_author_from_data_inst <- mapply(FUN = function(a, b, c) {
  any(c(unlist(a), unlist(b)) %in% unlist(c))}, 
  strsplit(wg$institution.of.first.author, split = ";"), 
  strsplit(wg$institution.of.last.author, split = ";"), 
  strsplit(wg$proximate.data.source, split = ";"))
```

```{r permute_author_inst}
# permute to break all associations between first author, last author, and
# data providing institution
permute_institutions <- function(index, df) {
  ## ARGS:  index - an unused variable that is here as a first argument so that lapply can call this function many times
  ##        df - a data frame with the systematic review data.  Column names to be permuted are hard coded in this function, so those names must appear in df.
  # permute data source column
  df$proximate.data.source <- sample(df$proximate.data.source, replace = F)
  # permute first author column
  df$institution.of.first.author <- sample(df$institution.of.first.author, 
                                           replace = F)
  # find whether a lead author is from the data source institution
  matches <- mapply(FUN = function(a, b, c) {
    any(c(unlist(a), unlist(b)) %in% unlist(c))}, 
    strsplit(df$institution.of.first.author, split = ";"), 
    strsplit(df$institution.of.last.author, split = ";"), 
    strsplit(df$proximate.data.source, split = ";"))
  # count how many authors are from the data source institution
  sum(matches)
}

# set true data value
real_count <- sum(wg$lead_author_from_data_inst)
# get null distribution
permuted_counts <- sapply(c(1:9999), FUN = permute_institutions, df = wg)
# append true value as a permutation in the null distribution
permuted_counts <- c(permuted_counts, real_count)

# calculate empirical p-value from permutations
p_auth_perm <- min(length(which(permuted_counts >= real_count)), 
                   length(which(permuted_counts <= real_count))) / 
                     length(permuted_counts)

# hist(perm_n_author_from_inst, 
#      xlim = c(min(c(perm_n_author_from_inst, real_count)), 
#               max(c(perm_n_author_from_inst, real_count))))
# abline(v = real_count)
```

```{r boot_author_institution}
# get bootstrap CIs for proportion of studies with a lead author from the data
# providing institution and for the proportion of studies with any author from
# the data providing institution
lead_auth_inst_boot <- boot(wg$lead_author_from_data_inst, 
                            statistic = function(x, ind) {
                              sum(x[ind]) / length(x)}, 
                            R = 999)
any_auth_inst_boot <- boot(wg$author.associated.with.proximate.data.provider, 
                           statistic = function(x, ind) {sum(x[ind]) / length(x)}, 
                           R = 999)
```

# Model Assumptions

## Temporal extent of studies

The following plots are for assessing assumptions for a linear regression of the form `log(number of years) ~ data type`.  Natural-log transformation of the number of years that studies cover improves normality (`r figs(name = "temp_norm_box_cap", display = "cite")`, `r figs(name = "temp_norm_ridge_cap", display = "cite")`).  

```{r time_assess_correlation_predictors, fig.height=8, fig.width = 8, fig.cap=correlation_cap}
### assess correlation between data type predictors ----------------------
dt_df <- wg[, which(
  colnames(wg) %in% 
    c("data.structure...organized.data.collection.scheme", 
      "data.structure...sampling.effort.known", 
      "data.structure...non.detection", 
      "data.structure...multiple.datasets.integrated.for.analysis", 
      "data.type...what.where.when.only", 
      "data.type...abundance", 
      "data.type...visit.specific.covariates", 
      "data.type...life.stage", 
      "data.type...voucher.of.some.kind.necessary.for.analysis"))]
for(i in 1:ncol(dt_df)) {
  dt_df[, i] <- as.factor(as.character(data.frame(dt_df)[, i]))
}

if(diag_present) {
  print(ggpairs(dt_df, columns = 1:ncol(dt_df), 
              upper = list(discrete = "blank"),
              lower = list(discrete = "ratio"), 
              diag = list(discrete = "barDiag"), 
              columnLabels = c("organized scheme", 
                               "sampling effort",
                               "non detection",
                               "multiple datasets", 
                               "what where when only", 
                               "abundance",
                               "visit specific covariates", 
                               "life stage", 
                               "voucher used"), 
              labeller = label_wrap_gen(width = 10)) + 
        ggtitle("Correlation of data type predictor variables") + 
        theme(axis.text.x = element_text(angle = 50, hjust = 1)))
}

```

```{r temp_norm_box_cap} 
temp_norm_box_cap <- figs(name = "temp_norm_box_cap", 
                          "Boxplot of temporal extent of studies in log-transformed years for different data types.  Log transformation makes normality much better than it is with raw data.")
```

```{r temp_norm_ridge_cap} 
temp_norm_ridge_cap <- figs(name = "temp_norm_ridge_cap", 
                          "Histograms and densities of temporal extent of studies in log-transformed years for different data types.  Log transformation makes normality much better than it is with raw data.")
```

```{r temp_range_distribution_plot_print, fig.cap=temp_norm_box_cap} 
if(diag_present) {
  ## normality -------------------------------------
  # distribution of all data
  # hist(wg$temp_extent)
  # hist(wg$temp_extent[which(wg$temp_extent <= quantile(wg$temp_extent, 
  #                                                        0.90, na.rm = T))])
  
  # data very non-normal but log transformation kinda fixes this
  # (reciprocal and sqrt transformations did not help)
  print(ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
          geom_boxplot(varwidth = T) +
          scale_y_log10() + 
          theme_bw() + 
          ggtitle("log transformation of year\nmakes normality better") + 
          ylab("Temporal extent (years)") + 
          theme(axis.text.x = element_text(angle = 40, vjust = 0.9, 
                                           hjust = 1)))
}
```

```{r temp_range_distribution_ridge_plot_print, fig.cap=temp_norm_ridge_cap}
if(diag_present) {
  print(ggplot(data = temp_extent, aes(x = temp_extent, y = data_type)) + 
          scale_x_log10() + 
          geom_density_ridges(stat = "binline", alpha = 0.4) +
          geom_density_ridges(alpha = 0.4) + 
          xlab("Temporal extent (years)") + 
          theme_bw())
}
```

```{r temp_range_data_table}
if(diag_present) {
  ## data are not balanced
  print("Data are not balanced.")
  kable(table(temp_extent$data_type))
}
```

```{r temp_range_distribution_years_by_data_type}
# distribution of log(years) by data type  
if(diag) {
  for(i in unique(temp_extent$data_type)) { 
    #hist(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
    qqnorm(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)], 
           main = i)
    qqline(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
  }
}
```

```{r temp_extent_lm_diags_print, fig.height=4, fig.width=4}
if(diag_present) {
  print("Diagnostics for main effects model log_years ~ all 9 data types (no interactions)")
  plot(time_lm_full)
  # hist(time_resids, 
  #      main = "standardized residuals of full model for temporal extent", 
  #      xlab = "standardized residuals")
  boxplot(time_resids, ylab = "standardized residuals")
}
time_summary <- summary(time_lm_full)
if(diag) time_summary

## to interpret outcome on original scale:
# for a one-unit increase in x, the predicted value of y 
# changes by (e^beta1 - 1)*100 percent
```

## Analysis of study questions
### What is the most popular study question paradigm?

No diagnostics needed for bootstrap estimation of confidence intervals, right?

### Ecological question

No diagnostics needed for bootstrap confidence intervals, right?

### Development of methodology across different study questions
```{r method_assumptions}
if(diag_present) {
  method_mod_vif <- vif(method_mod) # assess multicollinearity
  # H-L test H_0 is that model fits data well
  method_mod_hlTest <- hoslem.test(method_mod$y, method_mod$fitted.values, 
                                   g = 10) 
  # make confusion matrix for full model with threshold 0.5
  method_conf_matrix <- table(method_mod$y, method_mod$fitted.values > 0.5)
  method_misclass_rate <- (method_conf_matrix["1", "FALSE"] + 
                             tryCatch(method_conf_matrix["0", "TRUE"], 
                                      error = function(x) 0))/length(method_mod$y)
  # confusion matrix for null model
  method_null_conf_matrix <- table(method_mod_null$y, 
                                   method_mod_null$fitted.values > 0.5)
  method_null_misclass_rate <- (method_null_conf_matrix["1", "FALSE"] + 
                             tryCatch(
                               method_null_conf_matrix["0", "TRUE"], 
                               error = function(x) 0))/length(
                                 method_mod_null$y)
  method_diags <- LogisticDx::dx(method_mod, byCov = F)
  
  # determine which articles had large residuals or leverage
  big_resids <- c(wg$title[abs(method_diags$Pr) > 2], 
                  wg$title[abs(method_diags$dr) > 2], 
                  wg$title[abs(method_diags$h) > 2])
  big_resids <- unique(big_resids)
  big_resids_df <- data.frame(
    title = wg$title[which(wg$title %in% big_resids)],
    pearson_resids = method_diags$Pr[which(wg$title %in% big_resids)], 
    deviance_resids = method_diags$dr[which(wg$title %in% big_resids)], 
    leverage = method_diags$h[which(wg$title %in% big_resids)], 
    methodology.development.or.analysis = 
      wg$methodology.development.or.analysis[which(wg$title %in% big_resids)])
}
```

```{r method_vif_cap} 
method_vif_cap <- tabs(name = "method_vif_cap", 
                       "Variance inflation factors for variables used in the logistic regression estimating the proportion of studies with a methodological focus across different ecological study questions.")
```

```{r pearson_resids_cap}
pearson_resids_cap <- figs(name = "pearson_resids_cap", 
                           "Histogram of Pearson residuals for the logistic regression model of methodology studies across study questions.  Residuals with an absolute value larger than 2 are potentially problematic.")
```

```{r deviance_resids_cap}
deviance_resids_cap <- figs(name = "deviance_resids_cap", 
                           "Histogram of deviance residuals for the logistic regression model of methodology studies across study questions.  Residuals with an absolute value larger than 2 are potentially problematic.")
```

```{r leverage_cap}
leverage_cap <- figs(name = "leverage_cap", 
                           "Histogram of leverage for the logistic regression model of methodology studies across study questions.  Value larger than 2 are potentially problematic.")
```

```{r big_resids_cap}
big_resids_cap <- tabs(name = "big_resids_cap", 
                       "Studies with large Pearson or deviance residuals or leverage > 2.")
```

```{r cell_count_cap}
cell_count_cap <- tabs(name = "cell_count_cap", 
                       "Cell counts for predictor variables in methodology logistic regression.  Cell counts should be > 5.")
```

```{r confusion_matrix_cap}
confusion_matrix_cap <- tabs(name = "confusion_matrix_cap", 
                             "Confusion matrix for logistic regression model. (Right now the highest predicted probability from the model is 0.46, so the model does not predict any (TRUE) values.")
```

There were `r table(wg$methodology.development.or.analysis)[[2]]` methodological studies and `r table(wg$methodology.development.or.analysis)[[1]]` non-methodological studies, suggesting that we can fit `r min(table(wg$methodology.development.or.analysis))/10` predictors if we need at least 10 events or non-events per predictor. Obviously, if the number of methodological studies found by the review remains low, we will have to group some of the study question predictor variables.  We can do this based on which predictors have sparse cells (see `table()` outputs below).  

Variance inflation factors for all variables were well below 10 (`r tabs("method_vif_cap", display = "cite")`) indicating that correlation among the predictor variables is not likely to cause uncertainty in the coefficient estimates of the logistic regression model [@Fox1992] **Dina, am I interpreting this correctly?**.  A Hosmer-Lemeshow goodness of fit test failed to reject the null hypothesis that the model fits the data well (*p* = `r round(method_mod_hlTest$p.value, 2)`). There were `r length(big_resids)` observations with big Pearson or deviance residuals or with leverage larger than two (`r tabs("big_resids_cap", display = "cite")`).  The misclassification rates (when using a cutoff value of 0.5) for the model including main effects of all study questions and the null model (intercept only) were `r method_misclass_rate` and `r method_null_misclass_rate`, respectively. 

Print number of events:
```{r print_n_events, echo=TRUE}
if(diag_present) {
  # check number of events/ non-events to make sure we have enough data
table(wg$methodology.development.or.analysis)
}
```

`r tabs("cell_count_cap", display = "full")`

```{r cell_counts_table_print}
if(diag_present){
  # check for sparse cells in predictors (need > 5 observations in each cell)
  kable(data.frame(select(wg, response.variable...phenology, trends.over.time, 
                    response.variable...diversity, response.variable...abundance, 
                    response.variable...species.richness, 
                    response.variable...distribution) %>%
               apply(MARGIN = 2, FUN = table)) %>%
    rownames_to_column() %>%
    gather(-1, key = "variable", value = "count") %>%
    spread(rowname, count))
}
```

`r if(diag_present) tabs("method_vif_cap", display = "full")`

```{r print_method_vif}
if(diag_present) {
  vif_df <- data.frame(method_mod_vif)
  vif_df$variable <- rownames(vif_df)
  rownames(vif_df) <- NULL
  kable(vif_df, col.names = c("VIF", "variable"), digits = 2)
}
```

```{r print_pearson_resids_plot, fig.cap=pearson_resids_cap}
if(diag_present) {
  hist(method_diags$Pr, main = "Pearson residuals for methodology\nby study focus logistic regression")
}
```

```{r deviance_resids_methodology_plot, fig.cap=deviance_resids_cap}
if(diag_present) {
  hist(method_diags$dr, main = "Deviance residuals for methodology\nby study focus logistic regression")
}
```

```{r leverage_methodology_plot, fig.cap=leverage_cap}
if(diag_present) {
  hist(method_diags$h, main = "Leverage for methodology\nby study focus logistic regression")
}
```

`r if(diag_present) tabs("big_resids_cap", display = "full")`
```{r print_big_resids_table}
if(diag_present) kable(big_resids_df, digits = 2)
```

`r if(diag_present) tabs("confusion_matrix_cap", display = "full")`
```{r print_confusion_matrix_methodology}
if(diag_present) {
  kable(method_conf_matrix)
}
```

### What data types are used for each study question paradigm?

**I will wait for Dina's suggestions about how to assess assumptions and model fit for the Poisson regressions.**

```{r data_type_parad_table_cap}
data_type_parad_table_cap <- tabs(name = "data_type_parad_table_cap", 
                                  "Table of counts for data type by study question paradigm.")
```

`r tabs("data_type_parad_table_cap", display = "full")`

```{r print_data_type_paradigm_contingency_table}
if(diag_present) {
  dt_pd_df_wide$data_type <- gsub(
    "data\\.structure\\.\\.\\.|data\\.type\\.\\.\\.", "", 
    dt_pd_df_wide$data_type)
  dt_pd_df_wide$data_type <- gsub("\\.", " ", dt_pd_df_wide$data_type)
  colnames(dt_pd_df_wide) <- gsub("_", " ", colnames(dt_pd_df_wide))
  kable(data.frame(dt_pd_df_wide))
}
```

## Analysis approach & data type

**I will wait for Dina's suggestions about how to assess assumptions and model fit for the Poisson regressions.**

## Authors and data providing institutions

No diagnostics or assumption checking needed for permutation test, right?  

# Results

```{r kripp_sum_cap}
kripp_sum_cap <- figs(name = "kripp_sum_cap", 
                      paste0("Reader agreement measured using Krippendorff's alpha, for variables for which the sample size is large enough to estimate Krippendorff's alpha.  Dotted horizontal lines show Krippendorff's alpha values of 0.667 and 0.8, which are recommended minimum and preferred values at which variables can be relied upon for analysis."))
```

## Search results 
The search returned `r nrow(elig)` potentially relevant studies, of which we have evaluated `r sum(!is.na(elig$qualifies))` for eligibility, and judged `r length(which(elig$qualifies == TRUE))` to be eligible for inclusion in this review.  [???] percent of potentially relevant studies were returned by more than one search method, while [???] percent of potentially relevant studies were returned by only one search method.  

One reader has coded `r nrow(wg)` articles and a second reader has coded `r nrow(er)` of those. 

This preliminary analysis uses the `r nrow(wg)` articles coded so far.  

## Reader agreement

Most variables for which Krippendorff's *alpha* was estimated show that coding is reliable enough to be used in analysis, at least for drawing tentative conclusions (`r figs("kripp_sum_cap", display = "cite")`).  Many of the variables have Krippendorff's *alpha* values above the minimum required level of 0.67 but below the prefered level of 0.8, which indicates that they should be interpreted with caution.  Fewer variables have Krippendorff's *alpha* values higher than the prefered level of 0.8.  The number of articles that have been evaluated by two readers is too small for reliable estimation of Krippendorff's alpha for some variables for which the smallest class is particularly uncommon (`r tabs("kripp_samp_size_cap", display = "cite")`).  `r tabs("kripp_samp_size_cap", display = "cite")` shows estimates of Krippendorff's *alpha* for each variable and indicates whether the sample size is large enough to estimate Krippendorff's *alpha* with a confidence level of 0.1 for a minimum Krippendorff's *alpha* value of 0.667 [@Krippendorff2004].  Some of the variables we coded are not reliable according to Krippendorff's *alpha*.  

In the final manuscript I will not include those variables in analysis, though it may be worthwhile to discuss why they were difficult to code reliably.  For this draft, I have printed a table showing the Krippendorff's *alpha* values for the variables used in each analysis at the beginning of each sub-section in the *Results*.  In this draft I have kept variables in analyses even if the preliminary estimate of Krippendorff's *alpha* is too low.  

```{r krippendorfs_alpha_summary, fig.cap = kripp_sum_cap, include = F}
kripp_summary_long <- data.frame(
  variable = names(kripp_all_dbl_coded), 
  krip_alpha = sapply(kripp_all_dbl_coded, 
                        FUN = function(x) {x$value}))

if(diag) {
  print(
    ggplot(data = kripp_summary_long, 
           mapping = aes(y = krip_alpha)) + 
      geom_boxplot() + 
      geom_point(aes(x = 0)) + 
      geom_hline(aes(yintercept = 0.667), lty = "dotted") + 
      geom_hline(aes(yintercept = 0.8), lty = "dashed") + 
      ggtitle("Reader agreement using Krippendorff's alpha") + 
      xlab("") +
      ylab("Krippendorff's alpha") + 
      theme_bw() + 
      theme(axis.text.x = element_blank())
  )
}
```


```{r kripp_samp_size_cap}
kripp_samp_size_cap <- tabs(name = "kripp_samp_size_cap", 
                            paste0("Recommended sample size for accurate estimation of Krippendorff's alpha given the proportion of the minority class in the results, and a desired confidence level of 0.1 for a minimum Krippendorff's alpha value of 0.667 (Krippendorff 2004).  So far there are ", nrow(wg), " double-coded studies."))
```

`r if(diag_present) tabs("kripp_samp_size_cap", display = "full")`

```{r calculate_proportion_T}
## This calculates the probability of the smallest class for each variable, for 
## use in calculating minimum sample size needed for Kripp's alpha for that 
## variable.  
## This treats all variables as factor variables.
prob_of_values <- lapply(
  wg[, which(colnames(wg) %in% names(kripp_all_dbl_coded))], 
  FUN = function(x) {
    x <- factor(x)
    mn <- 1
    for(j in 1:length(levels(x))) {
      prob <- length(which(x == levels(x)[j])) / length(which(!is.na(x)))
      if(prob < mn) mn <- prob
    }
    mn
  })

prob_of_values <- data.frame(variable = names(prob_of_values), 
                             prob_small_class = unlist(prob_of_values), 
                             row.names = NULL)
prob_of_values$kripp_alpha <- NA
for(i in 1:nrow(prob_of_values)) {
  # add krippendorff's alpha for codings
  prob_of_values$kripp_alpha[i] <- kripp_all_dbl_coded[[which(
    names(kripp_all_dbl_coded) == prob_of_values$variable[i])]]$value
}
prob_of_values <- prob_of_values[order(prob_of_values$kripp_alpha, 
                                       decreasing = T), ]

# On p 240 of Krippendorff book, it says that if acceptable alpha is 0.667 and 
# desired significance level is 0.1, then sample size needed is as follows:
#prob_of_values$n_25_adequate <- prob_of_values$prob_small_class > 0.25
#prob_of_values$n_30_adequate <- prob_of_values$prob_small_class > 0.2
prob_of_values$n_40_adequate <- prob_of_values$prob_small_class >= 0.143
# TODO add column for sample size of 50

if(diag_present == TRUE) {
  kable(data.frame(prob_of_values[, -2]), 
              col.names = c("variable", "Krippendorff's alpha", 
                            "sample size adequate"), 
              digits = 2)
}
if(diag) {
  # list only those for which sample size is adequate
  samp_adequate <- prob_of_values[which(prob_of_values$n_40_adequate == T), ]
  print(kable(samp_adequate[order(
    samp_adequate$kripp_alpha, decreasing = T), -2], 
    digits = 2))
}
rm(samp_adequate)
```

```{r plot_kripp_for_adequate_sample_size, fig.cap=kripp_sum_cap}
# plot krippendorf's alpha values for only variables that have adequate sample size
ggplot(data = prob_of_values[which(prob_of_values$n_40_adequate == T), ], 
       aes(y = kripp_alpha)) +
  geom_boxplot() + 
  geom_point(aes(x = 0)) + 
  geom_hline(aes(yintercept = 0.667), lty = "dotted") + 
  geom_hline(aes(yintercept = 0.8), lty = "dashed") + 
  ggtitle("Krippendorf's alpha for the variables for\nwhich we have adequate sample size") +
  ylab("Krippendorf's Alpha value") + 
  xlab("") +
  theme_bw() + 
  theme(axis.text.x = element_blank())
```


## Temporal extent of studies

```{r temp_extent_cap}
temp_extent_cap <- figs(name = "temp_extent_cap", 
                        "The temporal extent covered by studies using different types biological records data.  This plot shows the observed temporal extent of studies (not the fitted values from the regression model).  Note the log-transformed y-axis.")
```

Coder agreement for variables in this analysis:

```{r vars_temp_extent}
if(diag_present) {
  vs <- c("start.year", "end.year", "data.type...what.where.when.only",
          "data.structure...sampling.effort.known", "data.type...abundance", 
          "data.structure...non.detection",
          "data.structure...organized.data.collection.scheme", 
          "data.type...visit.specific.covariates", 
          "data.structure...multiple.datasets.integrated.for.analysis", 
          "data.type...life.stage", 
          "data.type...voucher.of.some.kind.necessary.for.analysis")
  kable(data.frame(prob_of_values[which(prob_of_values$variable %in% vs), 
                       c("variable", "kripp_alpha", "n_40_adequate")]), 
              digits = 2)
  rm(vs)
}
```

```{r print_time_anova, include = F}
kable(m_time_full_anova)
```

```{r print_time_individual_predictor_significance}
if(diag) {
  for(i in 1:length(indiv_mods_time)) { 
    print(anova(indiv_mods_time[[i]], time_lm_full))
  }
}
```

The overall model using data type to predict the natural log of temporal extent of study (in years) was  significant ($F_{`r format(round(m_time_full_anova$Df[2], digits = 2), scientific = F)`, `r format(round(m_time_full_anova$Res.Df[2], digits = 2), scientific = F)`}$ = `r format(round(m_time_full_anova$F[2], digits = 2), scientific = F)`, *p* = `r format(round(m_time_full_anova[6][[1]][2], digits = 3), scientific = F)`, Adjusted $R^2%$ = `r format(round(time_summary$adj.r.squared, digits = 2), scientific = F)`, `r figs("temp_extent_cap", display = "cite")`).  In the final manuscript I will individually test whether each variable has a significant effect after accounting for the other variables.  I will interpret the direction and effect size for all variables in the context of the full model even if the variables are not significant on their own.  Example of how results will be written (specific results will change in the final version):
 
The expected temporal extent covered by a study that uses only "what, where, when" data is `r round(exp((time_lm_full$coefficients[1] + time_lm_full$coefficients[2])), digits = 1)` years when the data have no other data type characteristics.  When the data are "what, where, when" and multiple datasets were integrated for analysis, the expected temporal extent covered by a study is `r round(exp((time_lm_full$coefficients[1] + time_lm_full$coefficients[2] + time_lm_full$coefficients[8])), digits = 1)` years. 

Using data that includes sampling effort information had a negative effect on the temporal extent of studies after accounting for other data types (*p* = `r format(round(time_summary$coefficients[2, 4], digits = 3), scientific = F)`).  The effects of including data with non-detection and abundance information were not significant after accounting for the effects of the other data types.  The direction of the effects of using data with sampling effort information and detection/non-detection information were both negative, as expected.  However, the direction of the effect of using abundance data was unexpectedly positive.  This may be because abundance is correlated with detection/non-detection and sampling effort, and so the effect of abundance data is positive after correcting for those other data types.  When I fit a model with only the intercept and the abundance data type, the direction of the effect of abundance data was negative as expected.    

```{r time_extent_results_plot, fig.cap=temp_extent_cap}
## TODO: need to hand-annotate R2 and p-value in plot
temp_extent$sig_different_from_WhWhWh <- temp_extent$data_type %in% 
                                       c("[names of sig. diff. vars. here")
print(ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + #, color = sig_different_from_WhWhWh
        geom_boxplot(varwidth = T) +
        scale_y_log10() + 
        xlab("Data Type") + 
        ylab("Temporal Extent of Study (years)") + 
        annotate("text", x = 3, y = 2600, 
                 label = "paste(\"Adjusted \", 
                 italic(R) ^ 2, \" = 0.52\")", 
                parse = TRUE) + 
  annotate("text", x = 3, y = 1200, 
           label = "paste(\"Overall model significance \", 
                 italic(p), \" = 0.002\")", 
           parse = TRUE) +
        theme_bw() + 
        scale_color_discrete(name = "Significantly Different from\nWhat, Where, When-only\nstudies (alpha = 0.1 level)") + 
        theme(axis.text.x = element_text(angle = 35, vjust = 0.9, hjust = 1)) 
)
```

```{r time_extent_results_plot_for_presentation_slide, eval = F, include = F}
## This code is to be run in the console for making plot to be exported as jpg for slides
## TODO: need to hand-annotate R2 and p-value in plot
t_size = 22 # plot text size
ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + #, color = sig_different_from_WhWhWh
  geom_boxplot(varwidth = T) +
  scale_y_log10() + 
  xlab("Data Type") + 
  ylab("Temporal Extent of Study (years)") + 
  # annotate("text", x = 3, y = 140, 
  #          label = "paste(\"Adjusted \", 
  #                italic(R) ^ 2, \" = 0.27\")", 
  #          parse = TRUE, 
  #          size = t_size - 15) + 
  annotate("text", x = 3, y = 1400, 
           label = "paste(\"Overall model significance \", 
                 italic(p), \" = 0.012\")", 
           parse = TRUE, 
           size = t_size - 14) +
  theme_bw() + 
  scale_color_discrete(name = "Significantly Different from\nWhat, Where, When-only\nstudies (alpha = 0.1 level)") + 
  theme(axis.text.x = element_text(angle = 330, vjust = 1, hjust = 0), 
        text = element_text(size = t_size)) 
```

## Analysis of Study Questions

### What is the most popular study question paradigm?
```{r paradigm_table_caption}
paradigm_table_caption <- tabs("paradigm_table_caption", 
                               paste0("The number of studies published between 2014 and 2018 addressing different broad types of questions using biological records data from Ireland or the UK.  The 'other' category includes methodology studies and other studies that did not focus specifically on questions about individual species or communities."))
```

```{r paradigm_plot_cap}
paradigm_plot_cap <- figs("paradigm_plot_cap", 
                          "The number of studies published between 2014 and 2018 addressing different broad types of questions using biological records data from Ireland or the UK.  The 'other' category includes methodology studies and other studies that did not focus specifically on questions about individual species or communities.  Points show the observed number of studies and vertical lines show 95% bootstrap confidence intervals.")
```

Coder agreement for variables in this analysis:
```{r vars_paradigm}
if(diag_present) {
  vs <- c("community.question", "individual.species.question")
  kable(prob_of_values[which(prob_of_values$variable %in% vs), 
                       c("variable", "kripp_alpha", "n_40_adequate")], 
        digits = 2)
}
if(diag_present) rm(vs)
```

The number of studies asking questions only about individual species (`r paradigm_list$individual_sp$ci_obj$t0` studies, 95% bootstrap CI [`r paradigm_list$individual_sp$ci_obj$bca[4]`, `r paradigm_list$individual_sp$ci_obj$bca[5]`]) was significantly higher than the number of studies asking questions only about communities (`r paradigm_list$community$ci_obj$t0` studies, 95% bootstrap CI [`r paradigm_list$community$ci_obj$bca[4]`, `r paradigm_list$community$ci_obj$bca[5]`]), about both individual species and communities (`r paradigm_list$both$ci_obj$t0` studies, 95% bootstrap CI [`r paradigm_list$both$ci_obj$bca[4]`, `r paradigm_list$both$ci_obj$bca[5]`]), or asking other types of questions (`r paradigm_list$other$ci_obj$t0` studies, 95% bootstrap CI [`r paradigm_list$other$ci_obj$bca[4]`, `r paradigm_list$other$ci_obj$bca[5]`]) (`r tabs("paradigm_table_caption", display = "cite")`, `r figs("paradigm_plot_cap", display = "cite")`). 


`r tabs("paradigm_table_caption", display = "full")`


```{r print_study_question_paradigm_table}
paradigm_count_df <- data.frame(matrix(nrow = length(paradigm_list), 
                                        ncol = 4))
colnames(paradigm_count_df) <- c("paradigm", "n_studies", "l_95ci", "u_95ci")

# fill paradigm counts data frame using the list of paradigm bootstrap results
for(i in 1:length(paradigm_list)) {
  paradigm_count_df[i, "paradigm"] <- names(paradigm_list)[i]
  paradigm_count_df[i, "n_studies"] <- paradigm_list[[i]]$ci_obj$t0
  paradigm_count_df[i, "l_95ci"] <- paradigm_list[[i]]$ci_obj$bca[4]
  paradigm_count_df[i, "u_95ci"] <- paradigm_list[[i]]$ci_obj$bca[5]
}

kable(paradigm_count_df, col.names = c("broad type of study question", 
                                       "number of studies", 
                                       "lower 95% bootstrap CI", 
                                       "upper 95% bootstrap CI"))
```


```{r plot_paradigm_counts, fig.cap=paradigm_plot_cap}
# plot number of studies in each paradigm, with 95% CIs
print(ggplot(data = paradigm_count_df, 
             aes(x = factor(paradigm, 
                            levels = c("individual_sp_only", 
                                       "community_only", 
                                       "both", 
                                       "other"), 
                            labels = c(
                              "individual species\nquestions only", 
                              "community questions only", 
                              "both individual species\nand community questions",
                              "other types\nof questions")), 
                 y = n_studies)) + 
        geom_point() + 
        geom_linerange(aes(x = factor(paradigm, 
                            levels = c("individual_sp_only", 
                                       "community_only",
                                       "both", 
                                       "other"), 
                            labels = c(
                              "individual species\nquestions only", 
                              "community questions only", 
                              "both individual species\nand community questions",
                              "other types\nof questions")), 
                           ymin = l_95ci, 
                           ymax = u_95ci)) + 
        xlab("Study Question Paradigm") + 
        ylab("Number of Studies") + 
        theme_bw() + 
        theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)))
```

**Dina - Do I need to do some sort of explicit test to see if the number of studies in each category is different, or is it enough to just say that the 95% bootstrap CI for the number of studies addressing only individual species questions does not overlap with the other categories?  I think I could do a Poisson regression with counts as the response and question types as predictors, fit a model with predictors and also an intercept only model, and then do a likelihood ratio test.  But bootstrapping seems simpler because I don't have to assess model fit or whether the data follow a Poisson distribution.**


### Ecological question
Reader agreement for variables used in this analysis.
```{r vars_study_question}
if(diag_present) {
  vs <- c("community.question", "individual.species.question", 
          "response.variable...species.richness", "response.variable...diversity", 
          "response.variable...distribution", "response.variable...abundance", 
          "response.variable...phenology") 
  kable(prob_of_values[which(prob_of_values$variable %in% vs), 
                             c("variable", "kripp_alpha", "n_40_adequate")], 
              digits = 2)
}
if(diag_present) rm(vs)
```

```{r resp_var_tab_cap}
resp_var_tab_cap <- tabs(name = "resp_var_tab_cap", 
                         "The number of studies (with lower and upper 95% bootstrap confidence intervals) using each type of response variable.")
```

```{r resp_var_plot_cap}
resp_var_plot_cap <- figs(name = "resp_var_plot_cap", 
                     "The number of studies analyzing different ecological response variables within the two broad study question paradigms.  Points show number of studies, vertical lines show 95% bootstrap confidence intervals.")
```

Of the studies asking community-based questions, the number of studies analyzing species richness (*alpha*-diversity) (`r resp_var_list$comm_sp_rich$boot_obj$t0` studies, 95% bootstrap CI [`r resp_var_list$comm_sp_rich$ci_obj$bca[4]`, `r resp_var_list$comm_sp_rich$ci_obj$bca[5]`]) was higher than the number of studies analyzing species turnover (*beta*-diversity) (`r resp_var_list$comm_diversity$boot_obj$t0` studies, 95% bootstrap CI [`r resp_var_list$comm_diversity$ci_obj$bca[4]`, `r resp_var_list$comm_diversity$ci_obj$bca[5]`]) or other response variables (`r resp_var_list$comm_other$boot_obj$t0` studies, 95% bootstrap CI [`r resp_var_list$comm_other$ci_obj$bca[4]`, `r resp_var_list$comm_other$ci_obj$bca[5]`]).  Of the studies asking individual species-based questions, the number of studies analyzing species distributions (`r resp_var_list$ind_dist$boot_obj$t0` studies, 95% bootstrap CI [`r resp_var_list$ind_dist$ci_obj$bca[4]`, `r resp_var_list$ind_dist$ci_obj$bca[5]`]) was not significantly higher than the number of studies analyzing phenology (`r resp_var_list$ind_phen$boot_obj$t0` studies, 95% bootstrap CI [`r resp_var_list$ind_phen$ci_obj$bca[4]`, `r resp_var_list$ind_phen$ci_obj$bca[5]`]) but was higher than the number of studies analyzing abundance (`r resp_var_list$ind_abund$boot_obj$t0` studies, 95% bootstrap CI [`r resp_var_list$ind_abund$ci_obj$bca[4]`, `r resp_var_list$ind_abund$ci_obj$bca[5]`]) and other response variables (`r resp_var_list$ind_other$boot_obj$t0` studies, 95% bootstrap CI [`r resp_var_list$ind_other$ci_obj$bca[4]`, `r resp_var_list$ind_other$ci_obj$bca[5]`]).

`r tabs("resp_var_tab_cap", display = "full")`

```{r print_response_variable_table}
resp_count_df <- data.frame(matrix(nrow = length(resp_var_list), 
                                        ncol = 4))
colnames(resp_count_df) <- c("response_variable", "n_studies", 
                             "l_95ci", "u_95ci")

# fill response variable counts data frame using the list of bootstrap results
for(i in 1:length(resp_var_list)) {
  resp_count_df[i, "response_variable"] <- names(resp_var_list)[i]
  resp_count_df[i, "n_studies"] <- resp_var_list[[i]]$ci_obj$t0
  resp_count_df[i, "l_95ci"] <- resp_var_list[[i]]$ci_obj$bca[4]
  resp_count_df[i, "u_95ci"] <- resp_var_list[[i]]$ci_obj$bca[5]
}

# add paradigm column
resp_count_df$paradigm <- resp_count_df$response_variable
resp_count_df$paradigm <- gsub(".*comm.*", "Community Questions", 
                               resp_count_df$paradigm)
resp_count_df$paradigm <- gsub(".*ind.*", "Individual Species Questions", 
                               resp_count_df$paradigm)
# re-order columns 
resp_count_df <- resp_count_df[, c(5, 1:4)]

# clean response variable strings
resp_count_df$response_variable <- gsub(".*sp_rich.*", "species richness", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*diversi.*", "diversity (turnover)", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*other.*", "other", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*dist.*", "species distribution", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*abund.*", "abundance", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*phen.*", "phenology", 
                                        resp_count_df$response_variable)

kable(resp_count_df, col.names = c("study question paradigm", 
                                   "response variable", 
                                   "number of studies", 
                                   "lower bound 95% bootstrap CI", 
                                   "upper bound 95% bootstrap CI"), 
      digits = 1)
```

```{r resp_var_by_paradigm_plot, fig.cap=resp_var_plot_cap}
# plot number of studies using each response variable, with 95% CIs
print(ggplot(data = resp_count_df, 
             aes(x = factor(response_variable, 
                            levels = c("species distribution", 
                                       "phenology", 
                                       "abundance",
                                       "species richness", 
                                       "diversity (turnover)", 
                                       "other")), 
                 y = n_studies)) + 
        geom_point() + 
        geom_linerange(aes(x = factor(response_variable, 
                            levels = c("species distribution", 
                                       "phenology", 
                                       "abundance",
                                       "species richness", 
                                       "diversity (turnover)", 
                                       "other")), 
                           ymin = l_95ci, 
                           ymax = u_95ci)) + 
        facet_wrap( ~ paradigm, scales = "free_x", strip.position = "bottom") + 
        xlab("") + 
        ylab("Number of Studies") + 
        theme_bw() + 
        theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1), 
              strip.background = element_blank(), strip.placement = "outside"))
      # theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)))
```

**[TODO: Descriptive stats for studies focusing on alien species?]**


### Development of methodology across different study questions
Reader agreement for variables used in this analysis.
```{r vars_methodology_focus}
if(diag_present) {
    vs <- c("methodology.development.or.analysis", 
            "response.variable...species.richness", 
            "response.variable...diversity", 
            "response.variable...distribution", 
            "response.variable...abundance", 
            "response.variable...phenology", "trends.over.time")
  kable(prob_of_values[which(prob_of_values$variable %in% vs), 
                       c("variable", "kripp_alpha", "n_40_adequate")], 
        digits = 2)
}
if(diag_present) rm(vs)
```

```{r method_study_focus_cap}
method_study_focus_cap <- figs(name = "method_study_focus_cap", 
                               "The number of studies that develop or test methods for analyzing biological records data for different ecological questions.")
```

The overall model predicting the probability of a study addressing a methodological question based on the ecological focus of the study [was/was not] significant (*p* = `r round(anova(method_mod, method_mod_null, test = "Chisq")[2, 5], 2)`).

In the final manuscript, I plan to report estimated probabilities of a study addressing a methodological question for different ecological focuses.  I will calculate the probability of a study focusing on methodology given its ecological focus using this equation: 

$\hat{probability_{methodology}} = {e^{intercept + \beta_{j}}} / {(1+e^{intercept + \beta_{j}})}$

Here's an example of how the results will look (assuming assumptions are met):
The estimated probability of a study addressing a methodological question when the ecological focus of the study is species distributions is `r round(exp(method_mod$coefficients[["(Intercept)"]] + method_mod$coefficients[["factor(response.variable...distribution)TRUE"]]) / (1 + exp(method_mod$coefficients[["(Intercept)"]] + method_mod$coefficients[["factor(response.variable...distribution)TRUE"]])), 2)`. The estimated probability of a study addressing a methodological question when the ecological focus of the study is species richness is `r round(exp(method_mod$coefficients[["(Intercept)"]] + method_mod$coefficients[["factor(response.variable...species.richness)TRUE"]]) / (1 + exp(method_mod$coefficients[["(Intercept)"]] + method_mod$coefficients[["factor(response.variable...species.richness)TRUE"]])), 2)`.

```{r print_method_focus_model, include = F}
anova(method_mod, method_mod_null, test = "Chisq")
```

**Dina & Jon, should I change my graph to match my statistical test (logistic regression)?  Should I be graphing the probability or proportion of methodology studies within each category, rather than graphing the counts?  Or should I change the statistical analysis so that instead of logistic regression analyzing the probability of a study having a methodological focus based on the ecological question, instead I analyze the number of methodological studies (either Poisson regression or bootstrap confidence intervals around the counts)?**  My goal to see whether there are any ecological questions for which there is a lot of (or very little) methodological work happening.  Analyzing the counts is a little different from analyzing proportions, but I can't figure out which is more relevant or informative.  Any thoughts you have would be welcome.

```{r plot_method_study_focus, fig.cap=method_study_focus_cap}
meth_df <- select(wg, title, methodology.development.or.analysis, 
                  response.variable...species.richness, 
                  response.variable...diversity, 
                  response.variable...distribution,
                  response.variable...abundance, 
                  response.variable...phenology, 
                  trends.over.time)
colnames(meth_df) <- c("title", "methodology", "species.richness", 
                       "diversity", "species.distribution", "abundance", "phenology", 
                       "temporal.trends")
meth_df <- gather(meth_df, species.richness:temporal.trends, 
                  key = focus, value = focus_val) %>%
  filter(focus_val == TRUE) %>%
  # filter(methodology == TRUE) %>%
  group_by(focus) %>%
  count(methodology) %>%
  ungroup() %>%
  complete(focus, methodology, fill = list(n = 0)) %>%
  filter(methodology == TRUE) %>%
  arrange(desc(n))
  
print(ggplot(aes(x = factor(focus, levels = meth_df$focus, 
                            labels = gsub("\\.", " ", meth_df$focus)),
                 y = n), 
             data = meth_df) + 
        geom_bar(stat = "identity") + 
        xlab("Ecological focus") + 
        ylab("Number of methodology studies") +
        theme_bw() + 
        theme(axis.text.x = element_text(angle = 35, hjust = 1, vjust = 1)))
rm(meth_df)
```


### What data types are used for each study question paradigm?
Agreement for variables in this analysis:
```{r vars_data_type_study_question}
if(diag_present) {
  vs <- c("community.question", "individual.species.question", 
        "data.type...what.where.when.only", 
        "data.structure...organized.data.collection.scheme", 
        "data.structure...sampling.effort.known", 
        "data.structure...non.detection", "data.type...abundance", 
        "data.type...visit.specific.covariates", 
        "data.structure...multiple.datasets.integrated.for.analysis", 
        "data.type...life.stage", 
        "data.type...voucher.of.some.kind.necessary.for.analysis") 
  kable(prob_of_values[which(prob_of_values$variable %in% vs), 
                       c("variable", "kripp_alpha", "n_40_adequate")], 
              digits = 2)
}
if(diag_present) rm(vs)
```

```{r data_type_parad_cap}
data_type_parad_cap <- figs(name = "data_type_parad_cap", 
                            "The number of studies using each type of biological records data within each of the broad study question paradigms.")
```

A Chi-squared likelihood ratio test comparing the saturated Poisson regression model to a reduced model with only main effects for data type and study paradigm (but no interaction term) did not reject the null hypothesis that the reduced model is as good as the full model (*p* = `r round(dt_pd_interaction_test$"Pr(>Chi)"[2], 2)`).  So, there is no evidence that the use of data types differs between the different main study question paradigms (`r figs("data_type_parad_cap", display = "cite")`). For all question paradigms, "what where when only" data was the most common type of data, and the use of multiple data sets was also common (`r figs("data_type_parad_cap", display = "cite")`).

```{r data_type_by_paradigm_barplot, fig.cap=data_type_parad_cap, fig.height=7, fig.width=7}
### plot data type by study question paradigm ----------------------
# make plot, order factor levels by most important a-priori
# dt_pd_df was created in the Methods section
ggplot(dt_pd_df, 
       aes(x = factor(data_type, 
                         levels = c("data.type...what.where.when.only", 
                           "data.structure...sampling.effort.known", 
                           "data.type...abundance", 
                           "data.structure...non.detection", 
                           "data.structure...organized.data.collection.scheme", 
                           "data.structure...multiple.datasets.integrated.for.analysis",
                           "data.type...life.stage", 
                           "data.type...visit.specific.covariates",
                           "data.type...voucher.of.some.kind.necessary.for.analysis"), 
                         labels = c("what where when only", 
                                    "sampling effort", 
                                    "abundance", 
                                    "non detection", 
                                    "organized scheme", 
                                    "multiple datasets", 
                                    "life stage", 
                                    "visit-specific covariates", 
                                    "voucher available")), 
           y = n, 
           fill = factor(data_type, 
                         levels = c("data.type...what.where.when.only", 
                           "data.structure...sampling.effort.known", 
                           "data.type...abundance", 
                           "data.structure...non.detection", 
                           "data.structure...organized.data.collection.scheme", 
                           "data.structure...multiple.datasets.integrated.for.analysis",
                           "data.type...life.stage", 
                           "data.type...visit.specific.covariates",
                           "data.type...voucher.of.some.kind.necessary.for.analysis"), 
                         labels = c("what where\nwhen only", 
                                    "sampling effort", 
                                    "abundance", 
                                    "non detection", 
                                    "organized\nscheme", 
                                    "multiple\ndatasets", 
                                    "life stage", 
                                    "visit-specific\ncovariates", 
                                    "voucher\navailable")))) + 
  geom_bar(position = position_dodge(preserve = "single"), stat = "identity") + 
    facet_wrap( ~ factor(paradigm, levels = c("community_question_only", 
                                            "individual_species_question_only",
                                            "both_ind_and_community_questions", 
                                            "other_paradigm"),
                       labels = c(
                         "community questions", "individual species\nquestions",
                         "both individual species\nand community\nquestions", 
                         "other")), 
              strip.position = "bottom") +
  ggtitle("Data use based on study question paradigm") + 
  scale_fill_discrete(name = "Data Type") + 
  xlab("Study question paradigm") + 
  ylab("Number of studies") + 
  scale_y_continuous(breaks = seq(0, max(dt_pd_df$n), 5)) + 
  theme_bw() + 
  theme(axis.text.x = element_blank(), 
        text = element_text(size = plot_text_size),
        strip.background = element_blank(), strip.placement = "outside")
```

```{r print_data_type_paradigm_interaction_test, include = F}
# testing null hypothesis that the reduced model is as good as the full model
# cannot reject that null. Therefore there is no evidence that the interaction
# term is significant.  So we can say that the use of data types does not
# depend on the study question paradigm.  
dt_pd_interaction_test
```


## Analysis approach & data type
Agreement for variables in this analysis:
```{r vars_data_type_analysis_approach}
if(diag_present) {
  vs <- c("results.type...inference", "results.type...prediction", 
          "results.type...descriptive.only", 
        "data.type...what.where.when.only", 
        "data.structure...organized.data.collection.scheme", 
        "data.structure...sampling.effort.known", 
        "data.structure...non.detection", "data.type...abundance", 
        "data.type...visit.specific.covariates", 
        "data.structure...multiple.datasets.integrated.for.analysis", 
        "data.type...life.stage", 
        "data.type...voucher.of.some.kind.necessary.for.analysis") 
  kable(prob_of_values[which(prob_of_values$variable %in% vs), 
                       c("variable", "kripp_alpha", "n_40_adequate")], 
              digits = 2)
}
if(diag_present) rm(vs)
```

```{r data_type_analysis_tab_cap}
data_type_analysis_tab_cap <- tabs(name = "data_type_analysis_tab_cap", 
                                   "Table of counts for data type by analysis approach.")
```

```{r data_type_analysis_cap}
data_type_analysis_cap <- figs(name = "data_type_analysis_cap", 
                            "The number of studies using different types of biological records data as part of different analysis approaches.  Analysis approaches are inference (e.g. *p*-values or confidence intervals), prediction (e.g. prediction of species presence or absence into unsampled areas, often assessed with a prediction performance measure like AUC, but no inference) and description (maps, graphs, narrative descriptions, or descriptive statistics with no inference or prediction).")
```

A Chi-squared likelihood ration test comparing the saturated Poisson regression model  to a reduced model with only main effects for analysis approach and data type did not reject the null hypothesis that the reduced model is as good as the full model (*p* = `r round(dt_an_mod_independence_test$'Pr(>Chi)'[2], 2)`), suggesting that the data type does not influence or constrain the analysis approach (`r figs("data_type_analysis_cap", display = "cite")`).  

`r if(diag_present) tabs("data_type_analysis_tab_cap", display = "full")`

```{r print_data_type_analysis_approach_contingency_table}
if(diag_present) {
    colnames(dt_an_df_wide) <- gsub(
    "results.type\\.\\.\\.", "", 
    colnames(dt_an_df_wide))
  colnames(dt_an_df_wide) <- gsub("\\.", " ", colnames(dt_an_df_wide))
  dt_an_df_wide$data_type <- gsub(
    "data\\.structure\\.\\.\\.|data\\.type\\.\\.\\.", "",  
    dt_an_df_wide$data_type)
  dt_an_df_wide$data_type <- gsub("\\.", " ", dt_an_df_wide$data_type)

  kable(dt_an_df_wide)
}
```

```{r data_type_analysis_approach_graph, fig.cap=data_type_analysis_cap}
### plot data type by analysis approach ----------------------
# make plot, order factor levels by most important a-priori
# dt_an_df was created in the Methods section
ggplot(dt_an_df, 
       aes(x = factor(data_type, 
                         levels = c("data.type...what.where.when.only", 
                           "data.structure...sampling.effort.known", 
                           "data.type...abundance", 
                           "data.structure...non.detection", 
                           "data.structure...organized.data.collection.scheme", 
                           "data.structure...multiple.datasets.integrated.for.analysis",
                           "data.type...life.stage", 
                           "data.type...visit.specific.covariates",
                           "data.type...voucher.of.some.kind.necessary.for.analysis"), 
                         labels = c("what where when only", 
                                    "sampling effort", 
                                    "abundance", 
                                    "non detection", 
                                    "organized scheme", 
                                    "multiple datasets", 
                                    "life stage", 
                                    "visit-specific covariates", 
                                    "voucher available")), 
           y = n, 
           fill = factor(data_type, 
                         levels = c("data.type...what.where.when.only", 
                           "data.structure...sampling.effort.known", 
                           "data.type...abundance", 
                           "data.structure...non.detection", 
                           "data.structure...organized.data.collection.scheme", 
                           "data.structure...multiple.datasets.integrated.for.analysis",
                           "data.type...life.stage", 
                           "data.type...visit.specific.covariates",
                           "data.type...voucher.of.some.kind.necessary.for.analysis"), 
                         labels = c("what where\nwhen only", 
                                    "sampling effort", 
                                    "abundance", 
                                    "non detection", 
                                    "organized\nscheme", 
                                    "multiple\ndatasets", 
                                    "life stage", 
                                    "visit-specific\ncovariates", 
                                    "voucher\navailable")))) + 
  geom_bar(position = position_dodge(preserve = "single"), stat = "identity") + 
    facet_wrap( ~ factor(analysis_approach, 
                         levels = c("results.type...inference", 
                                    "results.type...prediction",
                                    "results.type...descriptive.only"),
                       labels = c(
                         "inference", "prediction",
                         "description only")), 
              strip.position = "bottom") +
  ggtitle("Data used in different analysis approaches") + 
  scale_fill_discrete(name = "Data Type") + 
  xlab("Analysis Approach") + 
  ylab("Number of studies") + 
  scale_y_continuous(breaks = seq(0, max(dt_pd_df$n), 5)) + 
  theme_bw() + 
  theme(axis.text.x = element_blank(), 
        strip.background = element_blank(), strip.placement = "outside")
```

```{r data_type_analysis_apprach_likelihood_test_print, include = F}
if(diag_present) {
  print("Likelihood test comparing saturated model to model with no interaction: ")
  dt_an_mod_independence_test
}
```


## Authors and Data Providing Institutions

**Who choses to use biological records data?**

Coder agreement for variables in this analysis: 
```{r vars_author_institution}
if(diag_present) {
  vs <- c("author.associated.with.proximate.data.provider", 
          "institution.of.first.author", "institution.of.last.author", 
          "proximate.data.source") 
  kable(prob_of_values[which(prob_of_values$variable %in% vs), 
                       c("variable", "kripp_alpha", "n_40_adequate")],
        col.names = c("variable", "Krippendorff's alpha", 
                    "sample size adequate"),
              digits = 2)
}
if(diag_present) rm(vs)
```

```{r auth_inst_cap}
auth_inst_cap <- figs("auth_inst_cap", 
                      "The proportion of studies that had a lead author or any author associated with the institution that provided the biological records data.  Points and lines show point estimates with 95% bootstrap confidence intervals.")
```

*Note that reader agreement is quite low for some of the variables in this analysis, and so this analysis may need to be modified or abandoned in the final manuscript.*

```{r stop_scientific_notation}
options("scipen" = 100, "digits" = 4)
```

Lead authors (the first and/or last authors on a study) were significantly more likely to use biological records data provided by their own institution (empirical permutation *p*-value = `r p_auth_perm`).  Over a quarter of the studies (proportion = `r round(lead_auth_inst_boot$t0, 2)`, 95% bootstrap CI [`r round(boot.ci(lead_auth_inst_boot)$bca[4], 2)`, `r round(boot.ci(lead_auth_inst_boot)$bca[5], 2)`]) used data provided by the institution of one of the lead authors, and two-thirds of the studies (proportion = `r round(any_auth_inst_boot$t0, 2)`, 95% bootstrap CI [`r round(boot.ci(any_auth_inst_boot)$bca[4], 2)`, `r round(boot.ci(any_auth_inst_boot)$bca[5], 2)`]) had at least one author associated with the data providing institution (`r figs("auth_inst_cap", display = "cite")`).

```{r restore_scientific_notation}
options("scipen" = 0, "digits" = 7)
```

```{r author_institution_graph, fig.cap=auth_inst_cap}
# make df to use for drawing plot
auth_inst_df <- data.frame(matrix(nrow = 2, ncol = 4))
colnames(auth_inst_df) <- c("author_relationship", "point_estimate", 
                            "lower_bound", "upper_bound")
auth_inst_df[1, ] <- c("lead author\nassociated with\ndata provider", 
                       lead_auth_inst_boot$t0, 
                       boot.ci(lead_auth_inst_boot)$bca[4], 
                       boot.ci(lead_auth_inst_boot)$bca[5])
auth_inst_df[2, ] <- c("any author\nassociated with\ndata provider", 
                       any_auth_inst_boot$t0, 
                       boot.ci(any_auth_inst_boot)$bca[4], 
                       boot.ci(any_auth_inst_boot)$bca[5])
auth_inst_df$point_estimate <- as.numeric(auth_inst_df$point_estimate)
auth_inst_df$lower_bound <- as.numeric(auth_inst_df$lower_bound)
auth_inst_df$upper_bound <- as.numeric(auth_inst_df$upper_bound)
auth_inst_plot <- ggplot(data = auth_inst_df, aes(y = point_estimate, 
                         x = author_relationship)) + 
  geom_point() + 
  geom_linerange(aes(x = author_relationship, ymin = lower_bound, 
                     ymax = upper_bound)) +
  xlab(element_blank()) + 
  ylab("Proportion of studies") + 
  ylim(0, 1) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 25, hjust = 1, vjust = 1), 
        text = element_text(size = plot_text_size))

auth_inst_plot
rm(auth_inst_df, auth_inst_plot)
```


# Discussion

## Reader Agreement

The purpose of having a second researcher code a subset of articles was to address the question of whether my categories are clear enough that an independent researcher trying to do the same study would come up with the same conclusions. Poor agreement between the two readers for any given variable could be due to: 1) poor variable definitions or poor instructions about how to evaluate the variable; 2) true ambiguity in how a study should be coded; 3) insufficient knowledge or skill of the reader(s); 4) accidental error, perhaps due in part to skimming articles rather than reading them in their entirety.  

Consider adding a step after the agreement calculation where I summarize studies for which there was a lot of disagreement overall - ie disagreement on many categories.  These may be particularly perplexing, novel, or creative studies.  

One option to avoid ambiguity in coding is only coding and analyzing variables for which coding can be done based entirely or largely on character string matching.  This would provide greater reliability, but at the cost of less interesting, informative, and insightful analysis (Krippendorff 2004, Section 11.1).

The goal of having two readers is to estimate how replicatable the analysis is.  If another researcher could code the articles in the same way I do, then it provides confidence that the analysis is revealing some sort of true characteristics of the literature.  

## Who uses biological records?
While the number of studies that used data provided by the instituion of one of the lead authors was higher than expected by chance, the majority of studies did not have a lead author associated with the data providing institution.  If the lead authors are primarily the people who concieve of and design a study, then these results suggest that, to answer their research questions, researchers frequently use data that they are not directly involved in curating, perhaps because they do not already have the data necessary to answer their question.  This suggests that data providing institutions perform an important data sharing role, and the majority of research that uses biological records data could not have been done solely using the data resources of the lead authors.  

Most studies in this review (`r any_auth_inst_boot$t0`) did have at least one author associated with the data providing institution.  Co-authorship may be a prefered way for data providers to recieve credit for their role in the research pipeline, especially when the data form a "non-trivial" contribution to the study (@Roche2014, @Whitlock2011).  This could reflect [**do I have some reference about the messy nature of bio recs?**].

While standards for citing datasets are developing [**need citations**], there still seems to be some uncertainty about when and how to cite data resources [**I have a citation for this, right?**] which may motivate data providers to seek recognition through co-authorship.  


### What data types are used for each study question paradigm?
I think the story is that data use is roughly similar between question paradigms.  Integrating multiple datasets for analyses is common in both question paradigms, and so it is worth making an effort to ensure that published datasets are in standardized, compatible formats.  


## Assorted bits to add
[[@Follett2015] found that methodology was focus of 17% of studies in a systematic review of cit. sci. use in publications. ]

[[@Follett2015] found that "the public preferred to publish their outcomes in" non peer reviewed places.  This is part of why I include grey literature. ] 

[[@Pearce-Higgins2018] Some data collection / providing schemes have a business model of getting funding to do research.  This might be particularly important for designed monitoring schemes, because the study authors can influence data collection methods.  ]

[[@Pearce-Higgins2018] warn of risks of free and open data being used without data providers knowledge and input.  Risks include inappropriate analysis or interpretation.  "Although not commonly regarded as a major issue in other disciplines where PDA is the norm, the complexities of ecological data can make archiving for independent use particularly challenging (Kenall et al. 2014, Mills et al. 2015)" [@Pearce-Higgins2018]. ]

[In a review of ecological and environmental citizen science projects, Pocock et al. [-@Pocock2017] distinguished between 'mass participation' projects that tended to have lower data quality, less detailed data, but cover larger geographic areas, and 'systematic monitoring' projects that tended to collect more complex and possible higher-quality data (e.g. counts instead of presence/absence observations) but covered smaller geographic areas.]



### old questions to drop or keep?
**sub-question about analysis approach and data type**
$H2_a$: The data types "non-detection", "survey effort known", "abundance", and "organized monitoring scheme" each increase the probability of studies using a model-based analysis (either inferential or predictive) rather than a descriptive analysis.

$H2_0$: The data types "non-detection", "survey effort known", "abundance", and "organized monitoring scheme" do not affect the probability of studies using a model-based analysis (either inferential or predictive) rather than a descriptive analysis.

*Proposed Test*: ~~Logistic Regression with probability of statistical / model-based results as the outcome and data types as predictors. For first hypothesis, fit the full model and the model with only the intercept (what where when data only) and test for significance of all data types.  Then, do this individually removing each data type to get estimate of significance for each data type. (4 regressions total)  Full model:~~

$logit(P_{model}) = \beta_0 + \beta_1 * non detection + \beta_2 * survey effort + \beta_3 * abundance$


**Another sub-question - Does data with more structure tend to be analyzed in-house more frequently than data w/less structure?  See @Pearce-Higgins2018**
**Not sure I should include this - I don't know if I can actually interpret it well.**
$H_a$: The proportion of studies for which a lead author is associated with the data providing institution is higher for studies using organized monitoring scheme data than for studies using data that is not from an organized monitoring scheme.  

$H_0$: The proportion of studies for which a lead author is associated with the data provider is not different for studies using data from an organized monitoring scheme and studies not using data from an organized monitoring scheme. 


# Appendix 1
[ Attach pdf of Instructions for Coders ]


# References






</div>