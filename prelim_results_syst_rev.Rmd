---
title: "Biological Records Systematic Review - Preliminary Results"
output:
  html_document: default
  html_notebook: default
  word_document: default
---


```{r, echo=F}
knitr::opts_chunk$set(echo=F, message=F, warning=T, cache=F)
```

```{r, load_data, cache = F, results='hide', warning = F}
setwd("~/Documents/Data_Analysis/UCD/systematic_review/")
library(wgutil)
library(Hmisc)
library(irr)
library(captioner)
library(knitr)
library(pander)
library(ggridges)
library(bestglm)
library(car)
library(GGally)
library(tidyverse)

diag <- F # produce exploratory & diagnostics plots & results
diag_present <- T # produce diagnostics for report to Dina

if(diag == F) { # turn off warnings when not running diagnostics
  knitr::opts_chunk$set(echo=F, message=F, warning=F, cache=F)
}

source("./clean_data_syst_rev.R")
```

```{r calc_temp_extent}
# calculate number of years covered by study
rev$temp_extent <- as.numeric(rev$end.year) - as.numeric(rev$start.year)
# individually assign values to studies with temporal extent < 1 year
if(diag) rev$title[rev$temp_extent == 0] 
rev$temp_extent[which(
  grepl("An assessment of bumblebee .* land use and floral.*", 
        rev$title))] <- 0.33
```

```{r voucher_data_to_whwhwh, include = F, eval = F}
## This chunk could be turned on if I decide that:
## For the purposes of analysis, if there was a specimen available but the 
# specimen was not used at all for analysis, then, if there are no other richer
# data types, I will consider the data what, where, when-only.  I think this
# best represents the use of physical.specimen data.  I do not think this would make sense
# for other data types (e.g. if there is abundance data but the analysis only
# uses it as what, where, when) because that is a choice to strip usually 
# meaningful information.  The physical specimen is likely not meaningful for
# many analyses that use physical.specimens as a source of wh, wh, wh data.

## But as of 23 August 2018 I think I will keep the physical.specimen data as "voucher
## available" and not transform the whwhwh column.  

print("Modifying what, where, when data type to TRUE if voucer specimen was not used.")
for(i in 1:nrow(rev)) {
  if(rev$data.type...physical.specimen[i] == T | rev$data.type...photo[i] == T | 
     rev$data.type...audio[i] == T | rev$data.type...video[i] == T) {
    # If a voucher is available
    if(rev$data.type...abundance[i] == F & 
       rev$data.type...sampling.effort.reported[i] == F & 
       rev$data.type...organized.data.collection.scheme[i] == F & 
       rev$data.type...visit.specific.covariates[i] == F & 
       rev$data.type...life.stage[i] == F) {
      # If there are no other rich data types
      if(rev$voucher.of.some.kind.necessary.for.analysis[i] == F) {
        # If the voucer was not used
        rev$data.type...what.where.when.only[i] <- TRUE
      }
    }
  }
} 
```

```{r numberingPrep}
# make functions for adding numbered captions to figures 
figs <- captioner(prefix = "Fig.") 
tabs <- captioner(prefix = "Table")
```

# Preliminary structure of analysis and results
This document states every question I plan to ask and shows methods, figures, tables and results text that I expect to use to answer each question in the biological records systematic review. Specific results and interpretations will change when I code the rest of the articles.

# Methods
We did a systematic review of original research published since 2014 that used biological records from Ireland and the UK.  We generated a list of relevant articles from Web of Science, Scopus, ProQuest, GoogleScholar, and the Global Biodiversity Information Facility (GBIF) website.  I evaluated each article for inclusion eligibility and coded information on [**??**] characteristics for each eligible article.  This coding was validated for a subset of articles by a second reader.  Agreement between the two readers was evaluated using Krippendorff's alpha (Krippendorff 2004).

## Study eligibility
Studies were eligible for inclusion if they met all of the following criteria:

1) original research
2) English language
3) used opportunistic biological data collected with non-standardized or semi-standardized protocols
4) included (but were not necessarily limited to) data from Ireland or the UK
5) the full text of the study was available through the UCD library online platform, Google, GoogleScholar, or ResearchGate.  
6) performs at least one analysis of the data (data papers and biotic atlases were not eligible)
7) sample size greater than 20
8) not restricted to fossil records

Grey literature was included and therefore peer review was not required.  Studies using semi-standardized data collection schemes (e.g. UK Butterfly Monitoring Scheme) were included as long as they included some opportunistic elements (e.g. locations chosen opportunistically by volunteers). Studies for which all data was collected by the study authors were excluded.  

## Article coding
### Data Type
We coded twelve variables describing aspects of data type: what, where, when only; sampling effort reported; abundance; detection / non-detection; organized monitoring scheme; visit-specific covariates; multiple datasets integrated for analysis; life stage information; physical specimen; photo; audio; video. Data types are not mutually exclusive except fot the "what where when only" data type, which cannot be true if any other data type is true.  We considered "what, where, when" as the baseline data type and considered all other data types supplementary additions to that basic data type.

~~For statistical analyses, we used only the sampling effort reported, abundance, and detection / non-detection data type variables based on *a-priori* expectations about their expected benefit in enabling inferential and predictive modelling and their expected cost in terms of complexity in collecting, recording, and storing the data.~~

For most analyses, we grouped the data types "physical specimen", "photo", "audio", and "video" into a "voucher specimen" group, but we coded them individually for in order to identify emerging trends in how vouchers are collected. 

For statistical analyses, we kept data type variables in models based on *a-priori* expectations about the variables' influence on data analysis strategy and based on expectations about different levels of complexity and effort involved in collecting, recording, and storing the data.  The data type variables we kept in models were: what, where, when only; sampling effort reported; abundance; detection / non-detection; organized monitoring scheme; visit-specific covariates; multiple datasets integrated for analysis; life stage information; voucher specimen.  

----------------------------

### Agreement between two readers (TODO)
**Dina**, do you have any comments about Krippendorff's alpha or other ways to quantify the agreement between two readers in how they code studies?

I used Krippendorf's *alpha* to evaluate the agreement between two readers for each of `r ncol(cp)-1` variables that had been coded by two people.  [**add equation of krippendorff's alpha for Dina to look at?**]. For each case of disagreement between the two readers, I reviewed the article.  When the disagreement seemed to be due to an oversight or an obvious error (e.g. when one reader coded "results type - inference" as FALSE but the article reported 95% confidence intervals around estimates), I corrected the erroneous value.  In cases where the disagreement was not the result of an obvious oversight, I did not change either value.  I calculated Krippendorff's alpha to assess agreement both before and after I corrected obviously erroneous codings.  


```{r inter_coder_agreement_subset_dfs}
## casey's pre-calibration results ------------------------------------------
# remove columns that aren't coded variables
cp_pre_cal <- cp_pre_cal[, which(colnames(cp_pre_cal) %nin% 
                                   c("link", "qualifies", "authors", 
                                     "publication", "doi", "year", 
                                     "coding.DONE"))]
# rename the old column names used in casey's 1st coding
colnames(cp_pre_cal)[grep(".*known.effort", colnames(cp_pre_cal))] <- 
  "data.type...sampling.effort.reported"
colnames(cp_pre_cal)[grep(".*semi.structured.*", colnames(cp_pre_cal))] <- 
  "data.type...organized.data.collection.scheme"

cp_pre_cal <- cp_pre_cal[order(cp_pre_cal$title), ] # order rows
cp_pre_cal <- cp_pre_cal[, order(colnames(cp_pre_cal))] # order columns
# put title column first
cp_pre_cal <- cp_pre_cal[, c(which(colnames(cp_pre_cal) == "title"), 
                             which(colnames(cp_pre_cal) != "title"))]
## end casey's pre-calibration results ---------------------------------------

## casey's post-calibration results before error correction ----------------------
# remove columns that aren't coded variables from casey's df
cp_post_cal <- cp_orig[, which(colnames(cp_orig) %nin% 
                             c("link", "qualifies", "authors", "publication", 
                               "doi", "year", "coding.DONE"))]
cp_post_cal <- cp_post_cal[order(cp_post_cal$title), ] # order rows
cp_post_cal <- cp_post_cal[, order(colnames(cp_post_cal))] # order columns
# put title column first
cp_post_cal <- cp_post_cal[, c(which(colnames(cp_post_cal) == "title"), 
                       which(colnames(cp_post_cal) != "title"))]
## end casey's post-calibration results ------------------------------------

## all casey's results before error correction --------------------------------
# remove columns that aren't coded variables from casey's df
cp_orig <- cp_orig[, which(colnames(cp_orig) %nin% 
                             c("link", "qualifies", "authors", "publication", 
                               "doi", "year", "coding.DONE"))]
cp_orig <- cp_orig[order(cp_orig$title), ] # order rows
cp_orig <- cp_orig[, order(colnames(cp_orig))] # order columns
# put title column first
cp_orig <- cp_orig[, c(which(colnames(cp_orig) == "title"), 
                       which(colnames(cp_orig) != "title"))]

## end casey's results before error correction --------------------------------

## my results before correction  ------------------------------------------------
# subset my coded results to titles and columns coded by casey
mult_coded_rev_orig <- rev_orig[which(rev_orig$title %in% cp_orig$title), 
                      which(colnames(rev_orig) %in% colnames(cp_orig))]
# order rows
mult_coded_rev_orig <- mult_coded_rev_orig[order(mult_coded_rev_orig$title), ] 
# order cols
mult_coded_rev_orig <- mult_coded_rev_orig[, order(colnames(mult_coded_rev_orig))] 
mult_coded_rev_orig <- mult_coded_rev_orig[, c(
  which(colnames(mult_coded_rev_orig) == "title"), 
  which(colnames(mult_coded_rev_orig) != "title"))] # put title column first
### end my results before correciton -----------------------------------------

## casey's results after error correction -----------------------------------
# remove columns that aren't coded variables 
cp <- cp[, which(colnames(cp) %nin% 
                             c("link", "qualifies", "authors", "publication", 
                               "doi", "year", "coding.DONE"))]
cp <- cp[order(cp$title), ] # order rows
cp <- cp[, order(colnames(cp))] # order columns
# put title column first
cp <- cp[, c(which(colnames(cp) == "title"), 
                       which(colnames(cp) != "title"))]
## end casey's results after error correction ---------------------------------

## my results after error correction ----------------------------------------
# subset my coded results to titles and columns coded by casey
mult_coded_rev <- rev[which(rev$title %in% cp$title), 
                      which(colnames(rev) %in% colnames(cp))]
# order rows
mult_coded_rev <- mult_coded_rev[order(mult_coded_rev$title), ] 
# order cols
mult_coded_rev <- mult_coded_rev[, order(colnames(mult_coded_rev))] 
mult_coded_rev <- mult_coded_rev[, c(
  which(colnames(mult_coded_rev) == "title"), 
  which(colnames(mult_coded_rev) != "title"))] # put title column first
## end my results after error correction --------------------------------------


## make sure all titles are present
if(any(cp_pre_cal$title %nin% mult_coded_rev_orig$title)) {
  warning("Some titles in cp_pre_cal are not in mult_coded_rev_orig. ")
  cp_pre_cal <- cp_pre_cal[which(cp_pre_cal$title %in% 
                                   mult_coded_rev_orig$title), ]
}
if(any(cp_orig$title %nin% mult_coded_rev_orig$title)) {
  warning("Some titles in cp_orig are not in mult_coded_rev_orig. ")
  cp_orig <- cp_orig[which(cp_orig$title %in% mult_coded_rev_orig$title), ]
}
if(any(mult_coded_rev_orig$title %nin% cp_orig$title)) {
  warning("Some titles in mult_coded_rev_orig are not in cp_orig. ")
  mult_coded_rev_orig <- mult_coded_rev_orig[which(mult_coded_rev_orig$title %in%
                                           cp_orig$title), ]
}
if(any(mult_coded_rev$title %nin% cp$title) | 
   any(cp$title %nin% mult_coded_rev$title)) {
  warning("cp and mult_coded_rev don't have the same titles. ")
  mult_coded_rev <- mult_coded_rev[which(mult_coded_rev$title %in% cp$title), ]
  cp <- cp[which(cp$title %in% mult_coded_rev$title), ]
}

## make sure df rows and cols are in the same order
if(!identical(cp_orig$title, mult_coded_rev_orig$title)) {
  stop("Study titles in cp_orig and mult_coded_rev_orig are either not the same or are not in the same order.  They must be in the same order for creating the variable data frames that will be used to calculate Krippendorf's alpha. ")
}
if(!identical(colnames(cp_orig), colnames(mult_coded_rev_orig))) {
  stop("Columns in cp_orig and mult_coded_rev_orig must be identical and in the same order. ")
}
if(!identical(colnames(cp_pre_cal), colnames(mult_coded_rev_orig))) {
  stop("Columns in cp_pre_cal and mult_coded_rev_orig must be identical and in the same order. ")
}
if(!identical(colnames(cp), colnames(mult_coded_rev))) {
  stop("Columns in cp and mult_coded_rev must be identical and in the same order. ")
}
```

```{r define_calibration_set}
calibration_set <- cp_pre_cal$title
```

```{r make_dfs_of_pre_calibration_double_coded_articles}
pre_cal_list <- list()

rev_pre_cal <- mult_coded_rev_orig[which(mult_coded_rev_orig$title %in% calibration_set), ]

if(colnames(cp_pre_cal)[1] != "title") {stop("The first column of cp_pre_cal and mult_coded_rev_orig must be the study title.")}

for(i in 2:ncol(cp_pre_cal)) { # awkward subsetting is b/c of tibble format
  pre_cal_list[[i-1]] <- data.frame(matrix(data = c(cp_pre_cal[, i][[1]],
                                                 rev_pre_cal[, i][[1]]),
                                        nrow = 2, ncol = nrow(cp_pre_cal), 
                                       byrow = T))
}
names(pre_cal_list) <- colnames(cp_pre_cal)[2:ncol(cp_pre_cal)]
```

```{r make_dfs_of_post_calibration_double_coded_articles}
post_cal_list <- list()

cp_post_cal <- cp_orig[which(cp_orig$title %nin% calibration_set), ]
rev_post_cal <- mult_coded_rev_orig[which(mult_coded_rev_orig$title %nin% calibration_set), ]

if(colnames(cp_post_cal)[1] != "title") {stop("The first column of cp_post_cal and mult_coded_rev_orig must be the study title.")}

for(i in 2:ncol(cp_post_cal)) { # awkward subsetting is b/c of tibble format
  post_cal_list[[i-1]] <- data.frame(matrix(data = c(cp_post_cal[, i][[1]], 
                                                     rev_post_cal[, i][[1]]),
                                            nrow = 2, ncol = nrow(cp_post_cal), 
                                            byrow = T))
}
names(post_cal_list) <- colnames(cp_post_cal)[2:ncol(cp_post_cal)]
```

```{r make_dfs_of_double_coded_variables_all_studies_pre_correction}
## make a data frame for each double-coded variable
# each row is codings from one person
# each column is a study
pre_correct_list <- list()

if(colnames(cp_orig)[1] != "title") {stop("The first column of cp_orig and mult_coded_rev_orig must be the study title.")}

for(i in 2:ncol(cp_orig)) { # awkward subsetting is b/c of tibble format
  pre_correct_list[[i-1]] <- data.frame(matrix(data = c(cp_orig[, i][[1]], 
                                                 mult_coded_rev_orig[, i][[1]]),
                                        nrow = 2, ncol = nrow(cp_orig), byrow = T))
}
names(pre_correct_list) <- colnames(cp_orig)[2:ncol(cp_orig)]
```

```{r make_dfs_of_double_coded_variables_all_studies_post_correction}
# each row is codings from one person
# each column is a study
post_correct_list <- list()
if(colnames(cp)[1] != "title" | colnames(mult_coded_rev)[1] != "title") {
  stop("The first column of cp and mult_coded_rev must be the study title")}

for(i in 2:ncol(cp)) {
  post_correct_list[[i-1]] <- data.frame(matrix(
    data = c(cp[, i][[1]], mult_coded_rev[, i][[1]]), 
    nrow = 2, ncol = nrow(cp), byrow = T))
}
names(post_correct_list) <- colnames(cp)[2:ncol(cp)]
```

```{r krippendorfs_alpha_calc, warning=FALSE, message=FALSE}
do_kripp.alpha <- function(x, method) {
  # function to call kripp.alpha while converting dataframe x to a matrix
  # ARGS: x - data frame in which rows are observers and columns are studies
  #       method - character string of method to pass to kripp.alpha
  kripp.alpha(as.matrix(x), method = method)
}

kripp_all_pre_correct <- lapply(pre_correct_list, FUN = do_kripp.alpha, 
                               method = "nominal")
kripp_all_post_correct <- lapply(post_correct_list, FUN = do_kripp.alpha, 
                                 method = "nominal")
kripp_pre_cal <- lapply(pre_cal_list, FUN = do_kripp.alpha, method = "nominal")
kripp_post_cal <- lapply(post_cal_list, FUN = do_kripp.alpha, 
                         method = "nominal")
```

```{r calc_kripp_wg_before_and_after_correction, include = F, eval = F}
## This is a bit dumb, as I didn't do a 3rd coding for all articles, so many of
## the coded values are carried directly over from the 1st coding and are thus
## very correlated.  I don't think this is a very worthwhile way to look at 
## things, and I should probably abandon it.
# calculate agreement between my codings before and after I corrected them based
# on CP's codings.  This will perhaps show how reliable my original codings are.
# The assumption would be that if my original codings are reliable compared to
# my double-checking when prompted, then we can at least have confidence that
# my coding is consistent and I'm not making too many sloppy errors.
wg_comparison_list <- list()

if(colnames(mult_coded_rev)[1] != "title" | colnames(mult_coded_rev_orig)[1] != "title") {
  stop("The first columns of mult_coded_rev_orig and mult_coded_rev must be the study title.")}
if(nrow(mult_coded_rev) != nrow(mult_coded_rev_orig)) {
  warning("There are different numbers of rows in mult_coded_rev and mult_coded_rev_orig. This may be due to studies that were disqualified in the data cleaning script after review because of differing reader scorings.")
  mult_coded_rev_orig <- mult_coded_rev_orig[which(mult_coded_rev_orig$title %in%
                                                     mult_coded_rev$title), ]
  mult_coded_rev <- mult_coded_rev[which(mult_coded_rev$title %in%
                                           mult_coded_rev_orig$title), ]
}

for(i in 2:ncol(mult_coded_rev)) {
  wg_comparison_list[[i-1]] <- data.frame(matrix(
    data = c(mult_coded_rev_orig[, i][[1]], mult_coded_rev[, i][[1]]), 
    nrow = 2, ncol = max(nrow(mult_coded_rev), nrow(mult_coded_rev_orig)), 
    byrow = T))
}
names(wg_comparison_list) <- colnames(cp)[2:ncol(cp)]

## calculate Krippendorff's alpha
kripp_wg_comparison <- lapply(wg_comparison_list, FUN = do_kripp.alpha, 
                              method = "nominal")
```





--------------------------

## Temporal Extent

$H_a$: The mean temporal extent of studies using only what, where, when data is longer than the mean temporal extent of studies using richer data types. 

$H_0$: The mean temporal extent of studies using what, where, when-only data is the same as the mean temporal extent of studies using richer data types. 

*Proposed Test*: Linear regression with natural-log transformed time (in years) as the outcome and data types as predictors.

We used linear regression with natural-log transformed time (in years) as the response variable.  We used nine binary predictor variables indicating whether the data had the following information: 1) only what where when data, 2) sampling effort, 3) abundance, 4) detection / non-detection, 5) organized monitoring scheme, 6) visit-specific covariates, 7) multiple different datasets, 8) life stage information, and 9) voucher specimens.  While the predictor variables are correlated, we kept all variables in the multiple regression model because we are interested in the effect of each additional type of information *after accounting for other information types* (e.g. is there an additional effect of having abundance data after adjusting for whether the data came from an organized monitoring scheme).

Note that because the variables are not mutually exclusive these are each individual variables - these are not dummy variables representing a single categorical factor level "data type" variable.  

The intercept-only model in this analysis is the mean temporal extent of all studies.  


```{r time_assess_correlation_predictors, fig.height=8, fig.width = 8}
### assess correlation between data type predictors ----------------------
dt_df <- rev[, which(
  colnames(rev) %in% 
    c("data.type...what.where.when.only", 
      "data.type...sampling.effort.reported", 
      "data.type...abundance", 
      "data.type...detection...non.detection", 
      "data.type...organized.data.collection.scheme", 
      "data.type...visit.specific.covariates", 
      "data.type...multiple.datasets.integrated.for.analysis", 
      "data.type...life.stage", 
      "data.type...voucher.available"))]
for(i in 1:ncol(dt_df)) {
  dt_df[, i] <- as.factor(as.character(data.frame(dt_df)[, i]))
}

print(ggpairs(dt_df, columns = 1:ncol(dt_df), 
              upper = list(discrete = "ratio"),
              lower = list(discrete = "facetbar"), 
              diag = list(discrete = "barDiag"), 
              columnLabels = c("what where when only", 
                               "sampling effort",
                               "abundance",
                               "non detection", 
                               "organized scheme", 
                               "visit specific covariates", 
                               "multiple datasets", 
                               "life stage", 
                               "voucher available"), 
              labeller = label_wrap_gen(width = 10)) + 
        ggtitle("Correlation of data type predictor variables") + 
        theme(axis.text.x = element_text(angle = 50, hjust = 1)))
  
```


```{r prepare_temporal_extent_df}
temp_extent <- select(rev, 
                      c(title, temp_extent, 
                        data.type...what.where.when.only, 
                        data.type...sampling.effort.reported, 
                        data.type...abundance,
                        data.type...detection...non.detection,
                        data.type...organized.data.collection.scheme, 
                        data.type...visit.specific.covariates, 
                        data.type...multiple.datasets.integrated.for.analysis,
                        data.type...life.stage, 
                        data.type...voucher.available)) %>%
  gather(key = "data_type", value = "dt_value", 
         data.type...what.where.when.only:data.type...voucher.available, 
         factor_key = T) %>%
  filter(dt_value == T) %>%
  group_by(data_type)

# make names prettier
temp_extent$data_type <- gsub("data.type...", "", temp_extent$data_type)

temp_extent$data_type <- factor(as.character(temp_extent$data_type), 
                            levels = c("what.where.when.only", 
                                       "sampling.effort.reported",
                                       "abundance",
                                       "detection...non.detection", 
                                       "organized.data.collection.scheme", 
                                       "visit.specific.covariates", 
                                       "multiple.datasets.integrated.for.analysis", 
                                       "life.stage", 
                                       "voucher.available"), 
                            labels = c("what where when only", 
                                       "sampling effort reported",
                                       "abundance",
                                       "non detection", 
                                       "organized scheme", 
                                       "visit specific covariates", 
                                       "multiple datasets", 
                                       "life stage", 
                                       "voucher available"))
```

Evaluate normality of raw data:
```{r temp_range_distribution}
if(diag_present) {
  # distribution of all data
 # hist(rev$temp_extent)
 # hist(rev$temp_extent[which(rev$temp_extent <= quantile(rev$temp_extent, 
 #                                                        0.90, na.rm = T))])
  ## data are not balanced
  print(table(temp_extent$data_type))
  print("Data are not balanced.")
  
  ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
    geom_boxplot() +
    theme_bw() + 
    ggtitle("All data") + 
    theme(axis.text.x = element_text(angle = 330, vjust = 0.3))
  
  ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
    ylim(c(0, 800)) + 
    geom_boxplot() +
    theme_bw() + 
    ggtitle("Temporal Duration after\ncutting off the 8000 year study") + 
    theme(axis.text.x = element_text(angle = 330, vjust = 0.3))
  
  ggplot(data = temp_extent, aes(x = temp_extent, y = data_type)) + 
    xlim(c(0, 800)) + 
    geom_density_ridges() +
    ggtitle("Temporal Duration after\ncutting off the 8000 year study") + 
    theme_bw()
  
  ## normality -------------------------------------
  # data very non-normal (above) but log transformation kinda fixes this
  # (reciprocal and sqrt transformations did not help)
  print(ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
          geom_boxplot(varwidth = T) +
          scale_y_log10() + 
          theme_bw() + 
          ggtitle("log transformation of year\nmakes normality better") + 
          ylab("Temporal extent (years)") + 
          theme(axis.text.x = element_text(angle = 40, vjust = 0.9, 
                                           hjust = 1)))
  
  print(ggplot(data = temp_extent, aes(x = temp_extent, y = data_type)) + 
          scale_x_log10() + 
          geom_density_ridges(stat = "binline", alpha = 0.4) +
          geom_density_ridges(alpha = 0.4) + 
          xlab("Temporal extent (years)") + 
          theme_bw())
  
  if(diag) {
    for(i in unique(temp_extent$data_type)) { 
      # distribution of log(years) by data type
      hist(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
      qqnorm(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
      qqline(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
    }
  }
}
```

Fit full model and evaluate residuals for assumptions of linear regression.
```{r fit_temp_extent_lm}
## fit linear regression with log-transformed years as response variable
## and each data type as a separate variable 
# fit full model
rev$log_years <- log(rev$temp_extent)
# variables added based on a-priori expectation of most to least important
time_lm_full <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.type...sampling.effort.reported + 
                     data.type...abundance +
                     data.type...detection...non.detection + 
                     data.type...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.type...multiple.datasets.integrated.for.analysis + 
                     data.type...life.stage + 
                     data.type...voucher.available,
                   data = rev) 
time_resids <- rstandard(time_lm_full) # calculate residuals
if(diag_present) {
  print("Diagnostics for full model log_years ~ all 9 data types")
  plot(time_lm_full)
  hist(time_resids, 
       main = "standardized residuals of full model for temporal extent", 
       xlab = "standardized residuals")
  boxplot(time_resids, ylab = "standardized residuals")
}
time_summary <- summary(time_lm_full)
if(diag) time_summary

## to interpret outcome on original scale:
# for a one-unit increase in x, the predicted value of y 
# changes by (e^beta1 - 1)*100 percent
```

Fitted vs. residuals plot shows some fanning.  

```{r time_data_type_full_model_sig_test}
# Test significance of full model
m0 <- lm(log_years ~ 1, data = rev) # fit null model 
m_time_full_anova <- anova(m0, time_lm_full)
```

```{r time_data_type_test_main_effects, include = F}
## test effect of each data type predictor by comparing the full model to a
# model with that predictor removed
# 
# TODO: Does this need multiple comparison correction?

m_no_whwhwh <- lm(log_years ~ 1 + 
                    data.type...sampling.effort.reported + 
                    data.type...abundance +
                    data.type...detection...non.detection + 
                    data.type...organized.data.collection.scheme + 
                    data.type...visit.specific.covariates + 
                    data.type...multiple.datasets.integrated.for.analysis + 
                    data.type...life.stage + 
                    data.type...voucher.available,
                  data = rev)
m_no_effort <- lm(log_years ~ 1 + 
                    data.type...what.where.when.only + 
                    data.type...abundance +
                    data.type...detection...non.detection + 
                    data.type...organized.data.collection.scheme + 
                    data.type...visit.specific.covariates + 
                    data.type...multiple.datasets.integrated.for.analysis + 
                    data.type...life.stage + 
                    data.type...voucher.available,
                  data = rev)
m_no_abund <- lm(log_years ~ 1 + 
                   data.type...what.where.when.only + 
                   data.type...sampling.effort.reported + 
                   data.type...detection...non.detection + 
                   data.type...organized.data.collection.scheme + 
                   data.type...visit.specific.covariates + 
                   data.type...multiple.datasets.integrated.for.analysis + 
                   data.type...life.stage + 
                   data.type...voucher.available,
                 data = rev)
m_no_nonDet <- lm(log_years ~ 1 + 
                    data.type...what.where.when.only + 
                    data.type...sampling.effort.reported + 
                    data.type...abundance +
                    data.type...organized.data.collection.scheme + 
                    data.type...visit.specific.covariates + 
                    data.type...multiple.datasets.integrated.for.analysis + 
                    data.type...life.stage + 
                    data.type...voucher.available,
                  data = rev)
m_no_scheme <- lm(log_years ~ 1 + 
                    data.type...what.where.when.only + 
                    data.type...sampling.effort.reported + 
                    data.type...abundance +
                    data.type...detection...non.detection + 
                    data.type...visit.specific.covariates + 
                    data.type...multiple.datasets.integrated.for.analysis + 
                    data.type...life.stage + 
                    data.type...voucher.available,
                  data = rev)
m_no_visitCovs <- lm(log_years ~ 1 + 
                       data.type...what.where.when.only + 
                       data.type...sampling.effort.reported + 
                       data.type...abundance +
                       data.type...detection...non.detection + 
                       data.type...organized.data.collection.scheme + 
                       data.type...multiple.datasets.integrated.for.analysis + 
                       data.type...life.stage + 
                       data.type...voucher.available,
                     data = rev)
m_no_multData <- lm(log_years ~ 1 + 
                      data.type...what.where.when.only + 
                      data.type...sampling.effort.reported + 
                      data.type...abundance +
                      data.type...detection...non.detection + 
                      data.type...organized.data.collection.scheme + 
                      data.type...visit.specific.covariates + 
                      data.type...life.stage + 
                      data.type...voucher.available,
                    data = rev)
m_no_lifeStage <- lm(log_years ~ 1 + 
                       data.type...what.where.when.only + 
                       data.type...sampling.effort.reported + 
                       data.type...abundance +
                       data.type...detection...non.detection + 
                       data.type...organized.data.collection.scheme + 
                       data.type...visit.specific.covariates + 
                       data.type...multiple.datasets.integrated.for.analysis + 
                       data.type...voucher.available,
                     data = rev)
m_no_voucher <- lm(log_years ~ 1 + 
                     data.type...what.where.when.only + 
                     data.type...sampling.effort.reported + 
                     data.type...abundance +
                     data.type...detection...non.detection + 
                     data.type...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.type...multiple.datasets.integrated.for.analysis + 
                     data.type...life.stage,
                   data = rev)

indiv_mods_time <- list(m_no_whwhwh, m_no_effort, m_no_abund, m_no_nonDet, 
                        m_no_scheme, m_no_visitCovs, m_no_multData, 
                        m_no_lifeStage, m_no_voucher)
```






---------------------

## Authors and Data Providing Institutions (TODO)

**Who choses to use biological records data?**

$H_a$: The first author is more likely to use data held by their own institution than by other institutions.

$H_0$: The number of studies with a lead author from the institution that provided the data is the same as expected by chance.

*Proposed Test*: Permutation test of lead author institutions.  

----------------------

____________________________________________________________________________

# Results

```{r caption_kripp_summary}
kripp_sum_cap <- figs(name = "kripp_sum_cap", 
                      "Reader agreement for 12 categories measured using Krippendorff's alpha.  The pre-calibration results are for only articles before a meeting to calibrate expectations and definitions (n = 11).  Post-calibration results are for only articles coded after the calibration meeting (n = 10). The all_pre_correct group shows results for the original codings for all articles (both those coded before and after the callibration meeting, n = 21).  The post-correct group shows agreement for all articles after I evaluated each individual case of disagreement and corrected obvious errors (n = 20). Dotted lines show values of alpha of 0.667 and 0.8, which are recommended minimum and preferred values at which variables can be relied upon for analysis.")
```

## Search results 
The search returned `r nrow(elig)` potentially relevant studies, of which we have evaluated `r sum(!is.na(elig$qualifies))` for eligibility, and judged `r length(which(elig$qualifies == TRUE))` to be eligible for inclusion in the scoping review.  One reader has coded `r nrow(rev)` articles and a second reader has coded `r nrow(cp_orig)` of those. This preliminary analysis uses the `r nrow(rev)` articles coded so far.


## Results for reader agreement

Krippendorff's alpha values for the variables that have been double coded indicate substantial disagreement between the two readers, and show few variables for which codings are reliable enough to be used in analysis (`r figs("kripp_sum_cap", display = "cite")`).  However, the number of articles that have been evaluated by two readers is probably too small for reliable estimation of Krippendorff's alpha given the proportions of the TRUE and FALSE classes for most variables (`r tabs("kripp_samp_size_cap", display = "cite")`).

The poor agreement between the two readers for any given category could be due to: 1) poor catgegory definitions or poor instructions about how to evaluate the category; 2) true ambiguity in how a study should be coded; 3) insufficient knowledge or skill of the reader(s); 4) accidental error, perhaps due in part to skimming articles rather than reading them in their entirety.  

Possible solutions are: 1) refine definitions and instructions, when possible specifying specific words or phrases that can be searched for in the article; 2) removing from analysis variables for which many articles are difficult to definitively categorize; 3) requiring a higher level of skill or knowledge of the readers, or treating the less-skilled readers' categorizations as a trigger for a "tie breaker" 3rd review to reveal obvious errors by the first reader, but not as reliable codings in their own right; 4) require readers to read the entire article rather than skimming it, or using disagreements as a trigger for a 3rd "tie breaker" review.

During the third "tie breaker" evaluation of disagreement cases, I judged that the disagreement was due to an obvious error and therefore changed the original (erroneous) value for `r length(which(mult_coded_rev != mult_coded_rev_orig[which(mult_coded_rev_orig$title %in% mult_coded_rev$title), ]))` of Willson's original codings and `r length(which(cp != cp_orig[which(cp_orig$title %in% cp$title), ]))` of CP's original codings.  Disagreements were not a result of obvious errors and values were therefore not changed in `r length(which(mult_coded_rev != cp))` cases.


```{r krippendorfs_alpha_summary, fig.cap = kripp_sum_cap}
kripp_summary_precorrect_long <- data.frame(
  set = c(rep("pre calibration", length(kripp_pre_cal)), 
          rep("post calibration", length(kripp_post_cal)), 
          rep("all_pre_correct", length(kripp_all_pre_correct)), 
          rep("all_post_correct", length(kripp_all_post_correct))), 
  variable = c(names(kripp_pre_cal), 
               names(kripp_post_cal), 
               names(kripp_all_pre_correct), 
               names(kripp_all_post_correct)), 
  krip_alpha = c(sapply(kripp_pre_cal, 
                        FUN = function(x) {x$value}), 
                 sapply(kripp_post_cal, 
                        FUN = function(x) {x$value}), 
                 sapply(kripp_all_pre_correct, 
                        FUN = function(x) {x$value}), 
                 sapply(kripp_all_post_correct, 
                        FUN = function(x) {x$value})))

kripp_summary_precorrect_long$set <- factor(kripp_summary_precorrect_long$set, 
                                 levels = c("pre calibration", 
                                            "post calibration", 
                                            "all_pre_correct", 
                                            "all_post_correct"))

if(diag_present) {
  print(
    ggplot(data = kripp_summary_precorrect_long, 
           mapping = aes(y = krip_alpha, x = set)) + 
      geom_boxplot() + 
      geom_point() + 
      geom_line(aes(group = variable)) + 
      geom_hline(aes(yintercept = 0.667), lty = "dotted") + 
      geom_hline(aes(yintercept = 0.8), lty = "dotted") + 
      ggtitle("Reader agreement using Krippendorff's alpha") + 
      xlab("") +
      ylab("Krippendorff's alpha") + 
      theme_bw()
  )
}
```

```{r plot_kripp_by_nTRUE}
# this is exploratory to see if krippendorff's alpha is varying wildly b/c
# of insufficient variation
alpha_df_all <- data.frame(
  set = rep("all", length(names(kripp_pre_cal))), 
  variable = names(kripp_all_pre_correct),  
  alpha = sapply(kripp_all_pre_correct, FUN = function(x) {x$value}),
  n_wg_T = sapply(names(kripp_all_pre_correct), 
                  FUN = function(x, df = mult_coded_rev_orig) {
                    df <- data.frame(df)
                    col_vec <- df[, which(colnames(df) == x)]
                    sum(col_vec)}), 
  stringsAsFactors = FALSE,
  row.names = 1:length(names(kripp_all_pre_correct))
)

alpha_df_pre_cal <- data.frame(
  set = rep("pre calibration", length(names(kripp_pre_cal))), 
  variable = names(kripp_pre_cal), 
  alpha = sapply(kripp_pre_cal, FUN = function(x) {x$value}), 
  n_wg_T = sapply(names(kripp_pre_cal), 
                  FUN = function(x, df = mult_coded_rev_orig) {
                    df <- data.frame(df)
                    col_vec <- df[, which(colnames(df) == x)]
                    sum(col_vec)}),
  stringsAsFactors = FALSE, 
  row.names = 1:length(names(kripp_pre_cal))
)

alpha_df_post_cal <- data.frame(
  set = rep("post calibration", length(names(kripp_post_cal))), 
  variable = names(kripp_post_cal), 
  alpha = sapply(kripp_post_cal, FUN = function(x) {x$value}), 
  n_wg_T = sapply(names(kripp_post_cal), 
                  FUN = function(x, df = mult_coded_rev_orig) {
                    df <- data.frame(df)
                    col_vec <- df[, which(colnames(df) == x)]
                    sum(col_vec)}), 
  stringsAsFactors = FALSE,
  row.names = 1:length(names(kripp_post_cal))
)

alpha_df <- bind_rows(list(alpha_df_all, alpha_df_pre_cal, alpha_df_post_cal))

if(diag) {
  print(
    ggplot(alpha_df, aes(x = n_wg_T, y = alpha, color = set)) + 
      geom_point() + 
      geom_hline(aes(yintercept = 0.667), lty = "dotted") + 
      geom_hline(aes(yintercept = 0.8), lty = "dotted") + 
      ggtitle("Krippendorff's alpha by the number\nof TRUEs in Willson's coding\nOriginal Codings Before Correcting Errors") + 
      xlab("Number of TRUE values in Willson's coding") + 
      theme_bw()
  )
}
```

```{r kripp_agreement_wg_before_and_after_corrections}
# show agreement measure of how well my corrections based on looking at double
# codings agree with my original codings.
if(diag) {
  hist(sapply(kripp_wg_comparison, FUN = function(x) {x$value}))
  summary(sapply(kripp_wg_comparison, FUN = function(x) {x$value}))
  sapply(kripp_wg_comparison, FUN = function(x) {x$value})
}
```

The calculation of agreement between my codings before and after I corrected them based on CP's codings kinda shows how reliable my original codings are. The assumption would be that if my original codings are reliable compared to my double-checking when prompted, then we can at least have confidence that my coding is consistent and I'm not making too many sloppy errors.  We can then use my codings (either original or corrected) for analysis, and view the second reader's role as highlighting cases of sloppy errors or careless oversight on my part.  This doesn't address the reproducability question of whether my categories are clear enough that an independent researcher trying to do the same study would come up with the same conclusions.  In order to answer that, we would need a second reader with adequate ability to code each category.  Lacking such a person, we'll have to use the second reader as an error catcher rather than an independent second opinion.

Page 240 of the Krippendorf (2004) book has recommendations about sample size needed based on how many cases there are of the least common class.  `r tabs("kripp_samp_size_cap", display = "cite")` shows which variables have enough of the minority class that Krippendorff's alpha can be calculated with significance level of 0.1 and minimum acceptable Krippendorff's alpha of 0.667.

```{r kripp_samp_size_cap}
kripp_samp_size_cap <- tabs(name = "kripp_samp_size_cap", 
                            "Recommended sample size for accurate estimation of Krippendorff's alpha given the proportion of the minority class in the results, and a desired confidence level of 0.1 for a minimum Krippendorff's alpha value of 0.667 (Krippendorf 2004).")
```

`r tabs("kripp_samp_size_cap", display = "full")`

```{r calculate_proportion_T}
prob_of_values <- lapply(mult_coded_rev[, 2:ncol(mult_coded_rev)], 
                         FUN = function(x) {sum(x, na.rm = T) / length(which(!is.na(x)))})
prob_of_values <- data.frame(variable = names(prob_of_values), 
                             prob_T = unlist(prob_of_values), 
                             row.names = NULL)
prob_of_values$prob_small_class <- NA
prob_of_values$kripp_pre_correct <- NA
for(i in 1:nrow(prob_of_values)) {
  if(prob_of_values$prob_T[i] <= 0.5) { # calc prob of minority class
    prob_of_values$prob_small_class[i] <- prob_of_values$prob_T[i]
  } else prob_of_values$prob_small_class[i] <- 1 - prob_of_values$prob_T[i]
  
  # add krippendorff's alpha for codings before corrections
  prob_of_values$kripp_pre_correct[i] <- kripp_all_pre_correct[[which(
    names(kripp_all_pre_correct) == prob_of_values$variable[i])]]$value
}

# On p 240 of Krippendorff book, it says that if acceptable alpha is 0.667 and 
# desired significance level is 0.1, then sample size needed is as follows:
prob_of_values$n_20_adequate <- prob_of_values$prob_small_class > 0.3
prob_of_values$n_30_adequate <- prob_of_values$prob_small_class > 0.2
prob_of_values$n_40_adequate <- prob_of_values$prob_small_class >= 0.143

if(diag_present) kable(prob_of_values[, -2], digits = 2)
```


#### Notes about agreement 
(*italics* means I've corrected it, **bold** means I need to deal with it.)

- biological records in facilitative role: generally, it looks like CP codes this as TRUE much more often than I do.
    - *"An assessment of bumblebee (Bombus spp) land use..." Casey coded this as T, but the species observation data were clearly used as a response variable.  Perhaps the confusion is because survey participants were identifed from a list of people who had participated in a previous survey.  Regardless, this should be FALSE.*
    - *"Modelling and mapping UK emissions..." Casey coded TRUE, but to me this is a pretty clear case of biological records as a predictor (used to modify the response variable of emissions).  I think this is an error from lack of understanding of what it would look like to have bio recs as a predictor variable.  I don't think there's actually much ambiguity about how this article would be coded, I think perhaps there is ambiguity in my instructions about this category.  So, I changed this to FALSE.*
    - *"The role of historical environmental information in perceptions..." Casey coded TRUE but the occurrence records were used as a response variable to create the presence trajectory type categories that were used for trait analysis.  See p 27 of the study.  This is an error and I will change it to FALSE.*
    - *Edit instructions to make clear what bio recs as a predictor would look like from a general perspective, not just explicitly in a regression equation or something.*
- cross validation
    - *The only disagreement was on "Nature protection areas of Europe are insufficient..." which I have already disqualified b/c it doesn't use UK/IE records.*
- data type - physical specimen: 
    * *Casey missed "Congruence in fungal phenology patterns...", perhaps because methods say "fungarium" data, and he may not have known the word or not realized that those are probably physical specimens in a collection.*
    - **I think Casey missed "Evaluating promotional approaches for citizen science biological recording..." which states that all data used were records for which there was either a photo or specimen.  But I don't know if this is clear enough to be an error that I correct, or whether this article is a bit ambiguous.  I will not change it for now.** 
    - *Perhaps I need to change name from "museum" to "physical specimen" for this category.*  
    * *For "Explaining European fungal fruiting phenology..." it looks like Casey correctly had TRUE for museum and I just missed the 1st sentence of methods - a bad oversight on my part.*
    * *For "Nature protection areas are insufficient..." Casey had T for museum, I had F, but on re-reading I think this doesn't even qualify at all - even though UK was in their original study extent, there were no records of the beetle in UK or IE, so it doesn't actually meet my criteria of using UK/IE biological records.*
- data type - organized monitoring scheme:  
    * *For "Evaluating promotional approaches for citizen science biological recording..." Casey coded T for monitoring scheme.  However, despite records coming from named "schemes" (UK Ladybird Recording Scheme & BeeWatch), those named schemes are actually just data collection portals/websites for opportunistic records.  There is no organized monitoring effort, no regular survey protocol, locations, or times of surveys.  So the correct coding is FALSE for that article.*
    - *Need to edit instructions to make it clear that this is only if the actual act of recording is standardized in methods, locations, or effort, not just having a central repository.*
    * *For "Nature protection areas of Europe are insufficient..." Casey put TRUE for organized scheme, but I find no record of that in the article.  Seems data were agregated from many databases, but no scheme.  Don't know why he put that.  Either misunderstanding or oversight.  Regardless, the article is now disqualified (see above).*
- sampling effort reported
    - *"British phenological records indicate high diversity..." Casey coded T, I coded FALSE, but I was wrong, as they used data from UK Butterfly Monitoring Scheme (among other data sources).  This was an error by me.*
    - *"Congruency in fungal phenology..." Casey coded TRUE, I coded FALSE.  This is not obviously an error by either of us, but is due to ambiguity in the study.  The local dataset used for Switzerland is from surveys in quadrats, and seems to be the result of a specific study.  I considered this not bio recs data but rather other data that was used in addition to bio recs data.  However, one could view this as bio recs data in the sense that bio recs are sometimes derived by searching published literature.  So it is a legitimate interpretation to call that bio recs data, in which case sampling effort = TRUE for this study.  I will not change anything as this is not an obvious error.*
    - *"Evaluating promotional approaches for citizen science..." Casey coded TRUE.  But despite records coming from named "schemes" (UK Ladybird Recording Scheme & BeeWatch), those named schemes are actually just data collection portals/websites for opportunistic records.  There is no organized monitoring effort, no regular survey protocol, locations, or times of surveys, and, as far as I can tell from looking at those scheme websites, no record of survey effort.  This is an error by CP and I will change it to FALSE.*
    - *"Large reorganizations in butterfly communities..." CP coded FALSE but the study uses UKBMS data which is collected on transects of known length.  This is an error by CP and I will change it to TRUE.*
    - *Edit instructions to make clear that what is being asked is whether the sampling effort is reported with the data, not whether sampling effort is reported in the study or article.  So a study that uses UK Butterfly Monitoring Scheme data is TRUE for sampling effort reported, because it is reported with the data, even if the study doesn't report that sampling effort (e.g. transect length).*
- diversity as focus: 
    - CP seems to be coding TRUE way more often than I am.  I think the main issue here is that I didn't define clearly enough that by "diversity" I mean a narrow definition of either alpha/beta diversity (turnover) or a combination of species richness and evenness.  I am not considering species richness alone or narrative descriptions of communities as "diversity" as a study focus.
    - *"An assessment of bumblebee (Bombus spp) land use...".  This looks like clearly not diversity as an outcome to me.  This is an error by CP and I will change TRUE to FALSE.*
    - *"An invertebrate survey of Scragh Bog...".  I don't see any explicity study of diversity (in the strict sense) in this.  This is an error by CP and I will change TRUE to FALSE.*
    - *"British phenological records indicate high diversity...".  Despite the title, this study uses species richness as the outcome measure, and is therefore not a diversity study by my strict definition.  Therefore I consider CP's TRUE to be an error, and I will change it to FALSE.*
    - *"Ocean current connectivity propelling...".  Don't know why CP coded this as T.  That is an error and I will change this to FALSE.* 
    - *"The interplay of climate and land use change affects...".  Doesn't measure diversity in the strict sense.  So CP's TRUE is an error and I will change it to FALSE.*
    - *Edit instructions to make clear that this is about either alpha or beta diversity or about some combination of species richness and evenness.*
- non detection inferred using taxonomic group
    - I think the issue here is that I intended a narrow sense in which explicit absence points are inferred and then used as data for a SDM (e.g "pseudo-absence" points).  This is not the same as drawing conclusions e.g. about phenology based only on presence records, which does assume absences when there are no presence records.  But there are no explicitly generated absences that are used as data in a model in that case.  
    - **Edit instructions to make clear that this is about using explicit inferred pseudo-absences as data in a model.  Or evaluate whether this is actually helping me answer an important question, and if not, drop it from coding.**
    - *"An assessment of bumblebee (Bombus spp) land use...".  CP coded T but it should be false.  Sampling design gives explicit non-detection data.  This is an error, and I am changing CP's value to FALSE.*
    - *"British phenological records indicate...".  CP coded T but the study makes no mention of using a method that requires absence data, so CP's coding is an error and I will change it to FALSE.*
    - *"Changes in the geographical distribution of plant...".  CP had F, I had T.  Ugh.  This one is hard.  They don't explicitly generate an "absence" value that is used in a model, but they do calculate the area of loss of species range based on no current records for a species, which is basically inferring absence.  Though they don't do this on the basis of other records from the taxonomic group, so according to my narrow definition of this category, the coding here should be FALSE I think.  This is making me wonder if this category is possible to code consistently, as almost all analyses infer absence at some level, and there is a very wide range to which that assumption is actually used in models or is just used in interpretation.  I think my coding was an error and I will change it to FALSE.*
    - *"Evaluating promotional approaches for citizen...". CP coded T, I coded F. CP was in error so I will change this to FALSE.*
    - *"Factors driving population recovery...".  CP coded T, I coded F. Study says 'Zero counts were reported'.  CP was in error, so I will change this value to FALSE.*
    - *"Modelling and mapping UK emissions...".  CP coded F, I coded T.  This uses the presence of sp in a hectad to allocate emissions to that hectad, and no emissions are allocated when there is no record, which I evidently thought is enough of an inferred absence to code T.  Again, though, this is ambiguous and hard to figure out a strict rule for when the absence is being inferred.  Though I guess in this case the absence is not being inferred *based on other presences in the taxonomic group*, but rather is just inferred based on no presence record, so the coding in this category should be FALSE.  This is my error, and I will change my value to FALSE.*
    - *"Nature protection areas of Europe...". CP coded T, I coded F.  The study states that 10000 background points were placed randomly and not considered true absences (so the records of other species in the taxonomic group were not used.  Therefore, CPs coding is an error and I will change it to FALSE.*
    - *"Ocean current connectivity...". CP coded T, I coded F.  I don't see any mention of inferring absences, and at least part of the data are explicitly stated as being collected as presence/absence data.  So CP is in error and I will change the value to FALSE.*
    - *"The arrow points north-...".  CP coded T, I coded F.  Again, they assumed absences, but not based explicilty on the presence of other species, just assuming adequate coverage generally.  So this is FALSE by my narrow category definition, but TRUE in a larger sense that they use assumed absences to calculate their metrics (just not assumed based on other records in the taxonomic group).  But this is very ambiguous, so I will not change any values.  I am becoming more convinced that this is not a well defined category right now.*
- policy focus
    - *"An invertebrate survey of Scragh...". CP coded T, I coded F.  I think it should be TRUE because the introduction talks about the site's designation of a Special Area of Conservation (SAC) and the Annex I and II species there.  This was my error, and I will change my coding to TRUE.*
    - *"Factors driving population recovery of greater...". CP coded F, I coded T. Slightly ambiguous, as it was not done by a policy group (e.g. gov. agency), but it talks about policy applications in abstract and studies areas in Agro-Environment Scheme as a predictor/hypothesis, which means that policy instruments are the focus of one of the tested hypotheses.  This is enough that I think it has a policy focus.  I probably need to define this category more explicitly, and until I do that, this article is not clearly coded in error but rather is ambiguous.  I will not change the values for now.*
- results type - descriptive only
    - *"Ocean current connectivity...". CP coded F, I coded T. I don't see any non-descriptive analyses that use the bio recs data, but it's a little unclear which data were used for which analyses, so this article is probably ambiguous, which means the codings are not in error.  I will not change them.*
- inference: looks pretty good.
    * *Casey missed "Explaining European fungal fruiting..." that he marked as FALSE but that had bootstrap CIs on one plot.*
    * *On "Ocean current connectivity..." I put no inference b/c all the analyses using bio recs didn't use inference, while Casey put inference b/c there were other analyses with non-bio recs data that used inference.*
- *spatial bias mentioned*
    - I wonder if I should narrow this to being TRUE only if some specific keywords are present (e.g. "spatial bias" or something like that), since it can be a little unclear whether authors are talking about spatial bias.  e.g. see "Evaluating promotional approaches..." in which they talk about bias, and talk about bio recs data being "geographically limited".
    - *"British phenological records indicate...". CP coded T, I coded F. I don't see any mention of spatial bias.  Maybe I'm missing it?  I guess I shouldn't change a T to F ever, b/c I may still be overlooking it?  So for now I will not change this.*
    - *"Evaluating promotional approaches...". CP coded T, I coded F. The article mentions datasets being "geographically limited" for some species, and also talks about general bias in bio recs data, so I think I was in error and this should be T.  I will change my value to TRUE.*
    - *"Factors driving population recovery...". CP coded T, I coded F.  I don't see any mention, but again I can't be sure there isn't, so I can't change coding to FALSE.*
    - *"Modelling and mapping UK emissions...". CP coded T, I coded F.  I am fairly confident there is no mention here, but I guess still I can't change CP's to FALSE.  Ugh. This is a frustrating category as I currently define it.*
- testing using independent dataset
    - *"Population variability in species can be deduced...". CP coded F, I coded T.  The study compares results using UKBMS and a separate dataset of opportunistic records.  Therefore CP's coding is an error and I will change it to TRUE.*
    - *"Potential for coupling the monitoring of bush-crickets...". CP coded F, I coded T.  The introduction says "We validated these methods using data from intensive survey sites", so I think CP's code is an error and I will change it to TRUE.*


## Results for temporal extent of studies

```{r print_time_anova}
m_time_full_anova
```

```{r print_time_individual_predictor_significance}
if(diag) {
  for(i in 1:length(indiv_mods_time)) { 
    print(anova(indiv_mods_time[[i]], time_lm_full))
  }
}
```

The overall model using supplementary data types to predict the natural log of temporal extent of study (in years) was not significant ($F_{`r format(round(m_time_full_anova$Df[2], digits = 2), scientific = F)`, `r format(round(m_time_full_anova$Res.Df[2], digits = 2), scientific = F)`}$ = `r format(round(m_time_full_anova$F[2], digits = 2), scientific = F)`, p = `r format(round(m_time_full_anova[6][[1]][2], digits = 2), scientific = F)`, Adjusted $R^2%$ = `r format(round(time_summary$adj.r.squared, digits = 2), scientific = F)`).  

If the overall model is significant, I will individually test whether each variable has a significant effect after accounting for the other variables.  I will interpret the effect direction and effect size of all variables in the context of the full model even if the variables are not significant on their own.
 
The expected temporal extent covered by a study that uses only "what, where, when" data is `r round(exp((time_lm_full$coefficients[1] + time_lm_full$coefficients[2])), digits = 1)` years.  **I'm a bit perplexed that this estimate is so low, as almost all the what where when only studies have temporal extents higher than this.  What is going on here?**

The expected temporal extent of studies is shorter for studies that include sampling effort information than for studies that use only "what, where, when" data (*p* = `r format(round(time_summary$coefficients[2, 4], digits = 3), scientific = F)`).  The effects of additional information on detection / non-detection and abundance were not significant after accounting for the effects of the other data types.  The signs of the effects of sampling effort information and detection / non-detection information were both negative, as expected.  However, the sign of the effect for abundance information was unexpectedly positive.  This may be because abundance is correlated with detection/non-detection and sampling effort.  When I fit a model with only the intercept and the abundance data type, the direction of the effect of abundance data was negative as expected.    

```{r time_extent_results_plot}
## TODO: need to hand-annotate R2 and p-value in plot
temp_extent$sig_different_from_WhWhWh <- temp_extent$data_type %in% 
                                       c("sampling effort reported")
print(ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + #, color = sig_different_from_WhWhWh
        geom_boxplot(varwidth = T) +
        scale_y_log10() + 
        xlab("Data Type") + 
        ylab("Temporal Extent of Study (years)") + 
        annotate("text", x = 3, y = 2600, 
                 label = "paste(\"Adjusted \", 
                 italic(R) ^ 2, \" = ???\")", 
                parse = TRUE) + 
  annotate("text", x = 3, y = 1200, 
           label = "paste(\"Overall model significance \", 
                 italic(p), \" = 0.1\")", 
           parse = TRUE) +
        theme_bw() + 
        scale_color_discrete(name = "Significantly Different from\nWhat, Where, When-only\nstudies (alpha = 0.1 level)") + 
        theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) 
)
```

```{r time_extent_results_plot_for_presentation_slide, eval = F, include = F}
## This code is to be run in the console for making plot to be exported as jpg for slides
## TODO: need to hand-annotate R2 and p-value in plot
t_size = 22 # plot text size
ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + #, color = sig_different_from_WhWhWh
  geom_boxplot(varwidth = T) +
  scale_y_log10() + 
  xlab("Data Type") + 
  ylab("Temporal Extent of Study (years)") + 
  # annotate("text", x = 3, y = 140, 
  #          label = "paste(\"Adjusted \", 
  #                italic(R) ^ 2, \" = 0.27\")", 
  #          parse = TRUE, 
  #          size = t_size - 15) + 
  annotate("text", x = 3, y = 1400, 
           label = "paste(\"Overall model significance \", 
                 italic(p), \" = 0.012\")", 
           parse = TRUE, 
           size = t_size - 14) +
  theme_bw() + 
  scale_color_discrete(name = "Significantly Different from\nWhat, Where, When-only\nstudies (alpha = 0.1 level)") + 
  theme(axis.text.x = element_text(angle = 330, vjust = 1, hjust = 0), 
        text = element_text(size = t_size)) 
```


## Study Question Paradigm

**What is the most popular study question paradigm?**

$H_a$: Some broad study question paradigms are more common than others in studies using biological records.

$H_0$: The number of studies pursuing each of the broad study question paradigms is the same.

*Proposed Test* One Poisson regression with study question paradigm as predictor. 

```{r make_study_question_paradigm_tibble}
paradigm_df <- select(rev, title, individual.species.analysis, 
                      community.analysis) %>%
  gather(key = "paradigm", value = "used", individual.species.analysis:
           community.analysis) %>%
  filter(used == TRUE)

table(paradigm_df$paradigm)
paste0("Studies that are neither community nor individual sp analyses: ", 
       length(which(rev$community.analysis == F & 
                      rev$individual.species.analysis == F)))
```

**What data types are used for each study question paradigm?**
[ descriptive results only ] 

```{r make_study_question_paradigm_by_dataType_tibble}
paradigm_data_df <- select(rev, title, individual.species.analysis, 
                      community.analysis, data.type...what.where.when.only, 
                      data.type...sampling.effort.reported, 
                      data.type...detection...non.detection, 
                      data.type...abundance) %>%
  gather(key = "paradigm", value = "parad_used", individual.species.analysis:
           community.analysis) %>%
  filter(parad_used == TRUE) %>%
  gather(key = "data_type", value = "dat_used", 
         data.type...what.where.when.only:data.type...abundance) %>%
  filter(dat_used == TRUE)

paradigm_data_df$data_type <- gsub("data.type...what.where.when.only", 
                                   "what, where, when only", 
                                   paradigm_data_df$data_type)
paradigm_data_df$data_type <- gsub("data.type...sampling.effort.reported", 
                                   "sampling effort", 
                                   paradigm_data_df$data_type)
paradigm_data_df$data_type <- gsub("data.type...detection...non.detection", 
                                   "detection / non-detection", 
                                   paradigm_data_df$data_type)
paradigm_data_df$data_type <- gsub("data.type...abundance", 
                                   "abundance", 
                                   paradigm_data_df$data_type)
paradigm_data_df$paradigm <- gsub("community.analysis", 
                                   "community", 
                                   paradigm_data_df$paradigm)
paradigm_data_df$paradigm <- gsub("individual.species.analysis", 
                                   "individual species", 
                                   paradigm_data_df$paradigm)

kable(table(paradigm_data_df$data_type, paradigm_data_df$paradigm))
```


## Specific Study Focus

Descriptive stats for studies focusing on policy or alien species, testing ecological theory, and testing methods of data correction.

```{r specific_question_by_paradigm_bar_plot}
qt_df <- select(rev, "title", 
                "species.richness", "diversity", 
                "distribution", "abundance", "phenology") 
names(qt_df)[2:6] <- c("species.richness", "diversity", 
                        "distribution", "abundance", "phenology")

qt_df <- gather(qt_df, key = "study_question", value = "v_2", 
                species.richness, diversity, distribution, abundance, 
                phenology,
                factor_key = T) %>%
  filter(v_2 == TRUE) %>%
  group_by(study_question) %>%
  summarise(count = n()) %>%
  mutate(percent = (count/sum(count)) * 100) %>%
  mutate(paradigm = str_replace_all(study_question, 
                                    c("species.richness|diversity" = 
                                        "community", 
                                      "distribution|abundance|phenology" = 
                                        "individual species"))) 

ggplot(data = qt_df, 
       aes(x = factor(study_question, 
                      levels = c("species.richness", "diversity", 
                                 "distribution", "phenology", "abundance"), 
                      labels = c("species richness", "diversity", 
                                 "distribution", "phenology", "abundance")), 
           y = percent, 
           fill = paradigm)) + 
  geom_bar(stat = "identity") + 
  scale_fill_brewer() + 
  xlab("Study Focus") + 
  ylab("Percent of Studies") + 
  theme_bw()
```

```{r specific_question_multi_paradigm_bar_plot}
qt_df <- select(rev, "title", "methodology.development.or.analysis", 
                "testing.macro.or.ecological.theory", "trends.over.time") 
names(qt_df)[2:4] <- c("methodology", "testing.theory", "temporal.trends")

qt_df <- gather(qt_df, key = "study_question", value = "v_2", 
                temporal.trends,testing.theory, methodology,  
         factor_key = T) %>%
  filter(v_2 == TRUE) %>%
  group_by(study_question) %>%
  summarise(count = n()) %>%
  mutate(percent = (count/sum(count)) * 100)

ggplot(data = qt_df, 
       aes(x = factor(study_question, 
                      levels = c("temporal.trends", "methodology",
                                 "testing.theory"), 
                      labels = c("temporal trends", "methodology",
                                 "testing theory")), 
           y = percent)) + 
  geom_bar(stat = "identity") + 
  scale_fill_brewer() + 
  xlab("Study Focus") + 
  ylab("Percent of Studies") + 
  theme_bw()
```


_____________________________________________________________________
# Discussion

## Reader Agreement
The seemingly poor agreement between WG's and CP's codings either reflects true ambiguity in how to code the variables or errors by one of the readers.  My review of all cases of disagreement revealed many cases that I believe to be obvious errors or oversights.  Of the obviously erroneous values, most were CP's, though some were mine.  This suggest that either I have given poor instructions to CP or CP is carrying out the instructions poorly.  It is probably not possible to figure out which of these things is happening without having a third reader, and preferable one with a higher skill level.  In some cases, my instructions were clearly insufficient.  I have now edited the instructions and hopefully solved those problems for the 12 variables CP double coded (though similar inadequate instructions probably still exist for the other variables).  The best test now would be to test these same 12 variables with another reader with a higher level of ability.  If that reader also has substantial disagreement, then either the categories themselves or the instructions will need to be majorly revised.  If that $3^{rd}$ higher-ability reader agrees with my original codings better, then it indicates that a highly skilled reader is required for the second readings.  

The technique of having me do a "tie breaker" evaluation of all disagreement cases is a partial solution.  This method would basically treat the $2^{nd}$ reader's codings as a check for error's in my coding that trigger a tie breaker review.  Cases in which the $2^{nd}$ readers coding is judged to be obviously erroneous would not be of much concern, and would be attributed to insufficient knowledge (e.g. not knowing that "fungarium" is a collection of physical specimens) or to skimming the articles quickly rather than reading them (CP seemed to spend less than 30 minutes per article).  Cases in which my original coding is judged to be obviously erroneous would be of concern.  Those values could be corrected for analysis, and/or the overall reliability of my original codings would need to be evaluated in some way to determine if a $2^{nd}$ check is needed for all studies and variables or whether my errors are few enough that they introduce an acceptable amount of error/uncertainty into the analysis. The ideal soluttion would be to have a highly-skilled second reader and to calculate Krippendorff's alpha for agreement between the two readers.  However, lacking such a person, I think it would be acceptable to use a less skilled second reader as a trigger for a 3rd tie breaker review done by me.  I do need to decide whether the trigger/tie breaker approach requires checking all studies and variables, or whether there is an adequate way to estimate the reliability of my original codings. I am not sure whether Ellie would be knowledgable enough to be a highly-skilled second reader.  Perhaps not. 

A final solution that is perhaps worth pursuing is only coding and analyzing variables for which coding can be done based entirely or largely on character string matching.  This would provide greater reliability, but at the cost of less interesting, informative, and insightful analysis (Krippendorff 2004, Section 11.1).

There are two goals of having two readers.  First, to estimate how replicatable the analysis is.  If another researcher could code the articles in the same way I do, then it provides confidence that the analysis is revealing some sort of true characteristics of the literature.  Second, to improve the quality of this specific analysis by identifying and correcting sloppy errors on my part.  This is not necessarily replicatable, though, as I am doing the correcting, so the categories basically still show "Willson's inner vision of the literature" rather than some more widely true characteristics. 

_________________________________________________________________________

# Old versions below here

## Types of questions  
Analyses may appear in this table or graph more than once.  For example, a study proposing a new method for estimating species richness and then using that method to test predictions about a productivity/richness relationship would appear in the counts in three columns.

```{r questions_by_data_type}
qt_df <- select(rev, "title", "year", "methodology.development.or.analysis", 
                "distribution", "abundance", "phenology", 
                "testing.macro.or.ecological.theory", "trends.over.time", 
                "data.type...what.where.when.only", 
                "data.type...detection...non.detection", 
                "data.type...abundance", "data.type...sampling.effort.reported", 
                "data.type...organized.data.collection.scheme") 
names(qt_df)[3:8] <- c("methodology", "distribution", "abundance",
                        "phenology", "testing.theory", "temporal.trends")

qt_df <- gather(qt_df, key = "study_question", value = "v_1", 
                methodology:temporal.trends, factor_key = T) %>%
  filter(v_1 == TRUE) %>%
  gather(key = "data.type", value = "v_2", 
         data.type...what.where.when.only:
           data.type...organized.data.collection.scheme, 
         factor_key = T) %>%
  filter(v_2 == TRUE)


kable(table(qt_df$data.type, qt_df$study_question), 
      caption = "The types of questions addressed by studies that use biological records, according to what type of data they use.")

### testing stuff
chisq.test(table(qt_df$data.type, qt_df$study_question), simulate.p.value = T)
```

```{r}
qt_df <- select(rev, "title", "year", "methodology.development.or.analysis", 
                "individual.species.analysis", "community.analysis", 
                "compiled.individual.species.analysis", "distribution", 
                "abundance", "phenology", "testing.macro.or.ecological.theory", 
                "trends.over.time") 
names(qt_df)[3:10] <- c("methodology", "individual.species", "community", 
                        "compiled.individual", "distribution", "abundance",
                        "phenology", "testing.theory")

qt_df <- gather(qt_df, key = "population", value = "v_1", individual.species, 
                compiled.individual, community, factor_key = T) %>%
  filter(v_1 == TRUE) %>%
  gather(key = "study_question", value = "v_2", distribution, abundance, 
         phenology, testing.theory, methodology, trends.over.time, 
         factor_key = T) %>%
  filter(v_2 == TRUE)


kable(table(qt_df$study_question, qt_df$population) / length(unique(qt_df$title)), digits = 2, 
      caption = "The types of questions addressed by studies that use biological records.  Values are the proportion of all studies.  Study populations are columns, topic of the study is in rows.")
```




## Taxonomic Group
```{r}
taxa <- select(rev, title, year, taxonomic.group, distribution, abundance, 
               phenology, testing.macro.or.ecological.theory, trends.over.time)

tax_names <- unlist(strsplit(taxa$taxonomic.group, "; "))
tax_names <- unique(tax_names)
tax_names <- tax_names[which(!is.na(tax_names))]
tax_names <- tax_names[order(tax_names)]

temp_df <- data.frame(matrix(nrow = nrow(taxa), ncol = length(tax_names)))
colnames(temp_df) <- tax_names
taxa <- cbind(taxa, temp_df)
rm(temp_df) 

for(i in 1:nrow(taxa)) {
  groups <- taxa$taxonomic.group[i]
  groups <- unlist(strsplit(groups, "; "))
  taxa[i, which(colnames(taxa) %in% groups)] <- T
}

taxa <- gather(taxa, key = "tax_group", value = "value", 
               all:wasps) %>%
  filter(value == T) 

table(taxa$tax_group)[order(table(taxa$tax_group), decreasing = T)]
```

```{r}
if(diag) {
  taxa <- gather(taxa, key = "question", value = "v_2", 
                 distribution:trends.over.time, factor_key = T) %>%
    filter(v_2 == T)
  
  table(taxa$tax_group, taxa$question)
}
```
Above is good.  Need to simplify taxonomic groups.  Might not end up using this table, b/c the patterns it shows probably have more to do with the type of data (abundance, semi-structure) than taxonomic group.  So, butterflies don't lend themselves to abundance and phenology studies, but the monitoring scheme used for butterflies does lend itself to those types of analyses.


--------------------------------------------------------------------

## Results format by data type
$H_01$: Different types of biological records data are not analyzed with different broad analysis approaches.

$H_a1$: Different types of biological records data are analyzed with different broad analysis approaches.

$H_02$: wh,wh,wh-only data is not analyzed with a different broad analysis approach than other data types.

$H_a2$: wh,wh,wh-only data is analyzed with inference and/or prediction less often than richer data are.  

$H_03$: The ?rate? at which wh,wh,wh-only data is analyzed with prediction is not greater than for other data types.

$H_a3$: The ?rate? at which wh,wh,wh-only data is analyzed with prediction is greater than for richer data types.

```{r analysis_approach_data_prep}
rf <- select(rev, c(title, data.type...what.where.when.only:
                      results.type...descriptive.only)) %>%
  gather(key = "data_type", value = "v_1", 
         data.type...what.where.when.only:data.type...detection...non.detection, 
         data.type...abundance:data.type...physical.specimen, 
         factor_key = T) %>%
  filter(v_1 ==T) %>%
  gather(key = "results_format", value = v_2, 
         results.type...inference:results.type...descriptive.only, 
         factor_key = T) %>%
  filter(v_2 == T)

# make names prettier
rf$results_format <- gsub("results.type...", "", rf$results_format)
rf$data_type <- gsub("data.type...", "", rf$data_type)

rf$results_format <- factor(as.character(rf$results_format), 
                            levels = c("inference", "prediction", "descriptive.only"), 
                            labels = c("inference", "prediction", "descriptive only"),
                            ordered = T)
```

```{r analysis_approach_by_data_type_assumptions_poisson_reg}
if(diag) {
  table(rf$data_type, rf$results_format)
  hist(table(rf$data_type, rf$results_format)) # doesn't look poisson-y
  
  addmargins(table(rf$data_type, rf$results_format))
  
  # check unconditional mean and variance - doesn't look the same
  mean(as.numeric(table(rf$data_type, rf$results_format)))
  var(as.numeric(table(rf$data_type, rf$results_format))) 

  
}
```

```{r resultsFormatByDataType}


if(diag) {
  pander(table(rev$results.type...descriptive.only), caption = "Descriptive Only Results (need at least 10 events/non evenst per predictor)")
}

dt_table <- table(rf$data_type)
names(dt_table) <- c("What, Where,\nWhen", "Abundance", "Known\nEffort", 
                     "Semi-\nStructured", "Visit-specific\nCovariates", 
                     "physical\nspecimen")
# pander(dt_table, caption = "Table of the gathered data_type column (compare to number of studies included)", keep.line.breaks = T)


kable(table(rf$data_type, rf$results_format), 
      caption = "Results formats produced using different types of data (this is counting studies, not counting analyses).  Is there a chi-squared-type test for when grouping variable categories are not mutually exclusive?")

```




Above looks ok, but probably need to do a chi-squared test or something to compare expected v. observed counts.  Or do glm with inference & hypothesis testing (or H testing, inference, and prediction) as positive class, “descriptive” as negative class, and data type as categorical predictors?  

### $H_02$

```{r results_by_data}
rev$quantitative <- rev$results.type...hypothesis.testing == T |
  rev$results.type...inference == T | rev$results.type...prediction == T

long_rev <- select(rev, title, quantitative, data.type...what.where.when.only:
                     data.type...detection...non.detection, 
                   data.type...sampling.effort.reported:data.type...physical.specimen) %>%
  gather(key = "data_type", value = "v_1", 
         data.type...what.where.when.only:data.type...physical.specimen) %>%
  filter(v_1 == T)

long_rev$quantitative <- gsub("TRUE", "inference/prediction", as.character(long_rev$quantitative))
long_rev$quantitative <- gsub("FALSE", "descriptive", as.character(long_rev$quantitative))
kable(table(long_rev$data_type, long_rev$quantitative))

```

```{r, echo = T}
if(diag) {
  table(rev$data.type...what.where.when.only)
  table(rev$data.type...abundance)
  table(rev$data.type...detection...non.detection)
  table(rev$data.type...sampling.effort.reported)
  table(rev$data.type...organized.data.collection.scheme)
  table(rev$data.type...visit.specific.covariates)
  table(rev$data.type...physical.specimen)
}
```

```{r fit_glm_analysis_approach}
results_data_model <- glm(quantitative ~ 
                            as.factor(data.type...what.where.when.only) + 
                            as.factor(data.type...abundance) + 
                            as.factor(data.type...detection...non.detection) + 
                            as.factor(data.type...sampling.effort.reported) + 
                            as.factor(data.type...organized.data.collection.scheme) + 
                            as.factor(data.type...visit.specific.covariates) + 
                            as.factor(data.type...physical.specimen), 
                          family = binomial, 
                          data = rev)
```

```{r glm_results}
# one_cov_mod <- glm(quantitative ~ data.type...what.where.when.only + 
#                             data.type...abundance, 
#                           family = binomial, 
#                           data = rev)

# summary(results_data_model)
# library(car)
# vif(results_data_model)
```











