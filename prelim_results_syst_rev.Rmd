---
title: "Biological Records Systematic Review - Preliminary Results"
csl: ecology.csl
output:
  html_document: default
  fig_caption: yes
  html_notebook: default
  word_document: null
bibliography: PhD_references.bib
---

Consider adding a step after the agreement calculation where I summarize studies for which there was a lot of disagreement overall - ie disagreement on many categories.  These may be particularly perplexing, novel, or creative studies that could be removed from the formal analyses because they are poorly characterized by the existing categories.  But report number removed, and discuss them separately in discussion.  How to calculate agreement over all categories?  Kripp alpha but with categories as the replicate coded units?  Then use cluster analyses on the K's alphas for all individual studies?  I expect 2 groups - studies that are generally well characterized, and studies that are generally poorly characterized.

**TODO:**

- Methods
    - *Study eligibiligy* [DONE]
    - Article coding
        - Data Type
        - Study Questions
        - *Analysis approach* [DONE]
        - *Agreement* [DONE]
    - *Temporal Extent* [DONE]
    - Authors and Data Providing Institutions
    - Study Question
        - *Paradigms* [DONE]
        - Specific focus
    - Analysis approach & data type
- Results - Statistical
    - Search results
        - n returned, n not returned by multiple searche methods (represents incompleteness of search), eligibility, n vars coded, n studies coded, n double coded
    - Reader agreement
    - *Temporal extent* [DONE]
    - Authors and Data Providing Institutions
    - Study Question
        - *Paradigm* [DONE]
        - Data type by question paradigm (add stats)
    - Analysis approach & data type
- Results - Descriptive
    - Spatial extent
    - *Specific focus* [DONE]
    - Author associated with data provider
    - Spatial bias correction
    - Taxonomic group
    - Role of biological records
    - Prediction performance measure
    - Tabulate analysis methods
- Discussion
    - Article coding
        - Agreement
    - Temporal extent
    - Study question
    - Analysis approach & data type
    - Bio recs as predictors, facilitative roles (horizon scanning)
 

```{r, echo=F}
knitr::opts_chunk$set(echo=F, message=F, warning=T, cache=F)
```

```{r, load_data, cache = F, results='hide', warning = F}
setwd("~/Documents/Data_Analysis/UCD/systematic_review/")
library(wgutil)
library(Hmisc)
library(boot)
library(irr)
library(captioner)
library(knitr)
library(pander)
library(ggridges)
library(bestglm)
library(car)
library(GGally)
library(tidyverse)

diag <- F # produce exploratory & diagnostics plots & results
diag_present <- T # produce diagnostics for report to Dina

if(diag == F) { # turn off warnings when not running diagnostics
  knitr::opts_chunk$set(echo=F, message=F, warning=F, cache=F)
}

source("./clean_data_syst_rev.R")
```

```{r calc_temp_extent}
# calculate number of years covered by study
wg$temp_extent <- as.numeric(wg$end.year) - as.numeric(wg$start.year)
# individually assign values to studies with temporal extent < 1 year
if(diag) paste0("The following article has a temporal extent of zero: ", 
                wg$title[which(wg$temp_extent == 0)])
wg$temp_extent[which(
  grepl("An assessment of bumblebee .* land use and floral.*", 
        wg$title))] <- 0.33
```

```{r numberingPrep}
# make functions for adding numbered captions to figures 
figs <- captioner(prefix = "Fig.") 
tabs <- captioner(prefix = "Table")
```

# Preliminary structure of analysis and results
This document states every question I plan to ask and shows methods, figures, tables and results text that I expect to use to answer each question in the biological records systematic review. Specific results and interpretations will change when I code the rest of the articles.

# Methods
We did a systematic review of original research published since 2014 that used biological records from Ireland and the UK.  We generated a list of relevant articles from Web of Science, Scopus, ProQuest, GoogleScholar, and the Global Biodiversity Information Facility (GBIF) website.  I evaluated each article for inclusion eligibility and coded information on [**??**] characteristics for each eligible article.  This coding was validated for a subset of articles by a second reader.  Agreement between the two readers was evaluated using Krippendorff's alpha (Krippendorff 2004).

## Study eligibility
Studies were eligible for inclusion if they met all of the following criteria:

1) original research
2) English language
3) used opportunistic biological data collected with non-standardized or semi-standardized protocols
4) included (but were not necessarily limited to) data from Ireland or the UK
5) the full text of the study was available through the UCD library online platform, Google, GoogleScholar, or ResearchGate.  
6) performs at least one analysis of the data (data papers and biotic atlases were not eligible)
7) sample size greater than 20
8) not restricted to fossil records

Grey literature was included and therefore peer review was not required.  Studies using semi-standardized data collection schemes (e.g. UK Butterfly Monitoring Scheme) were included as long as they included some opportunistic elements (e.g. locations chosen opportunistically by volunteers). Studies for which all data was collected by the study authors were excluded.  

## Article coding
### Data Type
We coded twelve variables describing aspects of data type: what, where, when only; sampling effort reported; abundance; detection / non-detection; organized monitoring scheme; visit-specific covariates; multiple datasets integrated for analysis; life stage information; physical specimen; photo; audio; video. Data types are not mutually exclusive except fot the "what where when only" data type, which cannot be true if any other data type is true.  We considered "what, where, when" as the baseline data type and considered all other data types supplementary additions to that basic data type.

For most analyses, we grouped the data types "physical specimen", "photo", "audio", and "video" into a "voucher specimen" group, but we coded them individually in order to identify emerging trends in how vouchers are collected.

For statistical analyses, we kept data type variables in models based on *a-priori* expectations about the variables' influence on data analysis strategy and based on expectations about different levels of complexity and effort involved in collecting, recording, and storing the data.  The data type variables we kept in models were: what, where, when only; sampling effort reported; abundance; detection / non-detection; organized monitoring scheme; visit-specific covariates; multiple datasets integrated for analysis; life stage information; voucher specimen. 

### Study Question
[TODO]

### Analysis approach
We classified the broad analysis approach used by each study into one of three categories: 1) inference; 2) prediction; or 3) descriptive only.  For each study, we evaluated the analysis approach only for analyses within the study that used biological records data.  

We considered studies estimating parameters and reporting some measure of uncertainty (e.g. *p*-values, confidence intervals, posterior probability distributions) to be using inference.  We considered studies that built models and made predictions to be using an analysis strategy of prediction, even if no uncertainty or inference was reported with the predictions.  Finally, we considered the analysis approach to be descriptive only if results were descriptive with no prediction or inference.  Descriptive results could be narrative, graphical, or quantitative as long as no inference or uncertainty was reported (e.g. point estimates of descriptive statistics without any confidence interval or *p*-values).  Studies could use both inference and prediction as analysis strategies and so those categories were not mutually exclusive, but studies were only categorized as using a descriptive analysis approach if all results were descriptive only and the study used no inference or prediction.  

### Agreement between two readers
I used Krippendorff's *alpha* [@Krippendorff2004] to evaluate the agreement between two readers for each of `r ncol(er)-1` variables that had been coded by two people.  [**Dina, any thoughts about Krippendorff's alpha or other measures of agreement?**]. I calculated Krippendorff's *alpha* using the `kripp.alpha` function in the `irr` package [@irr2012].  For each case of disagreement between the two readers, I reviewed the article.  When the disagreement seemed to be due to an oversight or an obvious error (e.g. when one reader coded "results type - inference" as FALSE but the article reported 95% confidence intervals around estimates), I corrected the erroneous value.  In cases where the disagreement was not the result of an obvious oversight, I did not change either value.  I calculated Krippendorff's alpha to assess agreement both before and after I corrected obviously erroneous codings. 

Krippendorff's *alpha* for binary data is calculated as:

$\alpha = 1 - D_O/D_E$

Where $D_o$ is the observed disagreement between readers and $D_e$ is the disagreement expected by chance.  The number of agreements (and disagreements) expected by chance is calculated using the observed values by calculating a coincidence matrix of values expected by chance:

|   |    0     |     1    |
|---|----------|----------| 
| 0 | $E_{00}$ | $E_{01}$ | 
| 1 | $E_{10}$ | $E_{11}$ | 

where $E_{00} = n_0 \times\ (n_0 - 1)/(n-1)$

$E_{01} = E_{10} = n_0 \times\ n_1/(n - 1)$

$E_{11} = n_1 \times\ (n_1 - 1) / (n - 1)$

$n$ is the number of observations (twice the number of coded units when there are two observers), $n_0$ is the number of zeros in all observed pairs, and $n_1$ is the number of ones in all observed pairs.

The coincidence matrix for observed values is:

|   |    0     |     1    |
|---|----------|----------|
| 0 | $O_{00}$ | $O_{01}$ | 
| 1 | $O_{10}$ | $O_{11}$ | 

"Coincidences sum contingencies and their inverses, thereby omitting the references to the individual observers..." @Krippendorff2004



```{r inter_coder_agreement_subset_dfs, warning=TRUE}
## ellie's results ----------------------------------------------------
# remove columns that aren't coded variables 
er <- er[, which(colnames(er) %nin% c("link", "qualifies", "authors", 
                                      "publication", "doi", "year", 
                                      "coding.DONE"))]
er <- er[order(er$title), ] # order rows
er <- er[, order(colnames(er))] # order columns
# put title column first
er <- er[, c(which(colnames(er) == "title"), 
                       which(colnames(er) != "title"))]
## end ellie's results --------------------------------- ------------------

## my results after error correction ---------------------------------------
# subset my coded results to titles and columns coded by ellie
mult_coded_wg <- wg[which(wg$title %in% er$title), 
                      which(colnames(wg) %in% colnames(er))]
# order rows
mult_coded_wg <- mult_coded_wg[order(mult_coded_wg$title), ] 
# order cols
mult_coded_wg <- mult_coded_wg[, order(colnames(mult_coded_wg))] 
mult_coded_wg <- mult_coded_wg[, c(
  which(colnames(mult_coded_wg) == "title"), 
  which(colnames(mult_coded_wg) != "title"))] # put title column first
## end my results after error correction --------------------------------------

## make sure all titles are present
if(any(mult_coded_wg$title %nin% er$title) | 
   any(er$title %nin% mult_coded_wg$title)) {
  warning("er and mult_coded_wg don't have the same titles. One or both of those data frames will be subsetted.")
  mult_coded_wg <- mult_coded_wg[which(mult_coded_wg$title %in% er$title), ]
  er <- er[which(er$title %in% mult_coded_wg$title), ]
}

## make sure df rows and cols are in the same order
if(!identical(colnames(er), colnames(mult_coded_wg))) {
  stop("Columns in er and mult_coded_wg must be identical and in the same order. They must be in the same order for creating the variable data frames that will be used to calculate Krippendorf's alpha. ")
}
```

```{r make_dfs_of_double_coded_variables_all_studies}
# each row is codings from one person
# each column is a study
dbl_coded_list <- list()
if(colnames(er)[1] != "title" | colnames(mult_coded_wg)[1] != "title") {
  stop("The first column of er and mult_coded_wg must be the study title")}

for(i in 2:ncol(er)) {
  dbl_coded_list[[i-1]] <- data.frame(matrix(
    data = c(er[, i], mult_coded_wg[, i]), 
    nrow = 2, ncol = nrow(er), byrow = T))
}
names(dbl_coded_list) <- colnames(er)[2:ncol(er)]
```

```{r krippendorfs_alpha_calc, warning=FALSE, message=FALSE}
do_kripp.alpha <- function(x, method) {
  # function to call kripp.alpha while converting dataframe x to a matrix
  # ARGS: x - data frame in which rows are observers and columns are studies
  #       method - character string of method to pass to kripp.alpha
  kripp.alpha(as.matrix(x), method = method)
}

kripp_all_dbl_coded <- lapply(dbl_coded_list, FUN = do_kripp.alpha, 
                                 method = "nominal")
```




--------------------------

## Temporal Extent

$H_a$: The mean temporal extent of studies using only what, where, when data is longer than the mean temporal extent of studies using richer data types. 

$H_0$: The mean temporal extent of studies using what, where, when-only data is the same as the mean temporal extent of studies using richer data types. 

*Proposed Test*: Linear regression with natural-log transformed time (in years) as the outcome and data types as predictors.

We used linear regression with natural-log transformed time (in years) as the response variable.  We used nine binary predictor variables indicating whether the study used data with the following characteristics: 1) all data contained only what, where, when information, 2) at least some records associated with known sampling effort, 3) at least some records contained abundance data, 4) at least some data contained explicit non-detection information, 5) at least some data came from an organized monitoring scheme, 6) at least some records included visit-specific covariates, 7) multiple different biological records datasets were integrated for analyses, 8) at least some records contained life stage information, and 9) voucher specimens were used for the analysis.  While the predictor variables are correlated, we kept all variables in the multiple regression model because we are interested in the effect of each additional type of information *after accounting for other information types* (e.g. is there an additional effect of having abundance data after adjusting for whether the data came from an organized monitoring scheme).

Note that because the variables are not mutually exclusive these are each individual variables - these are not dummy variables representing a single categorical factor level "data type" variable.  

The intercept-only model in this analysis is the mean temporal extent of all studies.  


```{r time_assess_correlation_predictors, fig.height=8, fig.width = 8}
### assess correlation between data type predictors ----------------------
dt_df <- wg[, which(
  colnames(wg) %in% 
    c("data.structure...organized.data.collection.scheme", 
      "data.structure...sampling.effort.known", 
      "data.structure...non.detection", 
      "data.structure...multiple.datasets.integrated.for.analysis", 
      "data.type...exclusively.whwhwh", 
      "data.type...abundance", 
      "data.type...visit.specific.covariates", 
      "data.type...life.stage", 
      "data.type...voucher.of.some.kind.necessary.for.analysis"))]
for(i in 1:ncol(dt_df)) {
  dt_df[, i] <- as.factor(as.character(data.frame(dt_df)[, i]))
}

print(ggpairs(dt_df, columns = 1:ncol(dt_df), 
              upper = list(discrete = "ratio"),
              lower = list(discrete = "facetbar"), 
              diag = list(discrete = "barDiag"), 
              columnLabels = c("organized scheme", 
                               "sampling effort",
                               "non detection",
                               "multiple datasets", 
                               "exclusively what where when", 
                               "abundance",
                               "visit specific covariates", 
                               "life stage", 
                               "voucher used"), 
              labeller = label_wrap_gen(width = 10)) + 
        ggtitle("Correlation of data type predictor variables") + 
        theme(axis.text.x = element_text(angle = 50, hjust = 1)))
```


```{r prepare_temporal_extent_df}
temp_extent <- select(
  wg, c(title, temp_extent, 
        data.type...exclusively.whwhwh, 
        data.structure...sampling.effort.known, 
        data.type...abundance,
        data.structure...non.detection,
        data.structure...organized.data.collection.scheme, 
        data.type...visit.specific.covariates, 
        data.structure...multiple.datasets.integrated.for.analysis,
        data.type...life.stage, 
        data.type...voucher.of.some.kind.necessary.for.analysis)) %>%
  gather(key = "data_type", value = "dt_value", 
         data.type...exclusively.whwhwh:data.type...voucher.of.some.kind.necessary.for.analysis, 
         factor_key = T) %>%
  filter(dt_value == T) %>%
  group_by(data_type)

# make names prettier
temp_extent$data_type <- gsub("data.type...", "", temp_extent$data_type)
temp_extent$data_type <- gsub("data.structure...", "", temp_extent$data_type)

temp_extent$data_type <- factor(as.character(temp_extent$data_type), 
                            levels = c("exclusively.whwhwh", 
                                       "sampling.effort.known",
                                       "abundance",
                                       "non.detection", 
                                       "organized.data.collection.scheme", 
                                       "visit.specific.covariates", 
                                       "multiple.datasets.integrated.for.analysis", 
                                       "life.stage", 
                                       "voucher.of.some.kind.necessary.for.analysis"), 
                            labels = c("exclusively what where when", 
                                       "sampling effort known",
                                       "abundance",
                                       "non detection", 
                                       "organized scheme", 
                                       "visit specific covariates", 
                                       "multiple datasets", 
                                       "life stage", 
                                       "voucher used"))
```

Evaluate normality of raw data:
```{r temp_range_distribution}
if(diag_present) {
  # distribution of all data
 # hist(wg$temp_extent)
 # hist(wg$temp_extent[which(wg$temp_extent <= quantile(wg$temp_extent, 
 #                                                        0.90, na.rm = T))])
  ## data are not balanced
  print(table(temp_extent$data_type))
  print("Data are not balanced.")
  
  ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
    geom_boxplot() +
    theme_bw() + 
    ggtitle("All data") + 
    theme(axis.text.x = element_text(angle = 330, vjust = 0.3))
  
  ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
    ylim(c(0, 800)) + 
    geom_boxplot() +
    theme_bw() + 
    ggtitle("Temporal Duration after\ncutting off the 8000 year study") + 
    theme(axis.text.x = element_text(angle = 330, vjust = 0.3))
  
  ggplot(data = temp_extent, aes(x = temp_extent, y = data_type)) + 
    xlim(c(0, 800)) + 
    geom_density_ridges() +
    ggtitle("Temporal Duration after\ncutting off the 8000 year study") + 
    theme_bw()
  
  ## normality -------------------------------------
  # data very non-normal (above) but log transformation kinda fixes this
  # (reciprocal and sqrt transformations did not help)
  print(ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + 
          geom_boxplot(varwidth = T) +
          scale_y_log10() + 
          theme_bw() + 
          ggtitle("log transformation of year\nmakes normality better") + 
          ylab("Temporal extent (years)") + 
          theme(axis.text.x = element_text(angle = 40, vjust = 0.9, 
                                           hjust = 1)))
  
  print(ggplot(data = temp_extent, aes(x = temp_extent, y = data_type)) + 
          scale_x_log10() + 
          geom_density_ridges(stat = "binline", alpha = 0.4) +
          geom_density_ridges(alpha = 0.4) + 
          xlab("Temporal extent (years)") + 
          theme_bw())
  
  if(diag) {
    for(i in unique(temp_extent$data_type)) { 
      # distribution of log(years) by data type
      hist(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
      qqnorm(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
      qqline(log(temp_extent$temp_extent)[which(temp_extent$data_type == i)])
    }
  }
}
```

Fit full model and evaluate residuals for assumptions of linear regression.
```{r fit_temp_extent_lm}
## fit linear regression with log-transformed years as response variable
## and each data type as a separate variable 
# fit full model
wg$log_years <- log(wg$temp_extent)
# variables added based on a-priori expectation of most to least important
time_lm_full <- lm(log_years ~ 1 + 
                     data.type...exclusively.whwhwh + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                   data = wg, 
                   na.action = na.exclude) 

time_resids <- rstandard(time_lm_full) # calculate residuals
if(diag_present) {
  print("Diagnostics for full model log_years ~ all 9 data types")
  plot(time_lm_full)
  # hist(time_resids, 
  #      main = "standardized residuals of full model for temporal extent", 
  #      xlab = "standardized residuals")
  boxplot(time_resids, ylab = "standardized residuals")
}
time_summary <- summary(time_lm_full)
if(diag) time_summary

## to interpret outcome on original scale:
# for a one-unit increase in x, the predicted value of y 
# changes by (e^beta1 - 1)*100 percent
```

Fitted vs. residuals plot shows some fanning.  

The coefficient estimate for what, where, when data does not produce a good prediction (expected value is lower than any value actual temp extent for those data).  I think there are a couple issues here.  1) the estimate of the effect of what, where, when is *after correcting for multiple datasets used in analysis*, which tends to have long studies.  2) many temp extents are NA, which means sample size is quite small (7-ish or less for wh,wh,wh == T).  So I don't think anything is going wrong with the model.

```{r time_data_type_full_model_sig_test}
# Test significance of full model
m0 <- lm(log_years ~ 1, data = wg) # fit null model 
m_time_full_anova <- anova(m0, time_lm_full)
```

```{r time_data_type_test_main_effects, include = F}
## test effect of each data type predictor by comparing the full model to a
# model with that predictor removed
# 
# TODO: Does this need multiple comparison correction?

m_no_whwhwh <- lm(log_years ~ 1 + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                  data = wg)
m_no_effort <- lm(log_years ~ 1 + 
                     data.type...exclusively.whwhwh + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                  data = wg)
m_no_abund <- lm(log_years ~ 1 + 
                     data.type...exclusively.whwhwh + 
                     data.structure...sampling.effort.known + 
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                 data = wg)
m_no_nonDet <- lm(log_years ~ 1 + 
                     data.type...exclusively.whwhwh + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                  data = wg)
m_no_scheme <- lm(log_years ~ 1 + 
                     data.type...exclusively.whwhwh + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                  data = wg)
m_no_visitCovs <- lm(log_years ~ 1 + 
                     data.type...exclusively.whwhwh + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                     data = wg)
m_no_multData <- lm(log_years ~ 1 + 
                     data.type...exclusively.whwhwh + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.type...life.stage + 
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                    data = wg)
m_no_lifeStage <- lm(log_years ~ 1 + 
                     data.type...exclusively.whwhwh + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...voucher.of.some.kind.necessary.for.analysis,
                     data = wg)
m_no_voucher <- lm(log_years ~ 1 + 
                     data.type...exclusively.whwhwh + 
                     data.structure...sampling.effort.known + 
                     data.type...abundance +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.type...visit.specific.covariates + 
                     data.structure...multiple.datasets.integrated.for.analysis +
                     data.type...life.stage,
                   data = wg)

indiv_mods_time <- list(m_no_whwhwh, m_no_effort, m_no_abund, m_no_nonDet, 
                        m_no_scheme, m_no_visitCovs, m_no_multData, 
                        m_no_lifeStage, m_no_voucher)
```

## Study Question

### What is the most popular study question paradigm?

$H_a$: Some broad study question paradigms are more common than others in studies using biological records.

$H_0$: The number of studies pursuing each of the broad study question paradigms is the same.
 
We counted the number of studies that asked questions within each of the three broad study question areas (individual species questions, community questions, and other questions).  We estimated 95% confidence intervals around the number of studies asking each type of study question using bias-corrected accelerated bootstrap confidence intervals @Efron1993. **Should I do an explicit test of difference or adjust these CIs to account for multiple tests?**

```{r make_other_paradigm_category}
# make an "other paradigm" column for all studies that are not community or 
# individual species studies (this will be a catch-all category for methodology
# studies and whatever else)
wg$other_paradigm <- wg$community.question == F & 
  wg$individual.species.question == F
```

```{r paradigm_bootstrap}
count_fun <- function(x, indices) {
  # ARGS: x - a vector of TRUE/FALSE values, for which the TRUES will be counted
  #       indices - a vector of indices as required by boot
  x_b <- x[indices]
  sum(as.numeric(x_b), na.rm = TRUE)
}

boot_paradigm <- function(x) {
  b_o <- boot(x, count_fun, R = 10000, stype = "i")
  ci_o <- boot.ci(b_o, conf = 0.95, type = "bca")
  to_return <- list(obs = x, 
                    boot_obj = b_o, 
                    ci_obj = ci_o)
  to_return
}

paradigm_list <- lapply(list(community = factor(wg$community.question), 
                             individual_sp = factor(wg$individual.species.question), 
                             other = factor(wg$other_paradigm)), 
                        FUN = boot_paradigm)
```



---------------------

## Authors and Data Providing Institutions (TODO)

**Who choses to use biological records data?**

$H_a$: The first author is more likely to use data held by their own institution than by other institutions.

$H_0$: The number of studies with a lead author from the institution that provided the data is the same as expected by chance.

*Proposed Test*: Permutation test of lead author institutions.  

----------------------

____________________________________________________________________________

# Results

```{r caption_kripp_summary}
kripp_sum_cap <- figs(name = "kripp_sum_cap", 
                      paste0("Reader agreement for ", 
                             length(which(colnames(mult_coded_wg) != "title")), 
                             " categories measured using Krippendorff's alpha. The ??? group shows agreement for all articles (n = ", 
                             nrow(mult_coded_wg), 
                             ").  Dotted lines show values of alpha of 0.667 and 0.8, which are recommended minimum and preferred values at which variables can be relied upon for analysis."))
```

## Search results 
The search returned `r nrow(elig)` potentially relevant studies, of which we have evaluated `r sum(!is.na(elig$qualifies))` for eligibility, and judged `r length(which(elig$qualifies == TRUE))` to be eligible for inclusion in the scoping review.  ??? percent of potentially relevant studies were returned by more than one search method, while ??? percent of potentially relevant studies were returned by only one search method.  This suggests that additional searching is likely to be less rewarding as most studies identified by the later searches had already been identified by earlier searches.  

One reader has coded `r nrow(wg)` articles and a second reader has coded `r nrow(er)` of those. 

This preliminary analysis uses the `r nrow(wg)` articles coded so far.  


## Reader agreement

Krippendorff's alpha values for the variables that have been double coded indicate substantial disagreement between the two readers, and show few variables for which codings are reliable enough to be used in analysis (`r figs("kripp_sum_cap", display = "cite")`).  However, the number of articles that have been evaluated by two readers is probably too small for reliable estimation of Krippendorff's alpha given the proportions of the TRUE and FALSE classes for most variables (`r tabs("kripp_samp_size_cap", display = "cite")`).

The poor agreement between the two readers for any given category could be due to: 1) poor catgegory definitions or poor instructions about how to evaluate the category; 2) true ambiguity in how a study should be coded; 3) insufficient knowledge or skill of the reader(s); 4) accidental error, perhaps due in part to skimming articles rather than reading them in their entirety.  

Possible solutions are: 1) refine definitions and instructions, when possible specifying specific words or phrases that can be searched for in the article; 2) removing from analysis variables for which many articles are difficult to definitively categorize; 3) requiring a higher level of skill or knowledge of the readers, or treating the less-skilled readers' categorizations as a trigger for a "tie breaker" 3rd review to reveal obvious errors by the first reader, but not as reliable codings in their own right; 4) require readers to read the entire article rather than skimming it, or using disagreements as a trigger for a 3rd "tie breaker" review.



```{r krippendorfs_alpha_summary, fig.cap = kripp_sum_cap}
kripp_summary_long <- data.frame(
  variable = names(kripp_all_dbl_coded), 
  krip_alpha = sapply(kripp_all_dbl_coded, 
                        FUN = function(x) {x$value}))

if(diag_present) {
  print(
    ggplot(data = kripp_summary_long, 
           mapping = aes(y = krip_alpha)) + 
      geom_boxplot() + 
      geom_point(aes(x = 0)) + 
      geom_hline(aes(yintercept = 0.667), lty = "dotted") + 
      geom_hline(aes(yintercept = 0.8), lty = "dashed") + 
      ggtitle("Reader agreement using Krippendorff's alpha") + 
      xlab("") +
      ylab("Krippendorff's alpha") + 
      theme_bw() + 
      theme(axis.text.x = element_blank())
  )
}
```

```{r plot_kripp_by_nTRUE}
## As of 8 Nov this is broken because I have columns that aren't just T/F
# this is exploratory to see if krippendorff's alpha is varying wildly b/c
# of insufficient variation
# alpha_df <- data.frame(
#   variable = names(kripp_all_dbl_coded),  
#   alpha = sapply(kripp_all_dbl_coded, FUN = function(x) {x$value}),
#   n_wg_T = sapply(names(kripp_all_dbl_coded), 
#                   FUN = function(x, df = mult_coded_wg) {
#                     df <- data.frame(df)
#                     col_vec <- df[, which(colnames(df) == x)]
#                     if(is.logical(col_vec)) {
#                       sum(col_vec, na.rm = T)
#                     } else NA
#                   }), 
#   stringsAsFactors = FALSE,
#   row.names = 1:length(names(kripp_all_dbl_coded))
# )
# 
# if(diag) {
#   print(
#     ggplot(alpha_df, aes(x = n_wg_T, y = alpha)) + 
#       geom_point() + 
#       geom_hline(aes(yintercept = 0.667), lty = "dotted") + 
#       geom_hline(aes(yintercept = 0.8), lty = "dotted") + 
#       ggtitle("Krippendorff's alpha by the number\nof TRUEs in Willson's coding") + 
#       xlab("Number of TRUE values in Willson's coding") + 
#       theme_bw()
#   )
# }
```


We could use my codings (either original or corrected) for analysis, and view the second reader's role as highlighting cases of sloppy errors or careless oversight on my part.  This doesn't address the reproducability question of whether my categories are clear enough that an independent researcher trying to do the same study would come up with the same conclusions.  In order to answer that, we would need a second reader with adequate ability to code each category.  Lacking such a person, we'll have to use the second reader as an error catcher rather than an independent second opinion.

Page 240 of the Krippendorf (2004) book has recommendations about sample size needed based on how many cases there are of the least common class.  `r tabs("kripp_samp_size_cap", display = "cite")` shows which variables have enough of the minority class that Krippendorff's alpha can be calculated with significance level of 0.1 and minimum acceptable Krippendorff's alpha of 0.667.

```{r kripp_samp_size_cap}
kripp_samp_size_cap <- tabs(name = "kripp_samp_size_cap", 
                            paste0("Recommended sample size for accurate estimation of Krippendorff's alpha given the proportion of the minority class in the results, and a desired confidence level of 0.1 for a minimum Krippendorff's alpha value of 0.667 (Krippendorf 2004).  So far there are ", nrow(mult_coded_wg), " double-coded studies."))
```

`r if(diag_present) tabs("kripp_samp_size_cap", display = "full")`

```{r calculate_proportion_T}
## This calculates the probability of the smallest class for each variable, for 
## use in calculating minimum sample size needed for Kripp's alpha for that 
## variable.  
## This treats all variables as factor variables.
prob_of_values <- lapply(
  wg[, which(colnames(wg) %in% names(kripp_all_dbl_coded))], 
  FUN = function(x) {
    x <- factor(x)
    mn <- 1
    for(j in 1:length(levels(x))) {
      prob <- length(which(x == levels(x)[j])) / length(which(!is.na(x)))
      if(prob < mn) mn <- prob
    }
    mn
  })

prob_of_values <- data.frame(variable = names(prob_of_values), 
                             prob_small_class = unlist(prob_of_values), 
                             row.names = NULL)
prob_of_values$kripp_all <- NA
for(i in 1:nrow(prob_of_values)) {
  # add krippendorff's alpha for codings
  prob_of_values$kripp_all[i] <- kripp_all_dbl_coded[[which(
    names(kripp_all_dbl_coded) == prob_of_values$variable[i])]]$value
}

# On p 240 of Krippendorff book, it says that if acceptable alpha is 0.667 and 
# desired significance level is 0.1, then sample size needed is as follows:
prob_of_values$n_25_adequate <- prob_of_values$prob_small_class > 0.25
prob_of_values$n_30_adequate <- prob_of_values$prob_small_class > 0.2
prob_of_values$n_40_adequate <- prob_of_values$prob_small_class >= 0.143

if(diag_present) {
  kable(prob_of_values[order(
  prob_of_values$kripp_all, decreasing = T), -2], digits = 2)
  
  # list only those for which sample size is adequate
  samp_adequate <- prob_of_values[which(prob_of_values$n_30_adequate == T), ]
  kable(samp_adequate[order(
  samp_adequate$kripp_all, decreasing = T), -2], digits = 2)
  rm(samp_adequate)
}
```

```{r plot_kripp_for_adequate_sample_size}
# plot krippendorf's alpha values for only variables that have adequate sample size
ggplot(data = prob_of_values[which(prob_of_values$n_30_adequate == T), ], 
       aes(y = kripp_all)) +
  geom_boxplot() + 
  geom_point(aes(x = 0)) + 
  geom_hline(aes(yintercept = 0.667), lty = "dotted") + 
  geom_hline(aes(yintercept = 0.8), lty = "dashed") + 
  ggtitle("Krippendorf's alpha for the variables for\nwhich we have adequate sample size") +
  ylab("Krippendorf's Alpha value") + 
  xlab("") +
  theme_bw() + 
  theme(axis.text.x = element_blank())
```



#### Notes about agreement 
(*italics* means I've corrected it, **bold** means I need to deal with it.)



## Temporal extent of studies

```{r print_time_anova}
m_time_full_anova
```

```{r print_time_individual_predictor_significance}
if(diag) {
  for(i in 1:length(indiv_mods_time)) { 
    print(anova(indiv_mods_time[[i]], time_lm_full))
  }
}
```

The overall model using supplementary data types to predict the natural log of temporal extent of study (in years) was (not) significant ($F_{`r format(round(m_time_full_anova$Df[2], digits = 2), scientific = F)`, `r format(round(m_time_full_anova$Res.Df[2], digits = 2), scientific = F)`}$ = `r format(round(m_time_full_anova$F[2], digits = 2), scientific = F)`, p = `r format(round(m_time_full_anova[6][[1]][2], digits = 2), scientific = F)`, Adjusted $R^2%$ = `r format(round(time_summary$adj.r.squared, digits = 2), scientific = F)`).  

If the overall model is significant, I will individually test whether each variable has a significant effect after accounting for the other variables.  I will interpret the effect direction and effect size of all variables in the context of the full model even if the variables are not significant on their own.  Example of how results will be written:
 
The expected temporal extent covered by a study that uses only "what, where, when" data is `r round(exp((time_lm_full$coefficients[1] + time_lm_full$coefficients[2])), digits = 1)` years when the data have no other data types.  When the data are "what, where, when" only but multiple datasets were integrated for analysis, the expected temporal extent covered by a study is `r round(exp((time_lm_full$coefficients[1] + time_lm_full$coefficients[2] + time_lm_full$coefficients[8])), digits = 1)` years. 

Studies using data that includes sampling effort information have a shorter temporal extent after accounting for other data types that the data has (*p* = `r format(round(time_summary$coefficients[2, 4], digits = 3), scientific = F)`).  The effects of additional information on detection / non-detection and abundance were not significant after accounting for the effects of the other data types.  The signs of the effects of sampling effort information and detection / non-detection information were both negative, as expected.  However, the sign of the effect for abundance information was unexpectedly positive.  This may be because abundance is correlated with detection/non-detection and sampling effort, and so the effect of abundance data is positive after correcting for those other data types.  When I fit a model with only the intercept and the abundance data type, the direction of the effect of abundance data was negative as expected.    

```{r time_extent_results_plot}
## TODO: need to hand-annotate R2 and p-value in plot
temp_extent$sig_different_from_WhWhWh <- temp_extent$data_type %in% 
                                       c("[names of sig. diff. vars. here")
print(ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + #, color = sig_different_from_WhWhWh
        geom_boxplot(varwidth = T) +
        scale_y_log10() + 
        xlab("Data Type") + 
        ylab("Temporal Extent of Study (years)") + 
        annotate("text", x = 3, y = 2600, 
                 label = "paste(\"Adjusted \", 
                 italic(R) ^ 2, \" = ???\")", 
                parse = TRUE) + 
  annotate("text", x = 3, y = 1200, 
           label = "paste(\"Overall model significance \", 
                 italic(p), \" = ??\")", 
           parse = TRUE) +
        theme_bw() + 
        scale_color_discrete(name = "Significantly Different from\nWhat, Where, When-only\nstudies (alpha = 0.1 level)") + 
        theme(axis.text.x = element_text(angle = 45, vjust = 0.9, hjust = 1)) 
)
```

```{r time_extent_results_plot_for_presentation_slide, eval = F, include = F}
## This code is to be run in the console for making plot to be exported as jpg for slides
## TODO: need to hand-annotate R2 and p-value in plot
t_size = 22 # plot text size
ggplot(data = temp_extent, aes(y = temp_extent, x = data_type)) + #, color = sig_different_from_WhWhWh
  geom_boxplot(varwidth = T) +
  scale_y_log10() + 
  xlab("Data Type") + 
  ylab("Temporal Extent of Study (years)") + 
  # annotate("text", x = 3, y = 140, 
  #          label = "paste(\"Adjusted \", 
  #                italic(R) ^ 2, \" = 0.27\")", 
  #          parse = TRUE, 
  #          size = t_size - 15) + 
  annotate("text", x = 3, y = 1400, 
           label = "paste(\"Overall model significance \", 
                 italic(p), \" = 0.012\")", 
           parse = TRUE, 
           size = t_size - 14) +
  theme_bw() + 
  scale_color_discrete(name = "Significantly Different from\nWhat, Where, When-only\nstudies (alpha = 0.1 level)") + 
  theme(axis.text.x = element_text(angle = 330, vjust = 1, hjust = 0), 
        text = element_text(size = t_size)) 
```

-----------------------------------------------

```{r paradigm_table_caption}
paradigm_table_caption <- tabs("paradigm_table_caption", 
                               paste0("The number of studies published between 2014 and 2018 addressing different broad types of questions using biological records data.  The 'other' category includes methodology studies and other studies that did not focus specifically on questions about individual species or communities."))
```

```{r paradigm_plot_cap}
paradigm_plot_cap <- figs("paradigm_plot_cap", 
                          "The number of studies published between 2014 and 2018 addressing different broad types of questions using biological records data.  The 'other' category includes methodology studies and other studies that did not focus specifically on questions about individual species or communities.  Vertical lines show 95% bootstrap confidence intervals.")
```

## Study Question Paradigm

### What is the most popular study question paradigm?

$H_a$: Some broad study question paradigms are more common than others in studies using biological records.

$H_0$: The number of studies pursuing each of the broad study question paradigms is the same.

The number of studies asking questions about individual species (`r paradigm_list$individual_sp$ci_obj$t0` studies, 95% bootstrap CI [`r paradigm_list$individual_sp$ci_obj$bca[4]`, `r paradigm_list$individual_sp$ci_obj$bca[5]`]) was significantly higher than the number of studies asking questions about communities (`r paradigm_list$community$ci_obj$t0` studies, 95% bootstrap CI [`r paradigm_list$community$ci_obj$bca[4]`, `r paradigm_list$community$ci_obj$bca[5]`]) or asking other types of questions (`r paradigm_list$other$ci_obj$t0` studies, 95% bootstrap CI [`r paradigm_list$other$ci_obj$bca[4]`, `r paradigm_list$other$ci_obj$bca[5]`]) (`r tabs("paradigm_table_caption", display = "cite")`, `r figs("paradigm_plot_cap", display = "cite")`). 

`r tabs("paradigm_table_caption", display = "full")`

```{r print_study_question_paradigm_table}
paradigm_count_df <- data.frame(matrix(nrow = length(paradigm_list), 
                                        ncol = 4))
colnames(paradigm_count_df) <- c("paradigm", "n_studies", "l_95ci", "u_95ci")

# fill paradigm counts data frame using the list of paradigm bootstrap results
for(i in 1:length(paradigm_list)) {
  paradigm_count_df[i, "paradigm"] <- names(paradigm_list)[i]
  paradigm_count_df[i, "n_studies"] <- paradigm_list[[i]]$ci_obj$t0
  paradigm_count_df[i, "l_95ci"] <- paradigm_list[[i]]$ci_obj$bca[4]
  paradigm_count_df[i, "u_95ci"] <- paradigm_list[[i]]$ci_obj$bca[5]
}

kable(paradigm_count_df, col.names = c("broad type of study question", 
                                       "number of studies", 
                                       "lower 95% bootstrap CI", 
                                       "upper 95% bootstrap CI"))
```

```{r plot_paradigm_counts, fig.cap=paradigm_plot_cap}
# plot number of studies in each paradigm, with 95% CIs
print(ggplot(data = paradigm_count_df, 
             aes(x = factor(paradigm, 
                            levels = c("individual_sp", 
                                       "community", 
                                       "other"), 
                            labels = c("individual species questions", 
                                       "community questions", 
                                       "other types of questions")), 
                 y = n_studies)) + 
        geom_point() + 
        geom_linerange(aes(x = factor(paradigm, 
                            levels = c("individual_sp", 
                                       "community", 
                                       "other"), 
                            labels = c("individual species questions", 
                                       "community questions", 
                                       "other types of questions")), 
                      ymin = l_95ci, 
                      ymax = u_95ci)) + 
        xlab(element_blank()) + 
        ylab("Number of Studies") + 
        theme_bw())
```

## Specific Study Focus

```{r specific_question_by_paradigm_bar_plot}
qt_df <- select(wg, title, 
                community.question, individual.species.question, 
                response.variable...species.richness, 
                response.variable...diversity, 
                response.variable...distribution, 
                response.variable...abundance, response.variable...phenology,
                trends.over.time, methodology.development.or.analysis) 
names(qt_df)[4:10] <- c("species.richness", "diversity", 
                        "distribution", "abundance", "phenology", 
                        "temporal.trends", "methodology")

qt_df <- gather(qt_df, key = "paradigm", value = "parad_val", 
                community.question, individual.species.question,  
                factor_key = T) %>%
  filter(parad_val == TRUE) %>%
  gather(key = "focus", value = "foc_val", 
         species.richness, diversity, distribution, abundance, phenology, 
         temporal.trends, methodology, 
         factor_key = T) %>%
  filter(foc_val == TRUE) %>%
  group_by(paradigm, focus) %>%
  summarise(count = n()) %>% 
  # filter odd combinations of paradigm and focus
  filter(!(paradigm == "community.question" & 
             focus %in% c("distribution", "abundance", "phenology"))) %>%
  filter(!(paradigm == "individual.species.question" & 
             focus %in% c("species.richness", "diversity"))) 

ggplot(data = qt_df, 
       aes(x = factor(paradigm, 
                      levels = c("community.question", 
                                 "individual.species.question"), 
                      labels = c("community", "individual species")), 
           y = count, 
           fill = factor(focus, 
                      levels = c("species.richness", "diversity", 
                                 "distribution", "phenology", "abundance", 
                                 "temporal.trends", "methodology"), 
                      labels = c("species richness", "diversity", 
                                 "distribution", "phenology", "abundance", 
                                 "temporal trends", "methodology")))) + 
  geom_bar(position = "dodge", stat = "identity") + 
  scale_fill_brewer(type = "qual", palette = 2) + 
  #scale_fill_grey(end = 0.7) + 
  xlab("Study Focus") + 
  ylab("Number of Studies") + 
  theme_bw() + 
  theme(legend.title = element_blank())
```


[ Descriptive stats for studies focusing on alien species, testing ecological theory, and testing methods of data correction. ]

### What data types are used for each study question paradigm?

```{r make_study_question_paradigm_by_dataType_tibble, eval = F, include = F}
## I don't think a table is a good way to present this.  Too hard to see.
## But if I include this again later, should add all data type variables.
paradigm_data_df <- select(wg, title, individual.species.question, 
                      community.question, data.type...what.where.when.only, 
                      data.structure...sampling.effort.known, 
                      data.structure...non.detection, 
                      data.type...abundance) %>%
  gather(key = "paradigm", value = "parad_used", individual.species.question:
           community.question) %>%
  filter(parad_used == TRUE) %>%
  gather(key = "data_type", value = "dat_used", 
         data.type...what.where.when.only:data.type...abundance) %>%
  filter(dat_used == TRUE)

paradigm_data_df$data_type <- gsub("data.type...what.where.when.only", 
                                   "what, where, when only", 
                                   paradigm_data_df$data_type)
paradigm_data_df$data_type <- gsub("data.type...sampling.effort.known", 
                                   "sampling effort", 
                                   paradigm_data_df$data_type)
paradigm_data_df$data_type <- gsub("data.structure...non.detection", 
                                   "detection / non-detection", 
                                   paradigm_data_df$data_type)
paradigm_data_df$data_type <- gsub("data.type...abundance", 
                                   "abundance", 
                                   paradigm_data_df$data_type)
paradigm_data_df$paradigm <- gsub("community.question", 
                                   "community", 
                                   paradigm_data_df$paradigm)
paradigm_data_df$paradigm <- gsub("individual.species.question", 
                                   "individual species", 
                                   paradigm_data_df$paradigm)

kable(table(paradigm_data_df$data_type, paradigm_data_df$paradigm))
```


```{r data_type_by_paradigm_barplot}
### plot data type by study question paradigm ----------------------
dt_pd_df <- select(wg, data.type...what.where.when.only, 
      data.structure...sampling.effort.known, 
      data.type...abundance, 
      data.structure...non.detection, 
      data.structure...organized.data.collection.scheme, 
      data.structure...multiple.datasets.integrated.for.analysis, 
      data.type...life.stage, 
      data.type...voucher.of.some.kind.necessary.for.analysis,
      community.question, 
      individual.species.question) %>%
  gather(key = "data_type", value = "dt_val", data.type...what.where.when.only:
           data.type...voucher.of.some.kind.necessary.for.analysis) %>%
  filter(dt_val == TRUE) %>%
  gather(key = "paradigm", value = "pd_val", community.question, 
         individual.species.question) %>%
  filter(pd_val == TRUE) %>%
  count(paradigm, data_type) %>%
  complete(paradigm, data_type, fill = list(n = 0))

ggplot(dt_pd_df, 
       aes(x = factor(paradigm, 
                      levels = c("community.question", 
                                 "individual.species.question"), 
                      labels = c("community", "individual species")), 
           y = n, 
           fill = factor(data_type, 
                         levels = c("data.type...what.where.when.only", 
                           "data.structure...sampling.effort.known", 
                           "data.type...abundance", 
                           "data.structure...non.detection", 
                           "data.structure...organized.data.collection.scheme", 
                           "data.structure...multiple.datasets.integrated.for.analysis",
                           "data.type...life.stage", 
                           "data.type...voucher.of.some.kind.necessary.for.analysis"), 
                         labels = c("what where when only", 
                                    "sampling effort", 
                                    "abundance", 
                                    "non detection", 
                                    "organized scheme", 
                                    "multiple datasets", 
                                    "life stage", 
                                    "voucher available")))) + 
  geom_bar(position = position_dodge(preserve = "single"), stat = "identity") + 
  ggtitle("Data use based on study question paradigm") + 
  scale_fill_discrete(name = "Data Type") + 
  xlab("Study question paradigm") + 
  ylab("Number of studies") + 
  scale_y_continuous(breaks = seq(0, max(dt_pd_df$n), 5)) + 
  theme_bw() 
```

**Jon - Is there any useful statistical work to be done here? TODO: Yes, fit full model w/interaction and w/out interactin to see if distribution of bars is different in community v. individual sp studies.**  I think the story is that data use is roughly similar between question paradigms, except maybe that voucher data seems to be used more often in individual species studies.  Also integrating multiple datasets for analyses is common in both question paradigms, and so it is worth making an effort to ensure that published datasets are in standardized, compatible formats.  

## Analysis approach & data type
[**TODO**]

_____________________________________________________________________
# Discussion

## Reader Agreement
The seemingly poor agreement between WG's and CP's codings either reflects true ambiguity in how to code the variables or errors by one of the readers.  My review of all cases of disagreement revealed many cases that I believe to be obvious errors or oversights.  Of the obviously erroneous values, most were CP's, though some were mine.  This suggest that either I have given poor instructions to CP or CP is carrying out the instructions poorly.  It is probably not possible to figure out which of these things is happening without having a third reader, and preferable one with a higher skill level.  In some cases, my instructions were clearly insufficient.  I have now edited the instructions and hopefully solved those problems for the 12 variables CP double coded (though similar inadequate instructions probably still exist for the other variables).  The best test now would be to test these same 12 variables with another reader with a higher level of ability.  If that reader also has substantial disagreement, then either the categories themselves or the instructions will need to be majorly revised.  If that $3^{rd}$ higher-ability reader agrees with my original codings better, then it indicates that a highly skilled reader is required for the second readings.  

The technique of having me do a "tie breaker" evaluation of all disagreement cases is a partial solution.  This method would basically treat the $2^{nd}$ reader's codings as a check for error's in my coding that trigger a tie breaker review.  Cases in which the $2^{nd}$ readers coding is judged to be obviously erroneous would not be of much concern, and would be attributed to insufficient knowledge (e.g. not knowing that "fungarium" is a collection of physical specimens) or to skimming the articles quickly rather than reading them (CP seemed to spend less than 30 minutes per article).  Cases in which my original coding is judged to be obviously erroneous would be of concern.  Those values could be corrected for analysis, and/or the overall reliability of my original codings would need to be evaluated in some way to determine if a $2^{nd}$ check is needed for all studies and variables or whether my errors are few enough that they introduce an acceptable amount of error/uncertainty into the analysis. The ideal soluttion would be to have a highly-skilled second reader and to calculate Krippendorff's alpha for agreement between the two readers.  However, lacking such a person, I think it would be acceptable to use a less skilled second reader as a trigger for a 3rd tie breaker review done by me.  I do need to decide whether the trigger/tie breaker approach requires checking all studies and variables, or whether there is an adequate way to estimate the reliability of my original codings. I am not sure whether Ellie would be knowledgable enough to be a highly-skilled second reader.  Perhaps not. 

A final solution that is perhaps worth pursuing is only coding and analyzing variables for which coding can be done based entirely or largely on character string matching.  This would provide greater reliability, but at the cost of less interesting, informative, and insightful analysis (Krippendorff 2004, Section 11.1).

There are two goals of having two readers.  First, to estimate how replicatable the analysis is.  If another researcher could code the articles in the same way I do, then it provides confidence that the analysis is revealing some sort of true characteristics of the literature.  Second, to improve the quality of this specific analysis by identifying and correcting sloppy errors on my part.  This is not necessarily replicatable, though, as I am doing the correcting, so the categories basically still show "Willson's inner vision of the literature" rather than some more widely true characteristics. 

## Variables to abandon

* bio recs as predictor (keep, but wg only - horizon scanning)
* bio recs as response (keep, but wg only - horizon scanning)
* bio recs in facilitative role?  (This is more interesting, but hard to code.  wg will code this as a "horizon scanning" type variable to keep track of examples of interesting things, but only clear cases will be discussed in article.  No need for double checking this because it won't be analyzed in any specific way.)


_________________________________________________________________________

# Old versions below here


--------------------------------------------------------------------

## Results format by data type
$H_01$: Different types of biological records data are not analyzed with different broad analysis approaches.

$H_a1$: Different types of biological records data are analyzed with different broad analysis approaches.

$H_02$: wh,wh,wh-only data is not analyzed with a different broad analysis approach than other data types.

$H_a2$: wh,wh,wh-only data is analyzed with inference and/or prediction less often than richer data are.  

$H_03$: The ?rate? at which wh,wh,wh-only data is analyzed with prediction is not greater than for other data types.

$H_a3$: The ?rate? at which wh,wh,wh-only data is analyzed with prediction is greater than for richer data types.



# References







