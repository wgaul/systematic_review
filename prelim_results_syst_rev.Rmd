---
title: "What have biological records ever done for us?  A systematic scoping review - Code for plots and quantitative results"
date: 23 April 2020
output:
  html_document:
    number_sections: true
    df_print: kable
  word_document:
    toc: true
    toc_depth: 4
  pdf_document: default
  fig_caption: yes
---

<div style="line-height: 2em;">

```{r, echo=F}
knitr::opts_chunk$set(echo=F, message=F, warning=T, cache=F)
```

```{r, load_data, cache = F, results='hide', warning = F}
devtools::install_github("wgaul/wgutil") # install the wgutil package from github
library(wgutil)
library(Hmisc)
library(boot)
library(irr)
library(ResourceSelection)
library(captioner)
library(pander)
library(knitr)
library(ggridges)
library(bestglm)
library(Gifi)
library(MASS)
library(car)
library(GGally)
library(tidyverse)
library(ggrepel)

diag <- F # produce exploratory & diagnostics plots & results
diag_present <- T # produce diagnostics for report to Dina
plot_text_size <- 15

if(diag == F) { # turn off warnings when not running diagnostics
  knitr::opts_chunk$set(echo=F, message=F, warning=F, cache=F)
}

set.seed(18012020) # 18 Jan 2020 

source("./clean_data_syst_rev.R")
```

Willson Gaul, Dinara Sadykova, Ellie Roark, Hannah J. White, Lupe León-Sánchez, Paul Caplat, Mark C. Emmerson, Jon M. Yearsley

This R markdown document contains code that produces the plots, tables, and quantitative results reported in Gaul et al. (2020) "What have biological records ever done for us?  A systematic scoping review".  The actual text of the methods and results is not identical to the text of the published paper, but the numbers produced by the R code in this document are the same as those in the final paper.  

In order to run, this document must be saved in a directory containing the scripts "clean_data_syst_rev.R" and "combine_search_results.R" and the data files "master_eligibility_results.csv", "wg_systematic_review_coding.csv", and "ellie_systematic_review_coding_04.csv", plus another sub-directory called "search_results" that contains csv files with results from multiple different search engines (see the "combine_search_results.R" script for names of all the search results data files that are expected).  

Data are available from: https://doi.org/10.5281/zenodo.3760678

# Methods

```{r define_boot_function}
# bootstrap ci function for use throught this script
get_boot_CI <- function(x, fun_to_use, n_perm) {
  b_o <- boot(x, fun_to_use, R = n_perm, stype = "i")
  ci_o <- tryCatch(boot.ci(b_o, conf = 0.95, type = "bca"), 
                   error = function(x) NA)
  to_return <- list(obs = x, 
                    boot_obj = b_o, 
                    ci_obj = ci_o)
  to_return
}

# set number of permutations to perform for all bootstrap and randomization tests in this script
n_perm <- 10000 
```

We did a systematic review of original research published since 2014 that used biological records from Ireland and/or the United Kingdom (UK).  We generated a list of relevant articles from Web of Science, Scopus, ProQuest, Global Biodiversity Information Facility (GBIF) website, and GoogleScholar via Publish or Perish software (see Appendix 2 for search terms).  One researcher evaluated each study for inclusion eligibility and coded information for variables describing characteristics of each study.  A second reader coded information for a subset of 20% of the studies.  Agreement between the two readers was evaluated using Krippendorff's alpha (Krippendorff 2004).  Coded variables that did not meet minimum standards for reader agreement were not used in subsequent analyses.  Analyses were conducted in R version 3.5.

## Study eligibility
Studies were deemed eligible if they presented original research (no reviews or idea papers) published in the English language, used opportunistic biological records data collected with non-standardized or semi-standardized designs, included (but were not necessarily limited to) data from Ireland or the UK, and the full text of the study was available through the University College Dublin library, Google Scholar, Google search results, or ResearchGate.  Grey literature was included and therefore peer review was not required.  Studies using semi-standardized data collection schemes (e.g. UK Butterfly Monitoring Scheme) were included as long as data collection included some opportunistic elements (e.g. sampling locations chosen opportunistically).  Publications of data (e.g. atlases or data papers) were not considered eligible unless they included analysis of the data.  Only studies using a sample size of greater than 20 biological records were included; this sample size was chosen arbitrarily, mainly to exclude studies in which re-examination of museum specimens resulted in a taxonomic identification revision for one or a few specimens.  Studies using museum data were considered eligible when the museum data used was similar in format to biological records data (e.g. “what, where, when” data); studies that used museum specimens only for taxonomic, genetic or morphology studies were excluded.  Studies using only fossil records were not included.  Studies using data from phenology networks were included; for the purposes of this review such data were considered biological records data with associated additional data (e.g. flowering status of plants).  Studies for which all data was collected by the study authors were not considered biological records data for the purposes of this review and were excluded.  The minimum required information in the data was a taxonomic name, a location, and date (“what, where, when”); additional information was permissible.  

## Article coding

The variables that readers coded for each study are described in Appendix 1 *Instructions for Coders*.  Here we provide a brief summary of the broad types of information we coded.

### Data Type
We coded thirteen binary variables describing the structure and type of biological records data used by studies (Appendix 1).  The data type categories were not mutually exclusive.  For statistical analyses, we included data type variables in models based on *a-priori* expectations about the variables' influence on data analysis methods and based on expectations about different levels of complexity and effort involved in collecting, recording, and storing data.  

### Study Questions
We described the main ecological or biological questions that each study investigated in terms of whether the study was asking questions about individual species, ecological communities, or about methodology. We also identified whether studies analyzed each of the following biological or ecological responses: species richness; species turnover (beta-diversity); species distributions; abundance; or phenology.  We recorded the taxonomic group that was the focus of each study.

### Analysis approach
We classified the broad analysis approach used by each study into one of three categories: inference; prediction; or description.  For each study, we evaluated the analysis approach only for analyses that used biological records data.  We considered studies estimating parameters and reporting some measure of uncertainty (e.g. *p*-values, confidence intervals, posterior probability distributions) to be using inference.  We considered studies that used models to make predictions to be using an analysis strategy of prediction.  Finally, we considered the analysis approach to be descriptive only if results were descriptive with no prediction or inference.  Descriptive results could be narrative, graphical, or quantitative as long as no inference or uncertainty was reported (e.g. point estimates of descriptive statistics without any confidence interval or *p*-values).  Studies could use both inference and prediction as analysis strategies and so those categories were not mutually exclusive, but studies were only categorized as using a descriptive analysis approach if all results were descriptive only.  

### Temporal and spatial extent
We determined the temporal extent covered by studies by determining the start and end dates of the study.  We determined the spatial extent of the study (in km^2) by recording the spatial extent reported in the study if a quantitative spatial extent was reported, or, when spatial extents were reported only qualitatively, by using the spatial areas of political units (e.g. countries) or geographic regions reported in Google search results (www.google.com), Wikipedia (en.wikipedia.org), or from Eakins and Sharman (2010).

### Coding reliability
We used Krippendorff's *alpha* to evaluate the agreement between two readers for each of `r ncol(er)-6` variables that were coded by two people.  Krippendorff's alpha measures agreement between multiple coders while accounting for agrement by chance.  We calculated Krippendorff's *alpha* using the `kripp.alpha` function in the `irr` package.  Rule-of-thumb guidelines suggest that Krippendorff's alpha values below 0.67 indicate poor agreement such that the data cannot be trusted for analysis; values between 0.67 and 0.8 suggest that the data are moderately reliable and may be suitable for only tentative interpretation, and values above 0.8 indicate that the data are reliable.  In order to estimate reliability using Krippendorff's alpha, there must be sufficient variation in the variable being coded.  The amount of data needed to estimate alpha for a categorical variable depends on the probability of the least common class for that variable. Krippendorff provides guidelines about the number of coded cases needed to estimate the value of Krippendorff's alpha given the probability of the least common class for a variable.  We examined Krippendorff's alpha values only for variables for which the least common class had a probability of 0.14 or higher, which should allow us to identify with a confidence level of 0.1 those variables for which Krippendorff's alpha is above 0.667.  We removed from subsequent statistical analyses all variables with Krippendorff's alpha values below 0.67.  We also did not perform statistical analyses on variables for which we did not have enough codings to estimate Krippendorff's alpha, but we did present descriptive results for these variables because the rare classes of these variables are of interest from a horizon-scanning perspective. 

```{r inter_coder_agreement_subset_dfs, warning=TRUE}
## ellie's results ----------------------------------------------------
er <- er[order(er$title), ] # order rows
er <- er[, order(colnames(er))] # order columns
# put title column first
er <- er[, c(which(colnames(er) == "title"), 
                       which(colnames(er) != "title"))]
## end reorder ellie's results ------------------------------------------------

## reorder my results  --------------------------------------------------------
# subset my coded results to titles and columns coded by ellie
wg_dbl_cd <- wg[which(wg$title %in% er$title), which(colnames(wg) %in% colnames(er))]
# order rows
wg_dbl_cd <- wg_dbl_cd[order(wg_dbl_cd$title), ] 
# order cols
wg_dbl_cd <- wg_dbl_cd[, order(colnames(wg_dbl_cd))] 
wg_dbl_cd <- wg_dbl_cd[, c(which(colnames(wg_dbl_cd) == "title"), 
             which(colnames(wg_dbl_cd) != "title"))] # put title column first
## end reorder my results  ----------------------------------------------------

## make sure all titles are present
if(any(wg_dbl_cd$title %nin% er$title) | 
   any(er$title %nin% wg_dbl_cd$title)) {
  warning("er and wg_dbl_cd don't have the same titles. One or both of those data frames will be subsetted.")
  wg_dbl_cd <- wg_dbl_cd[which(wg_dbl_cd$title %in% er$title), ]
  er <- er[which(er$title %in% wg_dbl_cd$title), ]
}

## make sure df rows and cols are in the same order
if(!identical(colnames(er), colnames(wg_dbl_cd))) {
  stop("Columns in er and wg_dbl_cd must be identical and in the same order. They must be in the same order for creating the variable data frames that will be used to calculate Krippendorf's alpha. ")
}
```

```{r make_dfs_of_double_coded_variables_all_studies}
# each row is codings from one person
# each column is a study
dbl_coded_list <- list()
if(colnames(er)[1] != "title" | colnames(wg_dbl_cd)[1] != "title") {
  stop("The first column of er and wg must be the study title")}

for(i in 2:ncol(er)) {
  dbl_coded_list[[i-1]] <- data.frame(matrix(
    data = c(er[, i], wg_dbl_cd[, i]), 
    nrow = 2, ncol = nrow(er), byrow = T))
}
names(dbl_coded_list) <- colnames(er)[2:ncol(er)]
```

```{r krippendorfs_alpha_calc, warning=FALSE, message=FALSE}
do_kripp.alpha <- function(x, method) {
  # function to call kripp.alpha while converting dataframe x to a matrix
  # ARGS: x - data frame in which rows are observers and columns are studies
  #       method - character string of method to pass to kripp.alpha
  kripp.alpha(as.matrix(x), method = method)
}

warning("WG says: the kripp.alpha function makes a 'dv' object that is 'data values' but when values are character or logical (as in my case) this producees NAs during coercion.  However, that 'dv' object is only used if the method is set to interval', so in my case the NAs that the warning is about are never used because I am using the nominal method.  So the warnings are nothing to worry about as long as method is 'nominal'.")
kripp_all_dbl_coded <- lapply(dbl_coded_list, FUN = do_kripp.alpha, 
                              method = "nominal")
```


## Temporal and spatial extent of studies

We expected the temporal and spatial extent covered by studies to be smaller for studies using more structured or "rich" data types compared to studies using less structured data.  We tested whether the temporal extent of studies differed based on the data type by fitting a linear regression with the natural-log transformed temporal extent of studies (in years) as the response variable.  Of the four binary variables we coded indicating characteristics of the data, the three variables indicating structured data (sampling effort, non-detection, and data from an organized monitoring scheme) were correlated (Fig. S??).  We therefore summarized the predictor variables into two dimensions using multiple correspondence analysis (Mair & de Leeuw 2019) and used those two dimensions as predictors in the linear regression.  We assessed the significance of data types for predicting temporal extent by using a likelihood ratio test to compare a model containing the two MCA dimensions as predictors to an intercept only model.  Because the MCA dimensions were clearly interpretable in terms of data type, we then assessed the significance of the data types by comparing the model with all main effects to a reduceded models in which the MCA variables representing data types had been alternately removed.

We assessed whether the mean spatial extent was different for studies using different types of data.  We assessed differences between means by determining whether 95% bias-corrected accelerated bootstrap confidence intervals (Efron & Tibshirani 1993)  overlapped.  Bootstrap confidence intervals were calculated using the "boot" package in R (Canty & Ripley, 2015).

```{r prepare_spatial_extent_df}
spat_extent <- select(
  wg, c(title, spatial.extent.of.study, 
        data.structure...sampling.effort.known, 
        data.structure...non.detection,
        data.structure...organized.data.collection.scheme, 
        data.structure...multiple.datasets.integrated.for.analysis)) %>%
  gather(key = "data_type", value = "dt_value", 
         data.structure...sampling.effort.known:data.structure...multiple.datasets.integrated.for.analysis, 
         factor_key = T) %>%
  filter(dt_value == T) %>%
  group_by(data_type)

# make names prettier
spat_extent$data_type <- gsub("data.structure...", "", spat_extent$data_type)

# remove km^2 text
spat_extent$spatial.extent.of.study <- gsub(" km\\^2", "", 
                                            spat_extent$spatial.extent.of.study)
spat_extent$spatial.extent.of.study <- as.numeric(as.character(
  spat_extent$spatial.extent.of.study))
```

```{r spat_extent_randomization}
# make spatial extents numeric
wg$spat_extent <- gsub(" km\\^2", "", wg$spatial.extent.of.study)
wg$spat_extent <- as.numeric(as.character(wg$spat_extent))

mean_spat_extent_fun <- function(x, indices, var_name) {
  # ARGS: x - a dataframe with a column of spatial extents and a column 
  #           of T/F values for each data type
  #       indices - a vector of indices as required by boot
  #       var_name - character string with the column name of the data type
  #           for which the median spatial extent should be calculated
  x_b <- x[indices, ]
  mean(as.numeric(x_b$spat_extent[x_b[ , colnames(x_b) == var_name] == TRUE]), 
         na.rm = TRUE)
}

# bootstrap ci function for getting median spatial extent
call_boot_spat_extent <- function(x, fun_to_use, n_perm, var_name) {
  b_o <- boot(x, fun_to_use, R = n_perm, stype = "i", var_name = var_name)
  ci_o <- tryCatch(boot.ci(b_o, conf = 0.95, type = "bca"), 
                   error = function(x) NA)
  to_return <- list(obs = x, 
                    boot_obj = b_o, 
                    ci_obj = ci_o)
  to_return
}

# make list of data types for which median spatial extent is desired
data_types <- list(
  sampling.effort = "data.structure...sampling.effort.known", 
  non.detection = "data.structure...non.detection",
  organized.scheme = "data.structure...organized.data.collection.scheme", 
  multiple.datasets = "data.structure...multiple.datasets.integrated.for.analysis")

spat_ext_data_type_CIs <- lapply(
  data_types, 
  FUN = function(dt, wg, n_perm) {
    call_boot_spat_extent(x = wg, fun_to_use = mean_spat_extent_fun, 
                          n_perm = n_perm, var_name = dt)
  }, wg = wg, n_perm = n_perm)
```

```{r prepare_temporal_extent_df}
# only use data "structures" (not "data types") as most "data type" variables
# could not have Krippendorff's alpha estimated.  "Data structure" and "data
# type" variables are not very comparable in the same analysis because "data
# structure" was asking whether that data type was available, whether or not it
# was used, while "data type" was asking whether the data type was actually used
# for the analysis.  
temp_extent_plot_df <- select(
  wg, c(title, temp_extent, 
        data.structure...sampling.effort.known, 
        data.structure...non.detection,
        data.structure...organized.data.collection.scheme, 
        data.structure...multiple.datasets.integrated.for.analysis)) %>%
  gather(key = "data_type", value = "dt_value", 
         data.structure...sampling.effort.known:data.structure...multiple.datasets.integrated.for.analysis, 
         factor_key = T) %>%
  filter(dt_value == T) %>%
  group_by(data_type)

# make names prettier
temp_extent_plot_df$data_type <- gsub("data.structure...", "", temp_extent_plot_df$data_type)

temp_extent_plot_df$data_type <- factor(as.character(temp_extent_plot_df$data_type), 
                                levels = c("sampling.effort.known",
                                           "non.detection", 
                                           "organized.data.collection.scheme",                                        "multiple.datasets.integrated.for.analysis"), 
                                labels = c("sampling effort known",
                                           "non detection", 
                                           "organized scheme", 
                                           "multiple datasets"))
```

```{r fit_temp_extent_lm}
## fit linear regression with log-transformed years as response variable
## and each data type as a separate variable 
# fit full model
wg$log_years <- log(wg$temp_extent)
# variables added based on a-priori expectation of most to least important
time_lm_full <- lm(log_years ~ 1 + 
                     data.structure...sampling.effort.known +
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.structure...multiple.datasets.integrated.for.analysis,
                   data = wg[!is.na(wg$log_years), ], 
                   na.action = na.exclude, y = TRUE) 

time_resids <- rstandard(time_lm_full) # calculate residuals

time_summary <- summary(time_lm_full)
temp_extent_confint <- confint(time_lm_full)

# Test significance of full model
m0 <- lm(log_years ~ 1, data = wg[!is.na(wg$log_years), ], y = T) # fit null model 
m_time_full_anova <- anova(m0, time_lm_full)

## to interpret outcome on original scale:
# for a one-unit increase in x, the predicted value of y 
# changes by (e^beta1 - 1)*100 percent
```

```{r time_data_type_test_main_effects, include = F}
## test effect of each data type predictor by comparing the full model to a
# model with that predictor removed
m_no_effort <- lm(log_years ~ 1 + 
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme + 
                     data.structure...multiple.datasets.integrated.for.analysis,
                  data = wg[!is.na(wg$log_years), ])
m_no_nonDet <- lm(log_years ~ 1 + 
                     data.structure...sampling.effort.known + 
                     data.structure...organized.data.collection.scheme + 
                     data.structure...multiple.datasets.integrated.for.analysis,
                  data = wg[!is.na(wg$log_years), ])
m_no_scheme <- lm(log_years ~ 1 + 
                     data.structure...sampling.effort.known + 
                     data.structure...non.detection + 
                     data.structure...multiple.datasets.integrated.for.analysis,
                  data = wg[!is.na(wg$log_years), ])
m_no_multData <- lm(log_years ~ 1 + 
                     data.structure...sampling.effort.known + 
                     data.structure...non.detection + 
                     data.structure...organized.data.collection.scheme,
                    data = wg[!is.na(wg$log_years), ])

indiv_mods_time <- list("m_no_effort" = m_no_effort, "m_no_nonDet" = m_no_nonDet, 
                        "m_no_scheme" = m_no_scheme, 
                        "m_no_multData" = m_no_multData)
```

```{r time_MCA}
temp_extent <- wg[!is.na(wg$log_years), 
                  c("log_years", 
                    "data.structure...sampling.effort.known", 
                    "data.structure...non.detection", 
                    "data.structure...organized.data.collection.scheme", 
                    "data.structure...multiple.datasets.integrated.for.analysis")]
### Try CATPCA to summarize predictors before lm
time_hom <- homals(temp_extent[, 2:5], ndim = 2, ordinal = FALSE)

# put the two dimensions D1 and D2 on the original temp_extent data
temp_extent$D1 <- data.frame(time_hom$objectscores)$D1
temp_extent$D2 <- data.frame(time_hom$objectscores)$D2

## Make plot of loadings for dimensions
# get data frame of loadings of first 2 dimensions
mca_loadings_df <- data.frame(variable = c(names(time_hom$quantifications), 
                                           names(time_hom$quantifications)), 
                              value = c(T, T, T, T, F, F, F, F),
                              D1 = NA, D2 = NA)
for(i in 1:nrow(mca_loadings_df)) {
  mca_loadings_df[i, 3:4] <- as.numeric(
    time_hom$quantifications[[
      as.character(mca_loadings_df$variable)[i]]][as.character(
        mca_loadings_df$value[i]), ])
}
# clean variable names
mca_loadings_df$variable <- gsub("data.structure...", "", 
                                 mca_loadings_df$variable)
mca_loadings_df$paste <- paste(mca_loadings_df$variable, 
                               mca_loadings_df$value, sep = "_")
# graph loadings
mca_loadings_plot <- ggplot(data = mca_loadings_df, 
                            aes(x = D1, y = D2, color = variable, 
                                label = paste)) + 
  geom_point() + 
  scale_color_viridis_d(option = "magma", begin = 0.2, end = 0.8) + 
  xlab("Dimension 1") + ylab("Dimension 2") +
  theme_bw() + 
  theme(legend.position = "none")
mca_loadings_plot
```

```{r time_MCA_lm}
# Fit linear regression using the reduced dimensions as predictors and 
# log-transformed years as response variable
time_lm_log_catpca <- lm(log_years ~ 1 + D1 + D2,
                         data = temp_extent[!is.na(temp_extent$log_years), ], 
                         na.action = na.exclude, y = TRUE) 

#plot(time_lm_log_catpca, ask = F) # residuals look ok
mca_lm_summary <- summary(time_lm_log_catpca)

# Test significance of full model
mca_m0 <- lm(log_years ~ 1, data = temp_extent, y = T) # fit null model 
mca_time_full_anova <- anova(m0, time_lm_log_catpca)

# Test significance of each dimension
mca_noD1 <- lm(log_years ~ 1 + D2, 
               data = temp_extent[!is.na(temp_extent$log_years), ], 
                         na.action = na.exclude, y = TRUE)
mca_noD2 <- lm(log_years ~ 1 + D1, 
               data = temp_extent[!is.na(temp_extent$log_years), ], 
                         na.action = na.exclude, y = TRUE)

## VIF for the lm model of log(years) ~ data types 
temp_extent_catpca_vif <- vif(time_lm_log_catpca) # assess multicollinearity
temp_catpca_vif_df <- data.frame(temp_extent_catpca_vif)
colnames(temp_catpca_vif_df)[1] <- "VIF"
temp_catpca_vif_df$variable <- rownames(temp_catpca_vif_df)
rownames(temp_catpca_vif_df) <- NULL
# temp_catpca_vif_df # low VIF.  
```


## What is the most popular study question paradigm for studies using biological records?

We categorized study questions into two broad question paradigms: individual species questions and community questions [Appendix 2].  We estimated the proportion of studies that focused on each of the two broad study question paradigms, and used bias-corrected accelerated bootstrap confidence intervals to estimate 95% confidence intervals around the point estimates of the proportions.  

```{r paradigm_bootstrap}
proportion_fun <- function(x, indices) {
  # ARGS: x - a vector of TRUE/FALSE values, for which the TRUEs will be counted
  #       indices - a vector of indices as required by boot
  x_b <- x[indices]
  sum(as.numeric(x_b), na.rm = TRUE)/length(x_b)
}

count_fun <- function(x, indices) {
  # ARGS: x - a vector of TRUE/FALSE values, for which the TRUEs will be counted
  #       indices - a vector of indices as required by boot
  x_b <- x[indices]
  sum(as.numeric(x_b), na.rm = TRUE)
}

proportion_individual <- get_boot_CI(wg$individual.species.question,
                                     fun_to_use = proportion_fun, 
                                     n_perm = n_perm)
```

## Ecological focus

We assessed which ecological or biological phenomena were used as the response variable in analyses using biological records.  We coded each study based on whether it used the following response variables: species distribution; species abundance; phenology, species richness (*alpha*-diversity or the number of species present); species turnover (*beta*-diversity or any measure that incorporated species identity and/or abundance in addition to the number of species); or temporal trends.  A study could use more than one of these response variables if it contained more than one analysis using biological records, or if it studied temporal trends in one of the other response variables (e.g. change in species distribution over time). After coding, we created an "other" response variable category which was coded as "TRUE" for studies that did not use any of the other biological or ecological response variables that we searched for during coding.  We used bias-corrected accelerated bootstrap confidence intervals with 10,000 bootstrap replicates to estimate 95% confidence intervals around the number of studies investigating each response variable. We determined statistical significance of differences in the number of studies investigating each type of response by assessing whether the 95% bootstrap confidence intervals overlapped.

```{r make_other_response_variable_columns}
# make columns indicating which studies analyze something that does not fit
# within the response variable categories I am using.  I do this by finding
# studies that do not have any of my response variables marked TRUE, which means
# they must have been doing something else.
resp_var_df <- select(wg, title, 
                      response.variable...species.richness,
                      response.variable...distribution, 
                      response.variable...abundance,
                      response.variable...phenology,
                      trends.over.time) 
# make column indicating individual species studies that don't use one of the 
# 4 main response variables
resp_var_df$other_resp <- 
  resp_var_df$response.variable...distribution == F & 
  resp_var_df$response.variable...abundance == F & 
  resp_var_df$response.variable...phenology == F  & 
  resp_var_df$response.variable...species.richness == F & 
  resp_var_df$trends.over.time == F
```

```{r boot_response_variable}
resp_var_list <- lapply(
  list(sp_rich = resp_var_df$response.variable...species.richness, 
       dist = resp_var_df$response.variable...distribution,
       abund = resp_var_df$response.variable...abundance, 
       phen = resp_var_df$response.variable...phenology, 
       temp_trends = resp_var_df$trends.over.time, 
       other = resp_var_df$other_resp),
  FUN = get_boot_CI, fun_to_use = proportion_fun, n_perm = n_perm)

resp_var_list_counts <- lapply(
  list(sp_rich = resp_var_df$response.variable...species.richness, 
       dist = resp_var_df$response.variable...distribution,
       abund = resp_var_df$response.variable...abundance, 
       phen = resp_var_df$response.variable...phenology, 
       temp_trends = resp_var_df$trends.over.time, 
       other = resp_var_df$other_resp),
  FUN = get_boot_CI, fun_to_use = count_fun, n_perm = n_perm)
```

## Development of methodology across different study questions

We asked whether methodological development is more active for some types of ecological questions than for others.  To test for differences in the proportion of methodology studies across the different biological and ecological study focuses, we used logistic regression with six binary predictor variables indicating the specific biological or ecological focus(es) of the study and a binary response variable indicating whether the study addressed a methodological question.  To evaluate whether assumptions were met for using logistic regression, we evaluated the number of samples per predictor variable, checked that all levels of predictor variables had more than five cases, checked for multicollinearity using the variance inflation factors, performed a Hosmer-Lemeshow goodness of fit test, and identified influential points using Pearson and deviance residuals and leverage.  

```{r methodology_by_study_focus_mod}
# make an "other" response variable / study focus column on wg data frame
wg$other_focus <- wg$response.variable...abundance == F & 
  wg$response.variable...distribution == F & 
  wg$response.variable...phenology == F & 
  wg$response.variable...species.richness == F & 
  wg$trends.over.time == F

# fit glm
method_mod <- glm(factor(as.character(methodology.development.or.analysis), 
                         levels = c("FALSE", "TRUE"), 
                         labels = c("FALSE", "TRUE")) ~ 1 +
                    factor(response.variable...species.richness) + 
                    factor(response.variable...distribution) + 
                    factor(response.variable...abundance) + 
                    factor(response.variable...phenology) + 
                    factor(trends.over.time) + 
                    factor(other_focus), 
                  data = wg, family = binomial())
method_mod_null <- glm(factor(as.character(methodology.development.or.analysis), 
                         levels = c("FALSE", "TRUE"), 
                         labels = c("FALSE", "TRUE")) ~ 1, 
                  data = wg, family = binomial())
```

```{r print_method_focus_model, include = F}
method_chisq <- anova(method_mod_null, method_mod, test = "Chisq")
```

```{r AIC_method_model}
# in response to reviewer comment, look at AIC of all models
# 27 July 2020
method_AIC <- stepAIC(method_mod, direction = "backward")
method_AIC$anova

AIC_test <- glmulti::glmulti(y = method_mod, 
                             level = 1, method = "h", 
                             family = binomial())
# AIC_test
AIC_test@crits
AIC_test@formulas[1] # best ranked model
AIC_test@crits[1]
AIC_test@formulas[26] # null model
AIC_test@crits[26]
AIC_test@crits[26] - AIC_test@crits[1] # delta AIC from best ranked to null
AIC_test@formulas[55] # full model
AIC_test@crits[55] - AIC_test@crits[1] # delta AIC from best ranked to full

# fit "best" model according to AIC
method_mod_best <- glm(
  factor(as.character(methodology.development.or.analysis), 
         levels = c("FALSE", "TRUE"), 
         labels = c("FALSE", "TRUE")) ~ 1 +
    factor(response.variable...distribution), 
  data = wg, family = binomial())

## look at AUC
auc_null <- pROC::roc(response = method_mod_null$y, 
                      predictor = method_mod_null$fitted.values)
plot(auc_null)
auc_null
auc_full <- pROC::roc(response = method_mod$y, 
                      predictor = method_mod$fitted.values)
plot(auc_full)
auc_full
auc_best <- pROC::roc(response = method_mod_best$y, 
                      predictor = method_mod_best$fitted.values)
plot(auc_best)
auc_best

## McFadden's pseudo-R2
# McFadden's R2 = 1 - (log(Lc) / log(Lnull))
# where Lc is the likelihood value from the current model and Lnull is the 
# likelihood from the null model

# pseudo R2 - full model
1 - (as.numeric(logLik(method_mod)) / as.numeric(logLik(method_mod_null)))
# pseudo R2 - "best" model
1 - (as.numeric(logLik(method_mod_best)) / as.numeric(logLik(method_mod_null)))
```

```{r methodological_rawData}
## make a table of raw data for methodological vs. study question
method_question_df <- data.frame(
  study_question = c("species.richness", "distribution", 
                     "response.variable...abundance", 
                     "phenology", "trends.over.time", "other_focus"), 
  methodological = NA, 
  not_methodological = NA)
for(i in 1:nrow(method_question_df)) {
  v <- which(grepl(as.character(method_question_df$study_question[i]), 
                   colnames(wg)))
  method_question_df$methodological[i] <- length(
    which(wg$methodology.development.or.analysis[wg[, v]] == T))
  method_question_df$not_methodological[i] <- length(
    which(wg$methodology.development.or.analysis[wg[, v]] == F))
}
```


```{r proportion_methodological_boot}
prop_method <- length(which(wg$methodology.development.or.analysis == TRUE)) / 
  nrow(wg)

# define function to pass to boot
prop_T <- function(x, index) {
  # ARGS: 
  # x: vector of TRUE/FALSE values, for which the proportion TRUE will be calculated
  # index - required by boot
  x <- x[index]
  length(which(x == TRUE)) / length(x)
}

prop_method_CI <- get_boot_CI(wg$methodology.development.or.analysis, 
                              fun_to_use = prop_T, n_perm = n_perm)
# prop_method_CI$boot_obj$t0
# prop_method_CI$ci_obj
```

## What data types are used for each study question? 

We asked whether different types of biological records data are used to study different types of questions.  We used random permutations to test the null hypothesis of no difference in the types of data used to study different biological or ecological questions.  The data type categories we tested based on *a-priori* expectations of importance were: sampling effort; non-detection; organized monitoring scheme; and multiple datasets integrated for analysis.  In the ranom permutation test, we broke the association between data type and study question by randomly shuffling the study question rows of our original data.  We preserved associations between study questions by preserving the rows (and therefore associations) within the study question variables, and we preserved associations between data types by preserving the rows (and therefore associations) within the data type variables (Fig. S??).  We performed 10,000 permutations.  To test for an overall interaction between data type and study question, we calculated a chi-squared test statistic for each permuted dataset and for the observed dataset, and calculated the significance level as the proportion of chi-squared statistic values from the permuted datasets that were higher than the chi-squared statistic for the observed dataset.  To determine which combinations of data type and study question were found more or less often than expected by chance, we counted the number of studies using each combination of study question and data type for the permuted datasets and for the observed data.  We calculated residuals (deviations from the expectation under the null hypothesis) for each cell of the contingency table by subtracting the mean count for each cell in the permuted data contingency tables from the observed cell count for the same cells. We calculated permutation significance levels for each cell by finding the proportion of the cell counts from the permuted datasets that were more extreme than the observed count for each cell.  Small permutation significance levels for a cell indicate that the observed number of studies in that cell was unlikely if there was no relationship between that data type and study question.

```{r data_type_paradigm_df_prepare}
# make data frame with counts of data type by study paradigm
dt_pd_df <- select(wg, data.structure...organized.data.collection.scheme, 
                   data.structure...sampling.effort.known,
                   data.structure...non.detection,
                   data.structure...multiple.datasets.integrated.for.analysis,
                   response.variable...distribution,
                   response.variable...phenology, 
                   response.variable...abundance, 
                   response.variable...species.richness, 
                   trends.over.time, 
                   other_focus) %>%
  gather(key = "data_type", value = "dt_val", 
         data.structure...organized.data.collection.scheme:
           data.structure...multiple.datasets.integrated.for.analysis) %>%
  filter(dt_val == TRUE) %>%
  gather(key = "study_focus", value = "focus_val", 
         response.variable...distribution:other_focus) %>%
  filter(focus_val == TRUE) %>%
  count(study_focus, data_type) %>%
  complete(study_focus, data_type, fill = list(n = 0))

# clean character strings
dt_pd_df$study_focus <- gsub("response.variable...", "", dt_pd_df$study_focus)
dt_pd_df$data_type <- gsub("data.structure...", "", dt_pd_df$data_type)

# spread for easy visualisation of 2-way table
dt_pd_df_wide <- spread(dt_pd_df, key = study_focus, value = n)
```

```{r data_type_paradigm_model}
dt_pd_mod_full <- glm(n ~ 1 + study_focus + data_type + study_focus:data_type, 
                      data = dt_pd_df, family = poisson(link = log))
dt_pd_mod_ind <- glm(n ~ 1 + study_focus + data_type, 
                     data = dt_pd_df, family = poisson(link = log))

# Do likelihood ratio test for full model vs. independence model
dt_pd_interaction_test <- anova(dt_pd_mod_ind, dt_pd_mod_full,  test = "Chisq")



# look at predicted values compared to observed values
dt_pd_df$predicted <- dt_pd_mod_ind$fitted.values
dt_pd_df$saturated_predictions <- dt_pd_mod_full$fitted.values

# get residuals from independence model
dt_pd_df$resids <- residuals(dt_pd_mod_ind, type = "deviance")
# show big residuals.  This is very interesting b/c it looks like the worst
# places where the independence model doesn't fit have to do with when 
# multiple data types are integrated for analysis.  So that might be the data
# type for which the study questions are different.  Specifically, people study
# abundance way less than expected with that data type, and they study 
# distribution more than expected with that data type.

# dt_pd_df[abs(dt_pd_df$resids) > 2, ] 

# Looking at resids > 1 shows a similar story.  Studies of abundance use 
# structured data types (non detection, sampling effort, etc) more than expected,
# while studies of distributions use structured data types less than expected.

# dt_pd_df[abs(dt_pd_df$resids) > 1, ] 
```

```{r data_type_study_question_randomization}
# randomly re-shuffle study questions (but keep them associated with each
# other in the same way), and keep data types associated with each other.
# This will give H_0 counts for each cell.
rand_array <- array(NA, c(4, 6, n_perm))

for(i in 1:n_perm-1) {
  ind <- sample(1:nrow(wg), replace = F)
  # make data frame of data types
  df_dat <- wg[, grepl(
    ".*multiple.dat.*|.*non.det.*|.*organized.*|.*sampling.eff.*", colnames(wg))]
  # make data frame of study questions
  df_qs <- wg[, grepl(".*respo.*distribution|.*respons.*phenology|.*response.*abundance|response.*species.richness|trends.*time|other_focus", 
                      colnames(wg))]
  # permute analysis methods
  df_qs <- df_qs[ind, ]
  dat <- cbind(df_dat, df_qs)
  
  dat <- gather(dat, key = "data_type", value = "dt_val", 
               data.structure...organized.data.collection.scheme:
                 data.structure...multiple.datasets.integrated.for.analysis) %>%
    filter(dt_val == TRUE) %>%
    gather(key = "study_question", value = "an_val", 
           response.variable...species.richness:
             other_focus) %>%
    filter(an_val == TRUE) %>%
    count(study_question, data_type) %>%
    complete(study_question, data_type, fill = list(n = 0))
  
  # clean character strings
  dat$data_type <- gsub("data.structure...", "", dat$data_type)
  dat$study_question <- gsub("response.variable...", "", dat$study_question)
  
  # spread for easy visualisation of 2-way table
  dat <- spread(dat, key = study_question, value = n)
  dat$data_type <- gsub("\\.", " ", dat$data_type) 
  rand_array[, , i] <- as.matrix(dat[, 2:7])
}

# add observed values as one permutation
rand_array[, , n_perm] <- as.matrix(dt_pd_df_wide[, 2:ncol(dt_pd_df_wide)])


# get permutation chi-squared p-value to test for overall interaction
# See Manley (2007) p. 313
chis_perm_study_question <- c()
for(k in 1:n_perm) {
  chis_perm_study_question[k] <- chisq.test(rand_array[, , k])$statistic
}
chis_perm_study_question_fun <- ecdf(chis_perm_study_question)
# calculate permutation p-value for observed chi-squared test statistic
chis_perm_study_question_pvalue <- 1 - 
  chis_perm_study_question_fun(chisq.test(dt_pd_df_wide[, 2:4])$statistic)

# make empty template df to hold cell p-values later
perm_sq_p_vals <- dt_pd_df_wide
perm_sq_p_vals[, 2:ncol(perm_sq_p_vals)] <- NA # clear counts
# make empty template df to hold residuals later
perm_sq_resids <- dt_pd_df_wide
perm_sq_resids[, 2:ncol(perm_sq_resids)] <- NA # clear counts 
# make template df to hold expected counts later
perm_sq_exp_counts <- dt_pd_df_wide
perm_sq_exp_counts[, 2:ncol(perm_sq_exp_counts)] <- NA # clear counts

for(i in 1:nrow(perm_sq_p_vals)) {
  for(j in 1:(ncol(perm_sq_p_vals) - 1)) {
    emp_dist <- ecdf(rand_array[i, j, ])
    # calculate empirical p-value
    perm_sq_p_vals[i, j + 1] <- emp_dist(dt_pd_df_wide[i, j + 1])
    # make this a 2-tailed p-value
    if(perm_sq_p_vals[i, j + 1] > 0.5) {
      perm_sq_p_vals[i, j + 1] <- 1 - perm_sq_p_vals[i, j + 1]
    }
    # Double the proportion to make it roughly two-tailed (i.e. the proportion
    # of values more extreme than the observed value)
    perm_sq_p_vals[i, j + 1] <- 2 * perm_sq_p_vals[i, j + 1] 
    # calculate residual
    perm_sq_resids[i, j + 1] <- dt_pd_df_wide[i, j + 1] - mean(rand_array[i, j, ])
    # get expected counts
    perm_sq_exp_counts[i, j + 1] <- mean(rand_array[i, j, ])
  }
}
```

## Analysis approach & data type

To investigate whether characteristics of the biological records data affected the analysis approach, we used random permutations to test the null hypothesis of no difference in the types of data used with different analysis approaches.  We identified the analysis approach of each study as inference, prediction, or descriptive analyses only.  The data type categories we tested based on *a-priori* expectations of importance were: sampling effort; non-detection; organized monitoring scheme; and multiple datasets integrated for analysis.  The permutation procedure and associated residual and test statistic calculations were similar to those used for the analysis of the relationship between data type and study questions (Section ??).  


```{r data_type_analysis_approach_df}
dt_an_df <- select(wg, data.structure...organized.data.collection.scheme, 
                   data.structure...sampling.effort.known,
                   data.structure...non.detection,
                   data.structure...multiple.datasets.integrated.for.analysis,
                   results.type...inference, 
                   results.type...prediction, 
                   results.type...descriptive.only) %>%
  gather(key = "data_type", value = "dt_val", 
         data.structure...organized.data.collection.scheme:
           data.structure...multiple.datasets.integrated.for.analysis) %>%
  filter(dt_val == TRUE) %>%
  gather(key = "analysis_approach", value = "an_val", results.type...inference,
         results.type...prediction, results.type...descriptive.only) %>%
  filter(an_val == TRUE) %>%
  count(analysis_approach, data_type) %>%
  complete(analysis_approach, data_type, fill = list(n = 0))

# clean character strings
dt_an_df$data_type <- gsub("data.structure...", "", dt_an_df$data_type)
dt_an_df$analysis_approach <- gsub("results.type...", "", dt_an_df$analysis_approach)

# spread for easy visualisation of 2-way table
dt_an_df_wide <- spread(dt_an_df, key = analysis_approach, value = n)
dt_an_df_wide$data_type <- gsub("\\.", " ", dt_an_df_wide$data_type)
```

```{r data_type_analysis_approach_model}
# fit saturated model
dt_an_mod_full <- glm(n ~ 1 + analysis_approach + data_type + 
                        analysis_approach:data_type, data = dt_an_df, 
                      family = poisson(link = log))
# fit model assuming independence between analysis approach and data type
dt_an_mod_ind <- glm(n ~ 1 + analysis_approach + data_type, 
                     data = dt_an_df, family = poisson(link = log))
dt_an_mod_independence_test <- anova(dt_an_mod_ind, dt_an_mod_full, test = "Chisq")

dt_an_df$predicted <- dt_an_mod_ind$fitted.values
dt_an_df$resids <- residuals(dt_an_mod_ind, type = "deviance")
```

```{r data_analysis_approach_randomization}
# randomly re-shuffle analysis strategies (but keep them associated with each
# other in the same way), and keep data types associated with each other.
# This will give H_0 counts for each cell.
rand_array <- array(NA, c(4, 3, n_perm))

for(i in 1:n_perm-1) {
  ind <- sample(1:nrow(wg), replace = F)
  # make data frame of data types
  df_dat <- wg[, grepl(
    ".*multiple.dat.*|.*non.det.*|.*organized.*|.*sampling.eff.*", colnames(wg))]
  # make data frame of analysis methods
  df_an <- wg[, grepl(".*descriptive.on.*|.*inferen.*|.prediction.*", 
                      colnames(wg))]
  # permute analysis methods
  df_an <- df_an[ind, ]
  dat <- cbind(df_dat, df_an)
  
  dat <- gather(dat, key = "data_type", value = "dt_val", 
               data.structure...organized.data.collection.scheme:
                 data.structure...multiple.datasets.integrated.for.analysis) %>%
    filter(dt_val == TRUE) %>%
    gather(key = "analysis_approach", value = "an_val", results.type...inference,
           results.type...prediction, results.type...descriptive.only) %>%
    filter(an_val == TRUE) %>%
    count(analysis_approach, data_type) %>%
    complete(analysis_approach, data_type, fill = list(n = 0))
  
  # clean character strings
  dat$data_type <- gsub("data.structure...", "", dat$data_type)
  dat$analysis_approach <- gsub("results.type...", "", dat$analysis_approach)
  
  # spread for easy visualisation of 2-way table
  dat <- spread(dat, key = analysis_approach, value = n)
  dat$data_type <- gsub("\\.", " ", dat$data_type) 
  rand_array[, , i] <- as.matrix(dat[, 2:4])
}

# add observed values as one permutation
rand_array[, , n_perm] <- as.matrix(dt_an_df_wide[, 2:ncol(dt_an_df_wide)])


# get permutation chi-squared p-value to test for overall interaction
# See Manley (2007) p. 313
chis_perm <- c()
for(k in 1:n_perm) {
  chis_perm[k] <- chisq.test(rand_array[, , k])$statistic
}
chis_perm_fun <- ecdf(chis_perm)
# calculate permutation p-value for observed chi-squared test statistic
chis_perm_pvalue <- 1 - chis_perm_fun(chisq.test(dt_an_df_wide[, 2:4])$statistic)
# double check using chisq.test.  This gives a different p-value by an order
# of magnitude (though still small).  I think this must be because the built-in
# MC method does not preserve correlations within each variable in the way my
# randomization process does. 
# chisq.test(dt_an_df_wide[, 2:4], simulate.p.value = TRUE, B = n_perm)


# make empty template df to hold cell p-values later
perm_p_vals <- dt_an_df_wide
perm_p_vals[, 2:4] <- NA # clear counts
# make empty template df to hold residuals later
perm_resids <- dt_an_df_wide
perm_resids[, 2:4] <- NA # clear counts 
# make template df to hold expected counts later
perm_exp_counts <- dt_an_df_wide
perm_exp_counts[, 2:4] <- NA # clear counts

for(i in 1:nrow(perm_p_vals)) {
  for(j in 1:(ncol(perm_p_vals) - 1)) {
    emp_dist <- ecdf(rand_array[i, j, ])
    # calculate empirical p-value
    perm_p_vals[i, j + 1] <- emp_dist(dt_an_df_wide[i, j + 1])
    # make this a 2-tailed p-value
    if(perm_p_vals[i, j + 1] > 0.5) {
      perm_p_vals[i, j + 1] <- 1 - perm_p_vals[i, j + 1]
    }
    # Double the proportion to make it roughly two-tailed (i.e. the proportion
    # of values more extreme than the observed value)
    perm_p_vals[i, j + 1] <- 2 * perm_p_vals[i, j + 1] 
    # calculate residual
    perm_resids[i, j + 1] <- dt_an_df_wide[i, j + 1] - mean(rand_array[i, j, ])
    # get expected counts
    perm_exp_counts[i, j + 1] <- mean(rand_array[i, j, ])
  }
}
```


## Authors and Data Providing Institutions

To answer the question of who chooses to use biological records data, we used bootstrap confidence intervals to describe the proportion of studies that have an author associated with the institution that provided the data. 

# Results

## Search results 

```{r eligibleDuplicates}
# count number of eligible studies that were returned more than once
# by the search
titles <- str_replace_all(wg$title, "\\s", "") %>%
  str_replace_all(., "[[:punct:]]", "") %>%
  str_to_lower()
```

The search returned `r nrow(elig)` potentially relevant studies, of which we judged `r length(which(wg$qualifies == TRUE))` (`r round((length(which(wg$qualifies == TRUE))/nrow(elig)) * 100, digits = 1)`%) to be eligible for inclusion in this review.  The number of potentially relevant studies returned by more than one search method was `r length(n_duplicate_titles)` (`r round((length(n_duplicate_titles)/nrow(elig))*100, digits = 1)`%).  Of the studies that we deemed eligible for inclusion in this review, `r length(which(titles %in% names(n_duplicate_titles)))` (`r round((length(which(titles %in% names(n_duplicate_titles)))/nrow(wg))*100, digits = 1)`%) were returned by more than one search method.  

One reader coded all `r nrow(wg)` studies and a second reader independently coded `r nrow(er)` of the studies. 

## Coding reliability

Most variables for which Krippendorff's *alpha* was estimated show that the coding is reliable enough to be used in analysis, at least for drawing tentative conclusions (Tab. 1, Fig. S1).  Many of the variables have Krippendorff's *alpha* values above the minimum required level of 0.67 but below the prefered level of 0.8, which indicates that they should be interpreted with caution.  The number of articles that have been evaluated by two readers was too small for reliable estimation of Krippendorff's alpha for some variables for which the smallest class is particularly uncommon (Tab. S1). 

```{r calculate_proportion_T}
## This calculates the probability of the smallest class for each variable, for 
## use in calculating minimum sample size needed for Kripp's alpha for that 
## variable.  
## This treats all variables as factor variables.
prob_of_values <- lapply(
  wg[, which(colnames(wg) %in% names(kripp_all_dbl_coded))], 
  FUN = function(x) {
    x <- factor(x)
    mn <- 1
    for(j in 1:length(levels(x))) {
      prob <- length(which(x == levels(x)[j])) / length(which(!is.na(x)))
      if(prob < mn) mn <- prob
    }
    mn
  })

prob_of_values <- data.frame(variable = names(prob_of_values), 
                             prob_small_class = unlist(prob_of_values), 
                             row.names = NULL)
prob_of_values$kripp_alpha <- NA
for(i in 1:nrow(prob_of_values)) {
  # add krippendorff's alpha for codings
  prob_of_values$kripp_alpha[i] <- kripp_all_dbl_coded[[which(
    names(kripp_all_dbl_coded) == prob_of_values$variable[i])]]$value
}
prob_of_values <- prob_of_values[order(prob_of_values$kripp_alpha, 
                                       decreasing = T), ]

# On p 240 of Krippendorff book, it says that if acceptable alpha is 0.667 and 
# desired significance level is 0.1, then sample size needed is as follows:
#prob_of_values$n_25_adequate <- prob_of_values$prob_small_class > 0.25
#prob_of_values$n_30_adequate <- prob_of_values$prob_small_class > 0.2
prob_of_values$n_40_adequate <- prob_of_values$prob_small_class >= 0.14
# prob_of_values$n_50_adequate <- prob_of_values$prob_small_class >= 0.105 

## using 50 double-codings would only get us an additional 2 variables, and I
## don't think those are variables important enough to be worth the effort.

# Put vars with adequate sample size at top
prob_of_values <- prob_of_values[order(prob_of_values$n_40_adequate, 
                                       prob_of_values$kripp_alpha, 
                                       decreasing = TRUE),]

# list only those for which sample size is adequate
samp_adequate <- prob_of_values[which(prob_of_values$n_40_adequate == T), ]
rownames(samp_adequate) <- NULL
samp_adequate$variable <- gsub("\\.\\.\\.", " - ", samp_adequate$variable)
samp_adequate$variable <- gsub("\\.", " ", samp_adequate$variable)
kable(samp_adequate[order(samp_adequate$kripp_alpha, decreasing = T), -c(2, 4)], 
      col.names = c("Variable", "Krippendorff's alpha"),
      digits = 2)
```

## Temporal and spatial extent of studies

```{r print_time_individual_predictor_significance}
if(diag) {
  for(i in 1:length(indiv_mods_time)) { 
    print(names(indiv_mods_time)[i])
    print(anova(indiv_mods_time[[i]], time_lm_full))
  }
}
```

The temporal extent of studies differed significantly for studies using different types of biological records data (anova of a linear regression model using data type to predict the natural log of the temporal extent of studies (in years) compared to a model that did not use data type $F_{`r format(round(mca_time_full_anova[2, "Df"], digits = 2), scientific = F)`, `r format(round(mca_time_full_anova[2, "Res.Df"], digits = 2), scientific = F)`}$ = `r format(round(mca_time_full_anova[, "F"][2], digits = 2), scientific = F)`, *p* `r if(mca_time_full_anova[6][[1]][2] >= 0.0001) {format(round(mca_time_full_anova[6][[1]][2], digits = 4), scientific = T)} else "< 0.0001"`, Adjusted $R^2$ = `r format(round(mca_lm_summary$adj.r.squared, digits = 2), scientific = F)`, Fig. 1).  The first MCA dimension was related to whether studies used more structured types of biological records, with high values of dimension one being associated with studies that had explicit non-detection data, sampling effort information, or data from organized monitoring schemes.  For the second MCA dimension, high values indicated studies that used multiple biological records datasets.  The estimated linear regression coefficient for dimension one was `r time_lm_log_catpca$coefficients[["D1"]]` ($F_{1, 199}$ = `r round(anova(mca_noD1, time_lm_log_catpca)$F[2], digits = 2)`, p < 0.0001), indicating that studies using more structured data covered significantly shorter temporal extents than studies using less structured data. The estimated linear regression coefficient for dimension two was `r time_lm_log_catpca$coefficients[["D2"]]` ($F_{1, 199}$ = `r round(anova(mca_noD2, time_lm_log_catpca)$F[2], digits = 2)`, p < 0.0001), indicating that studies using more multiple biological records datasets covered significantly longer temporal extents than studies that did not.  We removed from the analysis `r length(which(is.na(wg$temp_extent)))` studies for which the coder could not determine the temporal extent.

The mean spatial extent was not different for studies using different types of data based on overlapping 95% bootstrap confidence intervals (Fig. 2).

```{r time_extent_results_plot}
# ## boxplot version
# temp_extent_plot <- ggplot(data = temp_extent_plot_df, 
#                            aes(y = temp_extent, 
#                                x = factor(
#                                  data_type, 
#                                  levels = c("sampling effort known", 
#                                             "non detection", 
#                                             "organized scheme", 
#                                             "multiple datasets"), 
#                                  labels = c("sampling effort\nknown", 
#                                             "non-detection", 
#                                             "organized\nmonitoring\nscheme",  
#                                             "multiple\ndatasets")))) + 
#   geom_boxplot(varwidth = T) +
#   scale_y_log10() + 
#   xlab("Data type") + 
#   ylab("Temporal extent of study (years)") + 
#   # annotate("text", x = 3, y = 2600, 
#   #          label = "paste(\"Adjusted \", 
#   #                italic(R) ^ 2, \" = 0.36\")", 
#   #          parse = TRUE) + 
#   # annotate("text", x = 3, y = 1200, 
#   #          label = "paste(\"Overall model significance \", 
#   #                italic(p), \" < 0.0001\")", 
#   #          parse = TRUE) +
#   theme(axis.text.x = element_text(angle = 35, vjust = 0.9, hjust = 1), 
#         panel.background = element_rect(fill = "white", colour = "grey50")) 
# print(temp_extent_plot)

## make a ridge plot version
temp_extent_plot_ridge <- ggplot(
  data = temp_extent_plot_df, 
  aes(x = temp_extent, 
      y = factor(
        data_type, 
        levels = c("sampling effort known", 
                   "non detection", 
                   "organized scheme", 
                   "multiple datasets"), 
        labels = c("sampling effort\nknown", 
                   "non-detection", 
                   "organized\nmonitoring\nscheme",
                   "multiple\ndatasets")))) + 
  geom_density_ridges(panel_scaling = F, 
                      jittered_points = TRUE,
                      position = position_points_jitter(width = 0.05, height = 0),
                      point_shape = '|', point_alpha = 0.7, alpha = 0.7) + 
  scale_x_log10(breaks = c(1, 10, 100, 1000, 10000), 
                labels = c("1", "10", "100", "1000", "10000")) + 
  ylab("Data type") +
  xlab("Temporal extent of study (years)") +
  theme(axis.text.x = element_text(angle = 35, vjust = 0.9, hjust = 1), 
        panel.background = element_rect(fill = "white", colour = "grey50")) 
print(temp_extent_plot_ridge)
```

```{r time_extent_results_plot_for_presentation_slide, eval = F, include = F}
## This code is to be run in the console for making plot to be exported as jpg for slides
## TODO: need to hand-annotate R2 and p-value in plot
# summary(time_lm_full)
t_size = 22 # plot text size
ggplot(data = temp_extent_plot_df, aes(y = temp_extent, x = data_type)) + #, color = sig_different_from_WhWhWh
  geom_boxplot(varwidth = T) +
  scale_y_log10() + 
  xlab("Data type") + 
  ylab("Temporal extent of study (years)") + 
  annotate("text", x = 3, y = 2000,
           label = "paste(\"Adjusted \",
                 italic(R) ^ 2, \" = 0.28\")",
           parse = TRUE,
           size = t_size - 15) +
  annotate("text", x = 3, y = 5000, 
           label = "paste(\"Overall model significance \", 
                 italic(p), \" < 0.0001\")", 
           parse = TRUE, 
           size = t_size - 14) +
  theme_bw() + 
  scale_color_discrete(name = "Significantly Different from\nWhat, Where, When-only\nstudies (alpha = 0.1 level)") + 
  theme(axis.text.x = element_text(angle = 330, vjust = 1, hjust = 0), 
        text = element_text(size = t_size)) 
```

```{r spat_extent_bootCI_plot, include=FALSE}
spat_boot_df <- data.frame(matrix(nrow = length(data_types), 
                                  ncol = 7))
colnames(spat_boot_df) <- c("data_type", "minimum", "maximum", "median", "mean", 
                             "l_95ci", "u_95ci")

# fill response variable counts data frame using the list of bootstrap results
for(i in 1:length(data_types)) {
  spat_boot_df[i, "data_type"] <- names(data_types)[i]
  spat_boot_df[i, "minimum"] <- min(spat_ext_data_type_CIs[[i]]$obs$spat_extent, 
                                    na.rm = TRUE)
    spat_boot_df[i, "maximum"] <- max(spat_ext_data_type_CIs[[i]]$obs$spat_extent, 
                                    na.rm = TRUE)
      spat_boot_df[i, "median"] <- median(
        spat_ext_data_type_CIs[[i]]$obs$spat_extent, 
        na.rm = TRUE)
      spat_boot_df[i, "mean"] <- spat_ext_data_type_CIs[[i]]$ci_obj$t0
      spat_boot_df[i, "l_95ci"] <- spat_ext_data_type_CIs[[i]]$ci_obj$bca[4]
      spat_boot_df[i, "u_95ci"] <- spat_ext_data_type_CIs[[i]]$ci_obj$bca[5]
  
}

# plot mean spatial extent of studies using each data type, with 95% CIs
spat_extent_plot <- ggplot(data = spat_boot_df, 
                           aes(x = factor(data_type, 
                                          levels = c("sampling.effort",
                                                     "non.detection", 
                                                     "organized.scheme",
                                                     "multiple.datasets"), 
                                          labels = c("sampling effort",
                                                     "non-detection", 
                                                     "organized scheme",
                                                     "multiple datasets")), 
                               y = mean)) + 
  geom_point(size = 3) + 
  geom_linerange(aes(x = factor(data_type, 
                                levels = c("sampling.effort",
                                           "non.detection", 
                                           "organized.scheme",
                                           "multiple.datasets"), 
                                labels = c("sampling effort",
                                           "non-detection", 
                                           "organized scheme",
                                           "multiple datasets")), 
                     ymin = l_95ci, 
                     ymax = u_95ci), 
                     size = 1.1) + 
  xlab("") + 
  ylab(expression("Spatial extent of Studies (km"^{2}*")")) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1), 
        strip.background = element_blank(), strip.placement = "outside")
# theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)))
print(spat_extent_plot)
```



## What is the most popular study question paradigm?

The proportion of studies asking questions about individual species was `r round(proportion_individual$boot_obj$t0, digits = 2)` (95% boostrap CI [`r round(proportion_individual$ci_obj$bca[4], digits = 2)`, `r round(proportion_individual$ci_obj$bca[5], digits = 2)`]).  Coder agreement was low when determining whether studies were asking questions about communities (Krippendorff's alpha = 0.65, Tab. 1), which prevented us from interpreting that variable, and therefore prevented a comparison between the proportion of studies that were focused on individual species and the proportion of studies that focused on communities.  

## Ecological focus

Over half the studies in this review analyzed species distribution (proportion of studies = `r round(resp_var_list$dist$boot_obj$t0, digits = 2)`, 95% bootstrap CI [`r round(resp_var_list$dist$ci_obj$bca[4], digits = 2)`, `r round(resp_var_list$dist$ci_obj$bca[5], digits = 2)`]) and/or temporal trends (proportion of studies = `r round(resp_var_list$temp_trends$boot_obj$t0, digits = 2)`, 95% bootstrap CI [`r round(resp_var_list$temp_trends$ci_obj$bca[4], digits = 2)`, `r round(resp_var_list$temp_trends$ci_obj$bca[5], digits = 2)`]) as a response variable (Fig. 2, Tab. S?).  The proportion of studies analyzing species distributions and/or temporal trends was significantly higher than the proportion analyzing any other response variable based on non-overlapping 95% bootstrap confidence intervals (Fig. 2, Tab. S?).  The third most common response variable for studies in this review was abundance (proportion of studies = `r round(resp_var_list$abund$boot_obj$t0, digits = 2)`, 95% bootstrap CI [`r round(resp_var_list$abund$ci_obj$bca[4], digits = 2)`, `r round(resp_var_list$abund$ci_obj$bca[5], digits = 2)`]), but the proportion of studies analyzing abundance was not significantly different from the proportion analyzing species richness (proportion of studies = `r round(resp_var_list$sp_rich$boot_obj$t0, digits = 2)`, 95% bootstrap CI [`r round(resp_var_list$sp_rich$ci_obj$bca[4], digits = 2)`, `r round(resp_var_list$sp_rich$ci_obj$bca[5], digits = 2)`]) or phenology (proportion of studies = `r round(resp_var_list$phen$boot_obj$t0, digits = 2)`, 95% bootstrap CI [`r round(resp_var_list$phen$ci_obj$bca[4], digits = 2)`, `r round(resp_var_list$phen$ci_obj$bca[5], digits = 2)`]).  Too few studies were classified as using "diversity" as a response variable to estimate coder agreement using Krippendorff's alpha.  Therefore, we did not evaluate the diversity response variable category on its own, but studies that had been coded as using diversity and no other response variables are included in the "other" response variable category.  The proportion of studies that focused on alien species was too low for us to estimate coder agreement using Krippendorff's alpha (proportion of studies = `r round(length(which(wg$alien.species.focus == T)) / nrow(wg), digits = 2)`).  

```{r make_response_variable_df}
resp_count_df <- data.frame(matrix(nrow = length(resp_var_list), 
                                        ncol = 5))
colnames(resp_count_df) <- c("response_variable", "n_studies", "prop_studies",
                             "l_95ci", "u_95ci")

# fill response variable counts data frame using the list of bootstrap results
for(i in 1:length(resp_var_list)) {
  resp_count_df[i, "response_variable"] <- names(resp_var_list)[i]
  resp_count_df[i, "n_studies"] <- resp_var_list_counts[[i]]$ci_obj$t0
  resp_count_df[i, "prop_studies"] <- resp_var_list[[i]]$ci_obj$t0
  resp_count_df[i, "l_95ci"] <- resp_var_list[[i]]$ci_obj$bca[4]
  resp_count_df[i, "u_95ci"] <- resp_var_list[[i]]$ci_obj$bca[5]
}

# clean response variable strings
resp_count_df$response_variable <- gsub(".*sp_rich.*", "species richness", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*other.*", "other", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*dist.*", "species distribution", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*abund.*", "abundance", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*phen.*", "phenology", 
                                        resp_count_df$response_variable)
resp_count_df$response_variable <- gsub(".*temp_.*", "temporal trends", 
                                        resp_count_df$response_variable)
```


```{r resp_var_by_paradigm_plot}
# plot proportion of studies using each response variable, with 95% CIs
response_var_plot <- ggplot(data = resp_count_df, 
                            aes(x = factor(response_variable, 
                                           levels = c("abundance",
                                                      "species distribution", 
                                                      "phenology",
                                                      "species richness",
                                                      "temporal trends",
                                                      "other"), 
                                           labels = c("abundance",
                                                      "species\ndistribution", 
                                                      "phenology",
                                                      "species\nrichness",
                                                      "temporal\ntrends",
                                                      "other")), 
                                y = prop_studies)) + 
  geom_point(size = 3) + 
  geom_linerange(aes(x = factor(response_variable, 
                                levels = c("abundance",
                                           "species distribution", 
                                           "phenology",
                                           "species richness",
                                           "temporal trends",
                                           "other"), 
                                labels = c("abundance",
                                           "species\ndistribution", 
                                           "phenology",
                                           "species\nrichness",
                                           "temporal\ntrends",
                                           "other")), 
                     ymin = l_95ci, 
                     ymax = u_95ci), 
                     size = 1.1) + 
  xlab("") + 
  ylab("Proportion of studies") + 
  # ylim(c(0, 0.75)) + 
  theme(axis.text.x = element_text(angle = 40, hjust = 1, vjust = 1), 
        strip.background = element_blank(), strip.placement = "outside", 
        panel.background = element_rect(fill = "white", colour = "grey50"))
# theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)))
print(response_var_plot)
```

## Development of methodology across different study questions

The overall model predicting the probability of a study addressing a methodological question based on the ecological focus of the study was not significant (intercept-only model residual deviance = `r round(method_chisq$'Resid. Dev'[1], digits = 1)`, full model residual deviance = `r round(method_chisq$'Resid. Dev'[2], digits = 1)`, Chi-squared test *p* = `r round(anova(method_mod, method_mod_null, test = "Chisq")[2, 5], 2)`, Fig. 3).  The proportion of studies that were methodological was `r round(prop_method_CI$boot_obj$t0, digits = 2)` (95% bootstrap CI [`r round(prop_method_CI$ci_obj$bca[[4]], digits = 2)`, `r round(prop_method_CI$ci_obj$bca[[5]], digits = 2)`]).

```{r plot_method_study_focus_coefficients}
meth_df <- data.frame(matrix(nrow = 7, ncol = 6))
colnames(meth_df) <- c("variable", "coef_estimate", "l_bound", "h_bound", 
                       "z_value", "P_value")
meth_df$variable <- c("Intercept", "abundance", "distribution", "phenology", 
                      "richness", "time", "other")
cis <- confint(method_mod)
for(i in 1:nrow(meth_df)) {
  v <- meth_df$variable[i]
  coefs <- data.frame(summary(method_mod)$coefficients)
  coefs <- rownames_to_column(coefs)
  # get coefficient point estimate
  meth_df$coef_estimate[i] <- coefs$Estimate[grepl(v, coefs$rowname)]
  meth_df$l_bound[i] <- cis[grepl(v, dimnames(cis)[[1]]), "2.5 %"]
  meth_df$h_bound[i] <- cis[grepl(v, dimnames(cis)[[1]]), "97.5 %"]
  meth_df$z_value[i] <- coefs$z.value[grepl(v, coefs$rowname)]
  meth_df$P_value[i] <- coefs$Pr...z..[grepl(v, coefs$rowname)]
}

# make graph
methodology_plot <- ggplot(data = meth_df, 
                           aes(x = factor(
                             variable, 
                             levels = c("abundance", "distribution", 
                                        "phenology", "richness",
                                        "time", "other"), 
                             labels = c("abundance", "species\ndistribution", 
                                        "phenology", "species\nrichness",
                                        "temporal\ntrends", "other")), 
                             y = coef_estimate)) + 
  geom_point(size = 3) + 
  geom_linerange(aes(x = factor(variable, 
                                levels = c("abundance", "distribution", 
                                           "phenology", "richness",
                                           "time", "other"), 
                                labels = c("abundance", "species\ndistribution", 
                                           "phenology", "species\nrichness",
                                           "temporal\ntrends", "other")), 
                     ymin = l_bound, 
                     ymax = h_bound), 
                     size = 1.1) + 
  xlab("Study question") + 
  ylab("Coefficient estimate") + 
  theme_bw() + 
  ylim(-2, 2) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1))
print(methodology_plot)
```


## What data types are used for each type of ecological study question?

The permutation test suggested that data type and study question were not independent (Chi-square statistic = `r chisq.test(dt_pd_df_wide[, 2:4])$statistic`, df = `r chisq.test(dt_pd_df_wide[, 2:4])$parameter`p = `r chis_perm_study_question_pvalue` with `r n_perm` permutations).  Permutation test residuals and significance levels for each combination of data type and study question showed that residuals were generally large, positive, and significantly different from zero for combinations of structured data types and studies of species abundances, suggesting that those data types were used more frequently for abundance studies than would be expected if there were no relationship between data type and analysis approach. Residuals were large, negative, and highly significantly different from zero for combinations of more structured data types and studies of species distributions (Fig. 4?), suggesting that those rich data types are used for distribution studies less often than would be expected if there was no relationship between data type and analysis approach. The absolute value of residuals and significance levels for combinations of all data types with studies of phenology, species richness and "other" study questions generally indicated that the use of data types for those study questions did not differ strongly from what would be expected if there were no relationship between data type and study question.  The residuals and significance levels suggested that multiple datasets were used less often for abundance studies and more often for distribution studies (and perhaps for studies of "other" questions) than would be expected if there was no relationship between data type and study question.  Both the absolute magnitude of residuals and the significance levels suggested that the observed use of multiple datasets for abundance and distribution questions differed less strongly from the null expectation than did combinations of structured data types with those study questions.  


```{r print_data_type_study_question_heatmap_table_permutation}
# get permutation significance levels
perm_sq_p_vals_long <- gather(perm_sq_p_vals, key = "study_question", 
                           value = "significance", 2:ncol(perm_sq_p_vals))
# get permutation residuals
perm_sq_resids_long <- gather(perm_sq_resids, key = "study_question", 
                           value = "residual", 2:ncol(perm_sq_resids))
# add significance levels to residuals data frame
perm_sq_resids_long <- left_join(perm_sq_resids_long, perm_sq_p_vals_long, 
                              by = c("data_type", "study_question"))
# make symbols for significance levels
perm_sq_resids_long$sig_symb <- NA
perm_sq_resids_long$sig_symb[perm_sq_resids_long$significance >= 0.1] <- " "
perm_sq_resids_long$sig_symb[perm_sq_resids_long$significance < 0.1 & 
                               perm_sq_resids_long$significance >= 0.01] <- "*"
perm_sq_resids_long$sig_symb[perm_sq_resids_long$significance < 0.01 & 
                               perm_sq_resids_long$significance >= 0.001] <- "**"
perm_sq_resids_long$sig_symb[
  perm_sq_resids_long$significance < 0.001 & 
    perm_sq_resids_long$significance >= 0.0001] <- "***"
perm_sq_resids_long$sig_symb[perm_sq_resids_long$significance < 0.0001] <- "****"


data_type_study_question_perm_plot <- ggplot(
  data = perm_sq_resids_long,        
  aes(x = factor(study_question, 
                 levels = c("abundance", "distribution", "phenology", 
                            "species.richness", "trends.over.time", 
                            "other_focus"), 
                 labels = c("abundance", "species distribution", 
                            "phenology", 
                            "species richness", 
                            "trends over time", 
                            "other")), 
      y = factor(data_type,
                 levels = c("multiple.datasets.integrated.for.analysis", 
                            "sampling.effort.known", 
                            "non.detection",
                            "organized.data.collection.scheme"), 
                 labels = c("multiple\ndatasets", 
                            "sampling effort", 
                            "non-detection", 
                            "organized\nscheme")))) + 
  geom_tile(aes(fill = residual)) + 
  xlab("Study question") + 
  ylab("Data type") + # write the values
  scale_fill_gradient2(low = "#3366ff", mid = "white", high = "#ff751a") + 
  theme(panel.grid.major.x=element_blank(), #no gridlines
        panel.grid.minor.x=element_blank(), 
        panel.grid.major.y=element_blank(), 
        panel.grid.minor.y=element_blank(),
        panel.background=element_rect(fill="white"), # background=white
        axis.text.x = element_text(angle=45, hjust = 1, vjust=1),
        axis.text.y = element_text(angle=45)) +  
  # ggtitle("Residuals (significance level)\nfrom permutation test") + 
  labs(fill = "observed -\nexpected\ncounts")
print(data_type_study_question_perm_plot + 
  geom_text(aes(label = paste0(round(residual, digits = 1), "\n", sig_symb))))
```

Some types of additional data that can be associated with biological records were used too infrequently for us to evaluate the reliability of our codings.  Types of data used infrequently included photos (`r round((length(which(wg$data.type...photo == T)) / nrow(wg)) * 100, digits = 1)` % of studies), videos (`r round((length(which(wg$data.type...video == T)) / nrow(wg)) * 100, digits = 1)` % of studies), vouchers specimens (`r round((length(which(wg$data.type...voucher.of.some.kind.necessary.for.analysis == T)) / nrow(wg)) * 100, digits = 1)` % of studies), audio recordings (`r round((length(which(wg$data.type...audio == T)) / nrow(wg)) * 100, digits = 1)` % of studies), and life stage (e.g. phenology) information (`r round((length(which(wg$data.type...life.stage == T)) / nrow(wg)) * 100, digits = 1)` % of studies.  The coding reliability was too low for us to interpret whether studies used abundance data (Krippendorff's *alpha* = `r round(samp_adequate$kripp_alpha[samp_adequate$variable == "data type - abundance"], digits = 2)`) or only "what, where, when" data (Krippendorff's *alpha* = `r round(samp_adequate$kripp_alpha[samp_adequate$variable == "data type - what where when only"], digits = 2)`).

## Analysis approach & data type

There was strong evidence that data type and analysis approach are not independent (chi-squared test statistic = `r chisq.test(dt_an_df_wide[, 2:4])$statistic`, df = `r chisq.test(dt_an_df_wide[, 2:4])$parameter`,p = `r chis_perm_pvalue`, `r n_perm` permutations).  Permutation test residuals and significance levels for each combination of data type and analysis method show that residuals are generally large, negative, and highly significantly different from zero for combinations of more structured data types and descriptive-only analyses (Fig. 5), suggesting that those rich data types are analyzed descriptively less often than would be expected if there was no relationship between data type and analysis approach.  Residuals are generally large, positive, and significantly different from zero for combinations of structured data types and inferential analyses, suggesting that those data types are analyzed with inferential methods more frequently than would be expected if there were no relationship between data type and analysis approach.  The absolute value of residuals and significance levels for combinations of all data types with predictive analysis methods generally indicated that the use of data types in predictive analyses did not differ strongly from what would be expected if there were no relationship between data type and analysis approach.  The residuals and significance levels for the cells associated with multiple data sets being used in an analysis suggest that multiple datasets are used less often for inference and more often for descriptive-only analyses than would be expected if there was no relationship between data type and analysis approach, though both the absolute magnitude of the residuals and the significance levels suggested that the observed use of multiple datasets for those two analysis approaches differed less strongly from the null expectation than did the use of data with non-detection and/or sampling effort information and data from organized monitoring schemes.  

```{r print_data_type_analysis_approach_contingency_table}
if(diag_present) {
    colnames(dt_an_df_wide) <- gsub(
    "results.type\\.\\.\\.", "", 
    colnames(dt_an_df_wide))
  colnames(dt_an_df_wide) <- gsub("\\.", " ", colnames(dt_an_df_wide))
  dt_an_df_wide$data_type <- gsub(
    "data\\.structure\\.\\.\\.|data\\.type\\.\\.\\.", "",  
    dt_an_df_wide$data_type)
  dt_an_df_wide$data_type <- gsub("\\.", " ", dt_an_df_wide$data_type)

  kable(dt_an_df_wide)
}
```


```{r print_data_type_analysis_method_heatmap_table_permutation}
# get permutation significance levels
perm_p_vals_long <- gather(perm_p_vals, key = "analysis_approach", 
                           value = "significance", 2:4)
# get permutation residuals
perm_resids_long <- gather(perm_resids, key = "analysis_approach", 
                           value = "residual", 2:4)
# add significance levels to residuals data frame
perm_resids_long <- left_join(perm_resids_long, perm_p_vals_long, 
                              by = c("data_type", "analysis_approach"))
perm_resids_long$sig_symb <- NA # make column for significance level symbol
perm_resids_long$sig_symb[perm_resids_long$significance >= 0.1] <- " "
perm_resids_long$sig_symb[perm_resids_long$significance < 0.1 & 
                               perm_resids_long$significance >= 0.01] <- "*"
perm_resids_long$sig_symb[perm_resids_long$significance < 0.01 & 
                               perm_resids_long$significance >= 0.001] <- "**"
perm_resids_long$sig_symb[
  perm_resids_long$significance < 0.001 & 
    perm_resids_long$significance >= 0.0001] <- "***"
perm_resids_long$sig_symb[perm_resids_long$significance < 0.0001] <- "****"

data_type_analysis_approach_plot <- ggplot(
  data = perm_resids_long,        
  aes(x = factor(analysis_approach, 
                 levels = c("descriptive.only", "inference", "prediction"), 
                 labels = c("descriptive only", 
                            "inference", "prediction")), 
      y = factor(data_type,
                 levels = c("multiple datasets integrated for analysis", 
                            "sampling effort known", 
                            "non detection",
                            "organized data collection scheme"), 
                 labels = c("multiple\ndatasets", 
                            "sampling effort", 
                            "non-detection", 
                            "organized\nscheme")))) + 
  geom_tile(aes(fill = residual)) + 
  xlab("Analysis approach") + 
  ylab("Data type") + # write the values
  scale_fill_gradient2(low = "#3366ff", mid = "white", high = "#ff751a") + 
  theme(panel.grid.major.x=element_blank(), #no gridlines
        panel.grid.minor.x=element_blank(), 
        panel.grid.major.y=element_blank(), 
        panel.grid.minor.y=element_blank(),
        panel.background=element_rect(fill="white"), # background=white
        axis.text.x = element_text(angle=40, hjust = 1, vjust=1),
        axis.text.y = element_text(angle=45)) +  
  # ggtitle("Residuals (significance level)\nfrom permutation test") + 
  labs(fill = "observed -\nexpected\ncounts")
print(data_type_analysis_approach_plot + 
  geom_text(aes(label = paste0(round(residual, digits = 1), "\n", sig_symb))))
```


## Proximate data sources
```{r proximate_data_sources}
# extract proximate data sources
prox_src <- wg$proximate.data.source
prox_src <- str_split(prox_src, ";", simplify = TRUE)

prox_src_df <- data.frame(table(prox_src)[order(table(prox_src), decreasing = T)])
rownames(prox_src_df) <- NULL 
prox_src_df <- prox_src_df[which(as.character(prox_src_df$prox_src) != "" & 
                                   as.character(prox_src_df$prox_src) != "many" & 
                                   as.character(prox_src_df$prox_src) != "others"), ]

# how many studies used data provided one of the top 10 proximate data providers?
used_top_10_providers <- grepl(paste(prox_src_df$prox_src[1:10], collapse = "|"), 
                               wg$proximate.data.source)
# table(used_top_10_providers)
```

Studies in this review obtained their data from at least `r nrow(prox_src_df)` different proximate data sources.  The number of studies using each data source was strongly right-skewed, with many data sources that were used by only one study, and relatively few data sources that were used by many studies.  Among the most frequently used data sources were both national and international data aggregators (e.g. the UK-focused National Biodiversity Network, the Irish National Biodiversity Data Centre, and the Global Biodiversity Information Facility) and individual organizations that manage both the initial collection and the subsequent presentation of data (e.g. the British Trust for Ornithology and the UK Butterfly Monitoring Scheme).  Of the `r nrow(wg)` studies in this review, `r length(which(used_top_10_providers == T))` (`r round((length(which(used_top_10_providers == T)) / length(used_top_10_providers)) * 100, digits = 1)`%) obtained at least some of their data from one of the ten most commonly used proximate data providers.

Many of the data sources that provided data for only one study in this review were "traditional" data sources such as local records centres, taxon-specific monitoring schemes, or natural history museums.  But there were also a few non-traditional data sources used by studies in this review.  At least one study in this review used data from each of Google Images (Leighton et al. 2016), Flickr (Jeawak et al. 2017; Petrusková et al. 2015), YouTube (Petrusková et al. 2015), Vimeo (Petrusková et al. 2015), and SoundCloud, an audio sharing platform that contains music, podcasts, and other audio content (Petrusková et al. 2015).

```{r}
kable(prox_src_df[1:10, ], row.names = FALSE, 
      col.names = c("Proximate Data Source", "Number of Studies"))
```

## Authors and Data Providing Institutions
Coder agreement was low when determining whether a study had an author associated with the data providing institution (Krippendorff's alpha = 0.58).  Therefore we did not analyze the association between authors and data providers statistically.  In the Discussion we addressed some possible reasons that this determination was difficult for the coders to agree on.  

## Cross validation
We coded whether studies tested statistical models using cross-validation and/or independent test datasets, but the percent of studies that did those things was too low for us to estimate the reliability of our codings using Krippendorff's alpha (proportion of studies that tested using cross-validation = `r round(length(which(wg$cross.validation == T)) / nrow(wg), digits = 2)`, proportion of studies that tested using independently collected test data = `r round(length(which(wg$testing.using.independent.dataset == T)) / nrow(wg), digits = 2)`).  

## Acknowledgments
This publication emanated from research supported in part by a grant from Science Foundation Ireland under grant number 15/IA/2881.

```{r dist_cv}
# of the distribution studies, how many used cross-validation?
dist_studs <- wg[wg$response.variable...distribution == T, ]
dist_cv <- length(which(dist_studs$cross.validation == T))
dist_ind <- length(which(dist_studs$testing.using.independent.dataset == T))
# make data frame of studies that did "good" testing, i.e. CV or independent data or both
dist_cv_ind <- dist_studs[dist_studs$testing.using.independent.dataset == T | 
                              dist_studs$cross.validation == T, ]
# table(dist_cv_ind$cross.validation, dist_cv_ind$testing.using.independent.dataset)
```

Of the species distribution studies, `r nrow(dist_cv_ind)` of `r nrow(dist_studs)` (`r round((nrow(dist_cv_ind)/nrow(dist_studs))*100, digits = 0)`%) used cross-validation or tested on independent data (or both).  

An opportunity identified by our review is that multiple different biological records datasets were used for `r round((length(which(wg$data.structure...multiple.datasets.integrated.for.analysis[wg$response.variable...distribution == TRUE])) / length(which(wg$response.variable...distribution))) * 100, digits = 0)`% (`r length(which(wg$data.structure...multiple.datasets.integrated.for.analysis[wg$response.variable...distribution == TRUE]))` of `r length(which(wg$response.variable...distribution))`) species distribution studies and `r round(length(which(wg$data.structure...multiple.datasets.integrated.for.analysis)) / nrow(wg) * 100, digits = 0)`% (`r length(which(wg$data.structure...multiple.datasets.integrated.for.analysis))` of `r nrow(wg)`)

# Appendix 1 
## Model assumption checking

### Coding Reliability
```{r plot_kripp_for_adequate_sample_size}
# plot krippendorf's alpha values for only variables that have adequate sample size
krip_plot <- ggplot(data = prob_of_values[which(prob_of_values$n_40_adequate == T), ], 
                    aes(y = kripp_alpha)) +
  geom_boxplot() + 
  geom_point(aes(x = 0)) + 
  geom_hline(aes(yintercept = 0.667), lty = "dotted") + 
  geom_hline(aes(yintercept = 0.8), lty = "dashed") + 
  # ggtitle("Krippendorf's alpha for the variables for\nwhich we have adequate sample size") +
  ylab("Krippendorf's alpha") + 
  xlab("") +
  theme_bw() + 
  theme(axis.text.x = element_blank())
print(krip_plot)
```

```{r _print_kripp_sample_adequate_table}
kable(data.frame(prob_of_values), 
      col.names = c("variable", "prob of small class", 
                    "Krippendorff's alpha", "sample size 40 adequate"), 
      digits = 2)
```

```{r coding_match_temp_extent}
# in two cases ellie found a start year when I did not
wg_dbl_cd$start.year[er$start.year == "unknown"]
er$start.year[wg_dbl_cd$start.year == "unknown"]

both_numeric <- which(!is.na(as.numeric(wg_dbl_cd$start.year)) & 
                        !is.na(as.numeric(er$start.year)))
sy_df <- data.frame(wgyr <- as.numeric(wg_dbl_cd$start.year[both_numeric]), 
                    eryr <- as.numeric(er$start.year[both_numeric]))
start_year_cor_plot <- ggplot(data = sy_df, aes(x = wgyr, y = eryr)) + 
  geom_point(size = 3) + theme_bw() + 
  xlab("Start year determined by coder one") + 
  ylab("Start year determined by coder two")
print(start_year_cor_plot)
# correlation is good enough.  Two very different studies, but others match.
cor(as.numeric(wg_dbl_cd$start.year[both_numeric]),  
    as.numeric(er$start.year[both_numeric]), 
    method = "spearman") 
```

We only calculated Krippendorff's *alpha* for binary variables.  For temporal extent, we compared the start year and the end year determined by each of the two coders using scatter plots and correlation measured using Spearman's *rho*.  There were two cases in which one coder determined a start year but the other coder did not.  For cases in which both coders determined a start year, Spearman's *rho* for correlation of start years was `r round(cor(as.numeric(wg_dbl_cd$start.year[both_numeric]), as.numeric(er$start.year[both_numeric]), method = "spearman"), digits = 2)` (Fig. S??).  

### Temporal extent of studies

```{r print_time_anova, include = F}
kable(m_time_full_anova)
```

The following plots are for assessing assumptions for a linear regression of the form `log(number of years) ~ data type`.

```{r time_assess_correlation_predictors, fig.height=8, fig.width = 8}
### assess correlation between data type predictors ----------------------
dt_df <- wg[, which(
  colnames(wg) %in% 
    c("data.structure...organized.data.collection.scheme", 
      "data.structure...sampling.effort.known", 
      "data.structure...non.detection", 
      "data.structure...multiple.datasets.integrated.for.analysis"))]
for(i in 1:ncol(dt_df)) {
  dt_df[, i] <- as.factor(as.character(data.frame(dt_df)[, i]))
}

if(diag_present) {
  data_type_pairs_plot <- ggpairs(dt_df, columns = 1:ncol(dt_df), 
                                  upper = list(discrete = "blank"),
                                  lower = list(discrete = "ratio"), 
                                  diag = list(discrete = "barDiag"), 
                                  columnLabels = c("organized scheme", 
                                                   "sampling effort",
                                                   "non detection",
                                                   "multiple datasets"), 
                                  labeller = label_wrap_gen(width = 10)) + 
    ggtitle("Correlation of data type predictor variables") + 
    theme(axis.text.x = element_text(angle = 50, hjust = 1), 
          axis.text.y = element_text(angle = 50, hjust = 1))
  print(data_type_pairs_plot)
}
```

```{r temp_range_distribution_plot_print, include=FALSE} 
if(diag_present) {
  ## normality -------------------------------------
  # distribution of all data
  # hist(wg$temp_extent)
  # hist(wg$temp_extent[which(wg$temp_extent <= quantile(wg$temp_extent, 
  #                                                        0.90, na.rm = T))])
  
  # data very non-normal but log transformation kinda fixes this
  # (reciprocal and sqrt transformations did not help)
  print(ggplot(data = temp_extent_plot_df, aes(y = temp_extent, x = data_type)) + 
          geom_boxplot(varwidth = T) +
          scale_y_log10() + 
          theme_bw() + 
          ggtitle("log transformation of year\nmakes normality better") + 
          ylab("Temporal extent (years)") + 
          theme(axis.text.x = element_text(angle = 40, vjust = 0.9, 
                                           hjust = 1)))
}
```

```{r temp_range_distribution_ridge_plot_print, include=FALSE}
if(diag_present) {
  print(ggplot(data = temp_extent_plot_df, aes(x = temp_extent, y = data_type)) + 
          scale_x_log10() + 
          geom_density_ridges(stat = "binline", alpha = 0.4) +
          geom_density_ridges(alpha = 0.4) + 
          xlab("Temporal extent (years)") + 
          theme_bw())
}
```

```{r temp_range_distribution_years_by_data_type}
# distribution of log(years) by data type (raw data, not residuals) 
# Looking at model residuals is more useful.  However, this does show that the
# log(years) distribution is more normal for some data types than others.
if(diag) {
  for(i in unique(temp_extent_plot_df$data_type)) { 
    #hist(log(temp_extent_plot_df$temp_extent)[which(temp_extent_plot_df$data_type == i)])
    qqnorm(log(temp_extent_plot_df$temp_extent)[which(temp_extent_plot_df$data_type == i)], 
           main = i)
    qqline(log(temp_extent_plot_df$temp_extent)[which(temp_extent_plot_df$data_type == i)])
  }
}
```

Diagnostics for main effects model log_years ~ all 5 data types (no interactions).
```{r temp_extent_lm_diags_print, fig.height=4, fig.width=4}
if(diag_present) {
  print("Diagnostics for main effects model log_years ~ all 5 data types (no interactions)")
  par(mfrow = c(2, 2))
  plot(time_lm_full, ask = FALSE)
  par(mfrow = c(1, 1))
  # hist(time_resids, 
  #      main = "standardized residuals of full model for temporal extent", 
  #      xlab = "standardized residuals")
  # boxplot(time_resids, ylab = "standardized residuals", 
  #         main = "Residuals for lm model predicting log(temporal extent) ~ data type")
}
```

Table S1.  Variance inflation factors for predictor variables in a linear regression model of the log of number of years covered by a study as a function of the characteristics of the biological records data used in the study.  

```{r temp_extent_vif}
if(diag_present) {
  temp_extent_vif <- vif(time_lm_full) # assess multicollinearity
  temp_vif_df <- data.frame(temp_extent_vif)
  temp_vif_df$variable <- rownames(temp_vif_df)
  rownames(temp_vif_df) <- NULL
  kable(temp_vif_df, col.names = c("VIF", "variable"), digits = 2)
}
```

```{r temp_extent_mca_outputs}
summary(time_hom)

# dimension one is summarizing the structured data types variables.  Positive
# D1 values mean TRUE for the structured data type variables
# dimension two is mostly summarizing the "multiple datasets" variable.  
# Positive D2 values mean "TRUE" for the multiple datasets variable.
time_hom$loadings
plot(time_hom)
plot(time_hom, plot.type = "screeplot")
# I think this shows that there are cases with a range of D1 values at both the
# positive and negative end of the D2 dimension.  And D1 and D2 are not
# correlated with each other (obviously)
plot(time_hom, plot.type = "biplot")

# confirm that dimesion 1 values differ based on non-detection T/F
ggplot(data = temp_extent, aes(y = D1, x = data.structure...non.detection)) + geom_boxplot() + geom_point()
# confirm that dimesion 2 values do not differ much based on non-detection T/F
ggplot(data = temp_extent, aes(y = D2, x = data.structure...non.detection)) + geom_boxplot() + geom_point()

# confirm that dimesion 2 values differ based on multiple datasets T/F
ggplot(data = temp_extent, aes(y = D2, x = data.structure...multiple.datasets.integrated.for.analysis)) + geom_boxplot() + geom_point()
# but dimension 1 values do not differ much based on multiple datasets T/F
ggplot(data = temp_extent, aes(y = D1, x = data.structure...multiple.datasets.integrated.for.analysis)) + geom_boxplot() + geom_point()

temp_catpca_vif_df # low VIF.  
```

```{r check_time_lm_original_vars, include = F}
#### try model selection with original variables ------------------------------
# make all possible models
temp_extent_subsets <- temp_extent[, c("data.structure...sampling.effort.known", 
                  "data.structure...non.detection", 
                  "data.structure...organized.data.collection.scheme", 
                  "data.structure...multiple.datasets.integrated.for.analysis", 
                  "log_years")]
colnames(temp_extent_subsets)[ncol(temp_extent_subsets)] <- "y"
all_time_lms <- bestglm(
  temp_extent[, c("data.structure...sampling.effort.known", 
                  "data.structure...non.detection", 
                  "data.structure...organized.data.collection.scheme", 
                  "data.structure...multiple.datasets.integrated.for.analysis", 
                  "log_years")], 
  family = gaussian, IC = "AIC", intercept = TRUE, 
  RequireFullEnumerationQ = TRUE)
all_time_lms$BestModels

# look at summary of the best model
summary(all_time_lms$BestModel) # non-detectin is significant after removing sampling effort

# So it looks like the sampling effort variable is less important than 
# the non-detection variable.  Removing sampling effort from the full model 
# reduces AIC (as does removing non-detection, but that doesn't reduce AIC
# by as much, and the change is less than 2).  Removing organized collection scheme increases AIC.
```

### Ecological Focus
```{r print_response_variable_table}
kable(resp_count_df, col.names = c("response variable", 
                                   "number of studies", 
                                   "proportion of studies", 
                                   "lower bound 95% bootstrap CI on proportion", 
                                   "upper bound 95% bootstrap CI on proportion"), 
      digits = 2)
```

### Development of methodology across different study questions

```{r method_assumptions}
if(diag_present) {
  method_mod_vif <- vif(method_mod) # assess multicollinearity
  # H-L test H_0 is that model fits data well
  method_mod_hlTest <- hoslem.test(method_mod$y, method_mod$fitted.values, 
                                   g = 10) 
  # make confusion matrix for full model with threshold 0.5
  method_conf_matrix <- table(method_mod$y, method_mod$fitted.values > 0.5)
  method_misclass_rate <- (method_conf_matrix["1", "FALSE"] + 
                             tryCatch(method_conf_matrix["0", "TRUE"], 
                                      error = function(x) 0))/length(method_mod$y)
  # confusion matrix for null model
  method_null_conf_matrix <- table(method_mod_null$y, 
                                   method_mod_null$fitted.values > 0.5)
  method_null_misclass_rate <- (method_null_conf_matrix["1", "FALSE"] + 
                             tryCatch(
                               method_null_conf_matrix["0", "TRUE"], 
                               error = function(x) 0))/length(
                                 method_mod_null$y)
  method_diags <- LogisticDx::dx(method_mod, byCov = F)
  
  # determine which articles had large residuals or leverage
  big_resids <- c(wg$title[abs(method_diags$Pr) > 2], 
                  wg$title[abs(method_diags$dr) > 2], 
                  wg$title[abs(method_diags$h) > 2])
  big_resids <- unique(big_resids)
  big_resids_df <- data.frame(
    title = wg$title[which(wg$title %in% big_resids)],
    pearson_resids = method_diags$Pr[which(wg$title %in% big_resids)], 
    deviance_resids = method_diags$dr[which(wg$title %in% big_resids)], 
    leverage = method_diags$h[which(wg$title %in% big_resids)], 
    methodology.development.or.analysis = 
      wg$methodology.development.or.analysis[which(wg$title %in% big_resids)])
}
```

There were `r table(wg$methodology.development.or.analysis)[[2]]` methodological studies and `r table(wg$methodology.development.or.analysis)[[1]]` non-methodological studies, suggesting that we can fit `r min(table(wg$methodology.development.or.analysis))/10` predictors if we need at least 10 events or non-events per predictor.

Variance inflation factors for all variables were small indicating that correlation among the predictor variables is not likely to cause large uncertainty in the coefficient estimates of the logistic regression model [@Fox1992].  A Hosmer-Lemeshow goodness of fit test failed to reject the null hypothesis that the model fits the data well (*p* = `r round(method_mod_hlTest$p.value, 2)`). There were `r length(big_resids)` observations with big Pearson or deviance residuals or with leverage larger than two.

Most of the model diagnostics suggested that the logistic regression assumtions were met.  However, because there were `r length(big_resids)` large residuals, we checked the conclusions of the logistic regression by using bootstrapping to estimate 95% confidence intervals around the proportion of studies that were methodological within each of the six ecological or biological study focuses.  The 95% bootstrap CIs for the proportion of studies with each ecological focus that were methodological all overlapped (Fig. S??), confirming the conclusion from the logistic regression that we were unable reject a null hypothesis of no difference in the proportion of studies within each focus area that are methodological.

Print number of events:
```{r print_n_events, echo=TRUE}
if(diag_present) {
  # check number of events/ non-events to make sure we have enough data
  table(wg$methodology.development.or.analysis)
}
```

```{r cell_counts_table_print}
if(diag_present){
  # check for sparse cells in predictors (need > 5 observations in each cell)
  kable(data.frame(select(wg, response.variable...phenology, trends.over.time, 
                          response.variable...abundance, 
                          response.variable...species.richness, 
                          response.variable...distribution, other_focus) %>%
                     apply(MARGIN = 2, FUN = table)) %>%
          rownames_to_column() %>%
          gather(-1, key = "variable", value = "count") %>%
          spread(rowname, count))
}
```

```{r print_method_vif}
if(diag_present) {
  vif_df <- data.frame(method_mod_vif)
  vif_df$variable <- rownames(vif_df)
  rownames(vif_df) <- NULL
  kable(vif_df, col.names = c("VIF", "variable"), digits = 2)
}
```

```{r print_pearson_resids_plot}
if(diag_present) {
  method_glm_Pr_plot <- ggplot(data = method_diags, aes(x = Pr)) + 
    geom_histogram() + theme_bw() +
    xlab("Pearson residuals")
  print(method_glm_Pr_plot + 
          ggtitle("Pearson residuals for methodology\nby study focus logistic regression"))
}
```

```{r deviance_resids_methodology_plot}
if(diag_present) {
    method_glm_dr_plot <- ggplot(data = method_diags, aes(x = dr)) + 
    geom_histogram() + theme_bw() +
    xlab("Deviance residuals")
  print(method_glm_dr_plot + 
          ggtitle("Deviance residuals for methodology\nby study focus logistic regression"))
}
```

```{r leverage_methodology_plot}
if(diag_present) {
  method_glm_lev_plot <- ggplot(data = method_diags, aes(x = h)) + 
    geom_histogram() + theme_bw() +
    xlab("Leverage")
  print(method_glm_lev_plot + 
          ggtitle("Leverage for methodology\nby study focus logistic regression"))
}
```

```{r print_big_resids, include=FALSE}
if(diag_present) kable(big_resids_df, digits = 2)
```

```{r method_prop_boot}
# Get bootstrap CIs for proportion of studies with each focus that are 
# methodological
# This is a sensitivity analysis since logistic regression had some big resids.

method_prop_fun <- function(x, indices) {
  # ARGS: x - a 2-column dataframe with TRUE/FALSE values in each column, in 
  #           which the 1st column indicates whether the study is within the
  #           target study focus (e.g. phenology) and the
  #           2nd column indicates whether the study is methodological
  #       indices - a vector of indices as required by boot
  x_b <- x[indices, ]
  
  x_b <- x_b[x_b[, 1] == T, ] # subset to only studies with this focus
  # calculate proportion that are methodological
  sum(as.numeric(as.logical(as.character(x_b[, 2]))), na.rm = TRUE)/nrow(x_b) 
}

method_list_boot <- lapply(
  list(sp_rich = wg[, c("response.variable...species.richness", 
                        "methodology.development.or.analysis")], 
       dist = wg[, c("response.variable...distribution", 
                     "methodology.development.or.analysis")], 
       abund = wg[, c("response.variable...abundance", 
                     "methodology.development.or.analysis")], 
       phen = wg[, c("response.variable...phenology", 
                     "methodology.development.or.analysis")], 
       temp_trends = wg[, c("trends.over.time", 
                     "methodology.development.or.analysis")], 
       other_focus = wg[, c("other_focus", 
                     "methodology.development.or.analysis")]),
  FUN = get_boot_CI, fun_to_use = method_prop_fun, n_perm = n_perm)
```

```{r make_method_boot_df}
method_boot_df <- data.frame(matrix(nrow = length(method_list_boot), 
                                        ncol = 4))
colnames(method_boot_df) <- c("response_variable", "prop_studies",
                             "l_95ci", "u_95ci")

# fill response variable counts data frame using the list of bootstrap results
for(i in 1:length(method_list_boot)) {
  method_boot_df[i, "response_variable"] <- names(method_list_boot)[i]
  method_boot_df[i, "prop_studies"] <- method_list_boot[[i]]$ci_obj$t0
  method_boot_df[i, "l_95ci"] <- method_list_boot[[i]]$ci_obj$bca[4]
  method_boot_df[i, "u_95ci"] <- method_list_boot[[i]]$ci_obj$bca[5]
}

# clean response variable strings
method_boot_df$response_variable <- gsub(".*sp_rich.*", "species richness", 
                                        method_boot_df$response_variable)
method_boot_df$response_variable <- gsub(".*other.*", "other", 
                                        method_boot_df$response_variable)
method_boot_df$response_variable <- gsub(".*dist.*", "species distribution", 
                                        method_boot_df$response_variable)
method_boot_df$response_variable <- gsub(".*abund.*", "abundance", 
                                        method_boot_df$response_variable)
method_boot_df$response_variable <- gsub(".*phen.*", "phenology", 
                                        method_boot_df$response_variable)
method_boot_df$response_variable <- gsub(".*temp.*", "temporal trends", 
                                        method_boot_df$response_variable)
```

```{r method_boot_plot}
# plot proportion of studies using each response variable, with 95% CIs
prop_responses_boot_plot <- ggplot(data = method_boot_df, 
             aes(x = factor(response_variable, 
                            levels = c("species distribution", 
                                       "phenology", 
                                       "abundance",
                                       "species richness", 
                                       "temporal trends", 
                                       "other")), 
                 y = prop_studies)) + 
        geom_point(size = 3) + 
        geom_linerange(aes(x = factor(response_variable, 
                            levels = c("species distribution", 
                                       "phenology", 
                                       "abundance",
                                       "species richness", 
                                       "temporal trends", 
                                       "other")), 
                           ymin = l_95ci, 
                           ymax = u_95ci), size = 1.1) + 
        xlab("") + 
        ylab("Proportion of Studies\nthat were methodological") + 
        ylim(c(0, 1)) + 
        theme_bw() + 
        theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1), 
              strip.background = element_blank(), strip.placement = "outside")
print(prop_responses_boot_plot)
```

### What data types are used for each type of ecological study question?

To determine whether different types of biological records data were used to study different types of questions, we initially fit Poisson regression models of the number of studies with each combination of data type and biological or ecological study focus, fitting both a saturated model and an independence model with only main effects.  The Poisson independence model (no interatctions) was overdispersed (ratio of residual deviance to residual degrees of freedom = `r round(dt_pd_mod_ind$deviance/dt_pd_mod_ind$df.residual, digits = 2)`), so in the main text we opted to use a random permutation test which made no assumptions about the distribution of the data or residuals.  Here, we report the results and model residuals from the Poisson independence model (Fig. S??), which showed the same general pattern in the direction and size of residuals as the randomization test (Fig. ??), though there were some differences in relative sizes of residuals between the permutation and Poisson approaches.  Overall, the consistent pattern in the direction of residuals suggested that the interpretation of the results was robust to using different statistical tests. 
 
The use of data types differed significantly between study questions (*p* = `r round(dt_pd_interaction_test$"Pr(>Chi)"[2], 3)` for a chi-squared likelihood ratio test of a null hypothesis that a Poisson regression independence model with main effects for data type and ecological study focus (but no interaction term) model fit the data as well as a saturated model.  In the independence model, two cells had deviance residuals with an absolute value bigger than two (Fig. 4) - the cell for the number of studies investigating abundance using multiple biological datasets (residual = -3.5), and the cell for the number of studies investigating species distributions using multiple biological records datasets (residual = 2.8).  The residuals for those two cells were not as big in the permutation test reported in the main text, but the direction of the residuals was the same.  Deviance residuals from the Poisson model were generally positive for combinations of structured data types and study questions about abundance, and negative for combinations of structured data types and species distribution study questions (Fig. S??). This was in agreement with the permutation test (Fig ??) and suggested that abundance studies use structured data types more often than would be expected if data type and study question were not related, and distribution studies used structured data types less often than would be expected if study question and data type were not related.


```{r data_type_study_question_graph}
data_type_study_question_pois_plot <- ggplot(
  data = dt_pd_df, 
  aes(x = factor(study_focus, 
                 levels = c("abundance", "distribution", "phenology", 
                            "species.richness", "trends.over.time", 
                            "other_focus"), 
                 labels = c("abundance", "species\ndistribution", 
                            "phenology", 
                            "species\nrichness", "trends\nover\ntime", 
                            "other")), 
      y = factor(data_type,
                 levels = c("multiple.datasets.integrated.for.analysis", 
                            "sampling.effort.known", 
                            "non.detection",
                            "visit.specific.covariates",
                            "organized.data.collection.scheme"), 
                 labels = c("multiple\ndatasets", 
                            "sampling effort", 
                            "non-detection", 
                            "visit-specific\ncovariates",
                            "organized\nscheme")))) + 
  geom_tile(aes(fill = resids)) + 
  xlab("Study Question") + 
  ylab("Data Type") + 
  scale_fill_gradient2(low = "#3366ff", mid = "white", high = "#ff751a") + 
  theme(panel.grid.major.x=element_blank(), #no gridlines
        panel.grid.minor.x=element_blank(), 
        panel.grid.major.y=element_blank(), 
        panel.grid.minor.y=element_blank(),
        panel.background=element_rect(fill="white"), # background=white
        axis.text.x = element_text(angle=30, hjust = 1, vjust=1),
        axis.text.y = element_text(angle=45)) + 
  # ggtitle("Residuals from Poisson\nindependence model\nData Type by Study Question") + 
  labs(fill = "Deviance\nresiduals") 
print(data_type_study_question_pois_plot + 
        geom_text(aes(label = round(resids, digits = 1)))) # write the values
```

```{r print_data_type_paradigm_contingency_table}
if(diag_present) {
  dt_pd_df_wide$data_type <- gsub("\\.", " ", dt_pd_df_wide$data_type)
  colnames(dt_pd_df_wide) <- gsub("_", " ", colnames(dt_pd_df_wide))
  kable(data.frame(dt_pd_df_wide))
}
```

Big residuals from the Poisson regression of data type by study questions:

```{r print_big_resids_table}
kable(dt_pd_df[abs(dt_pd_df$resids) > 1, 
               c("study_focus", "data_type", "n", "predicted", "resids")], 
      col.names = c("Study Focus", "Data Type", "Observed number of studies", 
                    "Predicted number of studies", "Residual (on the scale of the linear predictors)"),digits = 1)
```

```{r print_data_type_paradigm_interaction_test, include = F}
# testing null hypothesis that the reduced model is as good as the full model
# cannot reject that null. Therefore there is no evidence that the interaction
# term is significant.  So we can say that the use of data types does not
# depend on the study question paradigm.  
dt_pd_interaction_test
```

```{r data_type_paradigm_assumption_check}
# following this: https://newonlinecourses.science.psu.edu/stat504/node/169/

# plot predicted v. observed for independence model
dt_pd_poisson_pred_obs_plot <- ggplot(data = dt_pd_df, aes(x = n, y = predicted)) + 
  geom_point(size = 3) + 
  geom_abline(slope = 1) + 
  xlab("observed") + theme_bw()
print(dt_pd_poisson_pred_obs_plot) # actually looks decent

# for independence model:
summary(dt_pd_mod_ind)
# resid dev / resid df - a bit bigger than one, but not huge.
dt_pd_mod_ind$deviance/dt_pd_mod_ind$df.residual 
1-pchisq(dt_pd_mod_ind$deviance, dt_pd_mod_ind$df.residual) 
# p = 0.007.  So reject null hypothesis that model is correctly specified.  
# However, http://thestatsgeek.com/2014/04/26/deviance-goodness-of-fit-test-for-poisson-regression/
# warns that this test can often wrongly indicate that the model is incorrectly
# specified.  But that seems to be more of a problem when expected means are low.
# When that post used means between 20 and 55, the pchisq test worked ok.  My
# observed cell count values are generally 10 or 15-ish up to a hundred.  So I
# am right on the border of having "large" means.  

# for saturated model:
summary(dt_pd_mod_full)
# (the residual deviance/df value would have division by 0.  Not meaningful. Of course fit is perfect in a saturated model.)
```


```{r data_type_by_paradigm_barplot, fig.height=7, fig.width=7, include = F}
### plot data type by study question paradigm ----------------------
# make plot, order factor levels by most important a-priori
# dt_pd_df was created in the Methods section
ggplot(dt_pd_df, 
       aes(x = factor(
         data_type, 
         levels = c("sampling.effort.known", 
                    "non.detection", 
                    "organized.data.collection.scheme", 
                    "multiple.datasets.integrated.for.analysis"), 
         labels = c("sampling effort", 
                    "non detection", 
                    "organized scheme", 
                    "multiple datasets")), 
         y = n, 
         fill = factor(
           data_type,  
           levels = c("sampling.effort.known", 
                      "non.detection", 
                      "organized.data.collection.scheme",
                      "multiple.datasets.integrated.for.analysis"), 
           labels = c("sampling effort", 
                      "non detection", 
                      "organized scheme", 
                      "multiple datasets")))) + 
  geom_bar(position = position_dodge(preserve = "single"), stat = "identity") + 
  facet_wrap( ~ factor(study_focus, levels = c("abundance", 
                                               "distribution",
                                               "phenology", 
                                               "species.richness", 
                                               "trends.over.time", 
                                               "other_focus"),
                       labels = c("abundance",
                                  "distribution",
                                  "phenology", 
                                  "species richness", 
                                  "trends over time", 
                                  "other focus")), 
              strip.position = "bottom") +
  ggtitle("Data type used for different ecological study questions") + 
  scale_fill_discrete(name = "Data Type") + 
  xlab("Study question") + 
  ylab("Number of studies") + 
  scale_y_continuous(breaks = seq(0, max(dt_pd_df$n), 5)) + 
  theme_bw() + 
  theme(axis.text.x = element_blank(), 
        text = element_text(size = plot_text_size),
        strip.background = element_blank(), strip.placement = "outside")
```

### Analysis approach & data type

To investigate whether the data type affects the analysis approach, we initially used a Poisson regression to model the relationship between three different analysis approaches (inference, prediction, and description) and data type.  The Poisson independence model (no interatctions) was overdispersed (ratio of residual deviance to residual degrees of freedom = `r round(dt_an_mod_ind$deviance/dt_an_mod_ind$df.residual, digits = 2)`), so in the main text we opted to use a random permutation test which made no assumptions about the distribution of the data or residuals.  Here, we report the results and model residuals from the Poisson independence model (Fig. S??), which showed the same pattern as the permutation test (Fig. 6), suggesting that the interpretation of the results is robust to using different statistical tests, and the overdispersion in the independence model is not severe enough to make the model uninformative. 

A Chi-squared likelihood ratio test comparing the saturated Poisson regression model  to a reduced model with only main effects for analysis approach and data type rejected the null hypothesis that the reduced model is as good as the full model (*p* = `r round(dt_an_mod_independence_test$'Pr(>Chi)'[2], 3)`), suggesting that the data type does influence or constrain the analysis approach.  

```{r data_type_analysis_approach_graph, include=FALSE}
### plot data type by analysis approach ----------------------
# make plot, order factor levels by most important a-priori
# dt_an_df was created in the Methods section
ggplot(dt_an_df, 
       aes(x = factor(data_type, 
                      levels = c("sampling.effort.known", 
                                 "non.detection", 
                                 "organized.data.collection.scheme", 
                                 "multiple.datasets.integrated.for.analysis",
                                 "visit.specific.covariates"), 
                      labels = c("sampling effort", 
                                 "non detection", 
                                 "organized scheme", 
                                 "multiple datasets", 
                                 "visit-specific covariates")), 
           y = n, 
           fill = factor(data_type, 
                         levels = c("sampling.effort.known", 
                                    "non.detection", 
                                    "organized.data.collection.scheme", 
                                    "multiple.datasets.integrated.for.analysis", 
                                    "visit.specific.covariates"), 
                         labels = c("sampling effort", 
                                    "non detection", 
                                    "organized\nscheme", 
                                    "multiple\ndatasets", 
                                    "visit-specific\ncovariates")))) + 
  geom_bar(position = position_dodge(preserve = "single"), stat = "identity") + 
    facet_wrap( ~ factor(analysis_approach, 
                         levels = c("inference", 
                                    "prediction",
                                    "descriptive.only"),
                         labels = c("inference", "prediction",
                                    "description only")), 
                strip.position = "bottom") +
  ggtitle("Data used in different analysis approaches") + 
  scale_fill_discrete(name = "Data Type") + 
  xlab("Analysis Approach") + 
  ylab("Number of studies") + 
  scale_y_continuous(breaks = seq(0, max(dt_an_df$n) + 5, 5)) + 
  theme_bw() + 
  theme(axis.text.x = element_blank(), 
        strip.background = element_blank(), strip.placement = "outside")
```

```{r print_data_type_analysis_contingency_table}
if(diag_present) {
  kable(data.frame(dt_an_df_wide), 
        col.names = c("Data Type", "Results - descriptive only", 
                      "Results - inference", "Results - prediction"))
}
```

```{r print_data_type_analysis_interaction_test, include = F}
# testing null hypothesis that the reduced model is as good as the full model.
# This rejects the null.  So we can say that the effect of the data type on the
# number of studies depends on analysis approach. 
dt_an_mod_independence_test # (same as below)
```

```{r data_type_analysis_assumption_check}
# following this: https://newonlinecourses.science.psu.edu/stat504/node/169/

# plot predicted v. observed for independence model
plot(dt_an_df$predicted ~ dt_an_df$n) # looks ok
abline(0, 1)

# for independence model:
summary(dt_an_mod_ind)
dt_an_mod_ind$deviance/dt_an_mod_ind$df.residual # resid dev / df - Over 3. Overdispersed
1-pchisq(dt_an_mod_ind$deviance, dt_an_mod_ind$df.residual) # (same as chisq test above)
# p = 0.002.  So reject null hypothesis that model is correctly specified.  
# However, http://thestatsgeek.com/2014/04/26/deviance-goodness-of-fit-test-for-poisson-regression/
# warns that this test can often wrongly indicate that the model is incorrectly
# specified.  But that seems to be more of a problem when expected means are low.
# When that post used means between 20 and 55, the pchisq test worked ok.  My
# observed cell count values are generally 10 or 15-ish up to a hundred.  So I
# am right on the border of having "large" means.  

# for saturated model:
summary(dt_pd_mod_full)
# (the residual deviance/df value would have division by 0.  Not meaningful. Of course fit is perfect in a saturated model.)
```

```{r ata_type_analysis_approach_nb_model}
# fit saturated model
dt_an_nb_full <- try(glm.nb(n ~ 1 + analysis_approach + data_type + 
                        analysis_approach:data_type, data = dt_an_df, 
                      init.theta = 1, link = identity, 
                      start = dt_an_mod_full$coefficients))
# fit model assuming independence between analysis approach and data type
dt_an_nb_ind <- glm.nb(n ~ 1 + analysis_approach + data_type, 
                     data = dt_an_df, 
                     init.theta = 1, link = identity, 
                     start = dt_an_mod_ind$coefficients)


# fit saturated model - quasipoisson
dt_an_qp_full <- glm(n ~ 1 + analysis_approach + data_type + 
                        analysis_approach:data_type, data = dt_an_df, 
                      family = quasipoisson(link = log))
summary.glm(dt_an_qp_full)
summary.glm(dt_an_qp_full)$dispersion
# fit model assuming independence between analysis approach and data type
dt_an_qp_ind <- glm(n ~ 1 + analysis_approach + data_type, 
                     data = dt_an_df, family = quasipoisson(link = log))
summary.glm(dt_an_qp_ind)
summary.glm(dt_an_qp_ind)$dispersion

dt_an_qp_independence_test <- try(anova(dt_an_qp_ind, dt_an_qp_full, 
                                        test = "Chisq"))
# that doesn't work, but regardless, the residual deviance from the 
# quasipoisson and poisson models are identical.  
```

```{r print_data_type_analysis_method_heatmap_table_poisson}
dt_an_pois_heatmap <- ggplot(
  data = dt_an_df, 
  aes(x = factor(analysis_approach, 
                 levels = c("descriptive.only", "inference", "prediction"), 
                 labels = c("descriptive\nonly", 
                            "inference", "prediction")), 
      y = factor(data_type,
                 levels = c("multiple.datasets.integrated.for.analysis", 
                            "sampling.effort.known", 
                            "non.detection",
                            "visit.specific.covariates",
                            "organized.data.collection.scheme"), 
                 labels = c("multiple\ndatasets", 
                            "sampling effort", 
                            "non detection", 
                            "visit-specific\ncovariates",
                            "organized\nscheme")))) + 
  geom_tile(aes(fill = resids)) + 
  xlab("Analysis Approach") + 
  ylab("Data Type") + 
  scale_fill_gradient2(low = "blue", mid = "white", high = "orange") + 
  theme(panel.grid.major.x=element_blank(), #no gridlines
        panel.grid.minor.x=element_blank(), 
        panel.grid.major.y=element_blank(), 
        panel.grid.minor.y=element_blank(),
        panel.background=element_rect(fill="white"), # background=white
        axis.text.x = element_text(angle=30, hjust = 1, vjust=1),
        axis.text.y = element_text(angle=45)) + 
  labs(fill = "Deviance\nresiduals")
print(dt_an_pois_heatmap + 
  geom_text(aes(label = round(resids, digits = 1))))
```

```{r}
kable(prox_src_df, col.names = c("Proximate Data Source", "Number of Studies"))
```


# Appendix 2 - Instructions for Coders
[ Attach pdf of Instructions for Coders ]

# Appendix 3 - Search Terms
The search terms used with each search engine to find studies for this review are listed below, along with the date of the search.

Web of Science, searched 15 June 2018 with search terms: `TS=(("biological record*" OR "citizen science" OR atlas OR gbif OR "Global Biodiversity Information Facility" OR eBird OR "National Biodiversity Data Centre" OR "NBN" OR "National Biodiversity Network" OR "Centre for Environmental Data and Recording") AND ("Great Britain" OR Ireland OR Scotland OR Wales OR "Northern Ireland" OR "British Isles" OR UK OR "United Kingdom") NOT (cancer OR disease OR palliative OR "cross-sectional" OR "cross sectional" OR "New South Wales" OR Pyle OR marmoset))`

Scopus, searched 15 June 2018 with search terms: `TITLE-ABS-KEY (("biological record*" OR "citizen science" OR (atlas AND species) OR gbif OR "Global Biodiversity Information Facility" OR eBird OR "National Biodiversity Data Centre" OR "NBN" OR "National Biodiversity Network" OR "Centre for Environmental Data and Recording") AND ("Great Britain" OR Ireland OR Scotland OR Wales OR "Northern Ireland" OR "British Isles" OR UK OR "United Kingdom") AND NOT (cancer OR disease OR palliative OR "cross-sectional" OR "cross sectional" OR "New South Wales" OR Pyle OR marmoset))`

ProQuest, searched 15 June 2018 with search terms: Full-text results with terms `noft("citizen science") AND noft("Ireland")`

Google Scholar searched through Publish or Perish (@Harzing2007) over multiple days.

- 8 June 2018: Publish or Perish search terms: all of the words: "biological records"‚ Ireland; none of the words: book, "new report", "new species", "new record"; years: 2014 to 2018
- 8 June 2018: Publish or Perish search term: all of the words: "citizen science", species, Ireland; none of the words: disease, book, "new report", "new species", "new record"; years: 2014 to 2018
- 8 June 2018: Publish or Perish search terms: all of the words: "citizen science", species, "United Kingdom"; none of the words: disease, book, "new report", "new species", "new record"; years: 2014 to 2018
- 12 June 2018: PoP search terms: all of the words: "biological records", "United Kingdom"; none of the words: book‚ "new report", "new species", "new record"

# Save plots and tables
```{r save_plots_main_text}
t_size_save <- 25

meth_df # print table 1 (methodology logistic regression results)
method_question_df
# spat_boot_df # print table 1 (spatial extent) # removed from the manuscript

# ggsave("Fig1.svg", temp_extent_plot_ridge + 
#          theme(text = element_text(size = t_size_save)), 
#        width = 25, height = 25, units = "cm", 
#        device = "svg")
# # ggsave("Fig2a.svg", spat_extent_plot + 
# #          theme(text = element_text(size = t_size_save)), 
# #        width = 25, height = 25, units = "cm", 
# #        device = "svg")
# ggsave("Fig2.svg", data_type_analysis_approach_plot + 
#          theme(text = element_text(size = t_size_save), 
#         legend.key.height = grid::unit(1.5, "cm")) + 
#   geom_text(aes(label = paste0(round(residual, digits = 1), "\n", sig_symb)), 
#             size = t_size_save/2), 
#        width = 25, height = 25, units = "cm", 
#        device = "svg")
# ggsave("Fig3.svg", response_var_plot + 
#          theme(text = element_text(size = t_size_save)), 
#        width = 25, height = 25, units = "cm", 
#        device = "svg")
# ggsave("Fig4.svg", data_type_study_question_perm_plot + 
#          theme(text = element_text(size = t_size_save), 
#         legend.key.height = grid::unit(1.5, "cm")) + 
#   geom_text(aes(label = paste0(round(residual, digits = 1), "\n", sig_symb)), 
#             size = t_size_save/3), # write the values
#        width = 25, height = 25, units = "cm", 
#        device = "svg")

## use eps
ggsave("Fig1.eps", temp_extent_plot_ridge + 
         theme(text = element_text(size = t_size_save)), 
       width = 25, height = 25, units = "cm", 
       device = "eps")
ggsave("Fig2.eps", data_type_analysis_approach_plot + 
         theme(text = element_text(size = t_size_save), 
        legend.key.height = grid::unit(1.5, "cm")) + 
  geom_text(aes(label = paste0(round(residual, digits = 1), "\n", sig_symb)), 
            size = t_size_save/2), 
       width = 25, height = 25, units = "cm", 
       device = "eps")
ggsave("Fig3.eps", response_var_plot + 
         theme(text = element_text(size = t_size_save)), 
       width = 25, height = 25, units = "cm", 
       device = "eps")
ggsave("Fig4.eps", data_type_study_question_perm_plot + 
         theme(text = element_text(size = t_size_save), 
        legend.key.height = grid::unit(1.5, "cm")) + 
  geom_text(aes(label = paste0(round(residual, digits = 1), "\n", sig_symb)), 
            size = t_size_save/3), # write the values
       width = 25, height = 25, units = "cm", 
       device = "eps")



tsize_manuscript <- 15
ggsave("Gaul Figure 1.jpg", temp_extent_plot_ridge + 
         theme(text = element_text(size = tsize_manuscript)), 
       width = 11, height = 11, units = "cm", dpi = 300, 
       device = "jpg")
ggsave("Gaul Figure 2.jpg", data_type_analysis_approach_plot + 
         theme(text = element_text(size = tsize_manuscript), 
               legend.key.height = grid::unit(1, "cm")) + 
  geom_text(aes(label = paste0(round(residual, digits = 1), "\n", sig_symb)), 
            size = tsize_manuscript/3), 
       width = 11, height = 11, units = "cm", 
       device = "jpg")
ggsave("Gaul Figure 3.jpg", response_var_plot + 
         theme(text = element_text(size = tsize_manuscript)), 
       width = 11, height = 11, units = "cm", 
       device = "jpg")
ggsave("Gaul Figure 4.jpg", data_type_study_question_perm_plot + 
         theme(text = element_text(size = tsize_manuscript), 
               legend.key.height = grid::unit(1, "cm"), 
               axis.text = element_text(
                 size = tsize_manuscript - 5)) + 
         geom_text(aes(label = paste0(round(residual, 
                                            digits = 1), "\n", 
                                      sig_symb)), 
                   size = tsize_manuscript/5), # write the values, 
       width = 11, height = 11, units = "cm", 
       device = "jpg")
```


```{r save_plots_appendices}
ggsave("FigS1.jpg", mca_loadings_plot + 
         geom_label_repel(point.padding = 1, 
                          box.padding = 1, size = t_size_save/4) + 
         theme(text = element_text(size = t_size_save)), 
       width = 25, height = 25, units = "cm", 
       device = "jpg")


ggsave("FigS1.1.svg", data_type_pairs_plot + 
         theme(text = element_text(size = t_size_save)), 
       width = 25, height = 25, units = "cm", 
       device = "svg")
ggsave("FigS1.3.svg", krip_plot + 
         theme(text = element_text(size = t_size_save)), 
       width = 25, height = 25, units = "cm", 
       device = "svg")
ggsave("FigS1.4.svg", start_year_cor_plot + 
         theme(text = element_text(size = t_size_save)), 
       width = 25, height = 25, units = "cm", 
       device = "svg")

svg("FigS1.5.svg")
par(mfrow = c(2, 2)) 
plot(time_lm_full, ask = FALSE) 
par(mfrow = c(1, 1))
dev.off()
ggsave("FigS1.6.svg", multiplot(
  method_glm_Pr_plot + theme(text = element_text(size = t_size_save)) + xlab("(a)"),
  method_glm_lev_plot + theme(text = element_text(size = t_size_save)) + xlab("(c)"), 
  method_glm_dr_plot + theme(text = element_text(size = t_size_save)) + xlab("(b)"), 
  cols = 2), 
       width = 25, height = 25, units = "cm", 
       device = "svg")
ggsave("FigS1.7.svg", prop_responses_boot_plot + 
         theme(text = element_text(size = t_size_save)), 
       width = 25, height = 25, units = "cm", 
       device = "svg")
ggsave("FigS1.8.svg", data_type_study_question_pois_plot + 
         theme(text = element_text(size = t_size_save), 
               legend.key.height = unit(1.1, "cm")) + 
        geom_text(aes(label = round(resids, digits = 1)), size = t_size_save/3), 
       width = 25, height = 25, units = "cm", 
       device = "svg")
ggsave("FigS1.9.svg", dt_an_pois_heatmap + 
         theme(text = element_text(size = t_size_save), 
               legend.key.height = unit(1.1, "cm")) + 
        geom_text(aes(label = round(resids, digits = 1)), size = t_size_save/3), 
       width = 25, height = 25, units = "cm", 
       device = "svg")



ggsave("FigS1.1.jpg", data_type_pairs_plot + 
         theme(text = element_text(size = t_size_save)), 
       width = 25, height = 25, units = "cm", 
       device = "jpg")
ggsave("FigS1.3.jpg", krip_plot + 
         theme(text = element_text(size = t_size_save)), 
       width = 25, height = 25, units = "cm", 
       device = "jpg")
ggsave("FigS1.4.jpg", start_year_cor_plot + 
         theme(text = element_text(size = t_size_save)), 
       width = 25, height = 25, units = "cm", 
       device = "jpg")

jpeg("FigS1.5.jpg", width = 25, height = 25, units = "cm", res = 290)
par(mfrow = c(2, 2)) 
plot(time_lm_full, ask = FALSE) 
par(mfrow = c(1, 1))
dev.off()
ggsave("FigS1.6.jpg", multiplot(
  method_glm_Pr_plot + theme(text = element_text(size = t_size_save)) + xlab("(a)"),
  method_glm_lev_plot + theme(text = element_text(size = t_size_save)) + xlab("(c)"), 
  method_glm_dr_plot + theme(text = element_text(size = t_size_save)) + xlab("(b)"), 
  cols = 2), 
  width = 25, height = 25, units = "cm", 
  device = "jpg")
ggsave("FigS1.7.jpg", prop_responses_boot_plot + 
         theme(text = element_text(size = t_size_save)), 
       width = 25, height = 25, units = "cm", 
       device = "jpg")
ggsave("FigS1.8.jpg", data_type_study_question_pois_plot + 
         theme(text = element_text(size = t_size_save), 
               legend.key.height = unit(1.1, "cm")) + 
        geom_text(aes(label = round(resids, digits = 1)), size = t_size_save/3), 
       width = 25, height = 25, units = "cm", 
       device = "jpg")
ggsave("FigS1.9.jpg", dt_an_pois_heatmap + 
         theme(text = element_text(size = t_size_save), 
               legend.key.height = unit(1.1, "cm")) + 
        geom_text(aes(label = round(resids, digits = 1)), size = t_size_save/3), 
       width = 25, height = 25, units = "cm", 
       device = "jpg")
```


</div>